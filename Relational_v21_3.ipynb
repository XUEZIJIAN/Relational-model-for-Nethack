{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPUFfjku6PYh"
      },
      "source": [
        "# Install NLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiytataVWnY4",
        "outputId": "4241f90b-818e-4c17-b7aa-16302d29f333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package libglib2.0\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mCouldn't find any package by glob 'libglib2.0'\u001b[0m\n",
            "Requirement already satisfied: gnuplotlib in /usr/local/lib/python3.10/dist-packages (0.42)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gnuplotlib) (1.26.4)\n",
            "Requirement already satisfied: numpysane>=0.3 in /usr/local/lib/python3.10/dist-packages (from gnuplotlib) (0.40)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "gnuplot is already the newest version (5.4.2+dfsg2-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n",
            "Requirement already satisfied: nle in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle) (2.13.6)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from nle) (1.26.4)\n",
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.10/dist-packages (from nle) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (0.0.4)\n",
            "Requirement already satisfied: nle[agent] in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (2.13.6)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (1.26.4)\n",
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (1.0.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.1->nle[agent]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.1->nle[agent]) (3.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "! apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0 libsm6 libxext6 cmake\n",
        "! pip install gnuplotlib\n",
        "! apt-get install -y gnuplot\n",
        "! pip install nle\n",
        "! pip install \"nle[agent]\"\n",
        "! pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sHtjr9ULoaW"
      },
      "outputs": [],
      "source": [
        "# From: https://github.com/facebookresearch/nle/issues/359#issue-1782082844\n",
        "# !sudo apt-get install -y build-essential autoconf libtool pkg-config \\\n",
        "#     python3-dev python3-pip python3-numpy git flex bison libbz2-dev\n",
        "# ! pip install --no-use-pep517 nle\n",
        "# ! pip install \"nle[agent]\"\n",
        "\n",
        "# Faile\n",
        "# !apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0 libsm6 libxext6 cmake\n",
        "# !pip install nle==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai1ceOJXJ1tR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRO_HN7y7ibG"
      },
      "source": [
        "# Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FbjyrY5c_xf"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import pprint\n",
        "import threading\n",
        "import time\n",
        "import timeit\n",
        "import traceback\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Necessary for multithreading.\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "load_model = True\n",
        "f_dir = 'torchbeast/plots/Relational_v21.3_4.tar'\n",
        "log_time = 100\n",
        "\n",
        "act_list = [\n",
        "    \"N\", \"E\", \"S\", \"W\", \"NE\", \"SE\", \"SW\", \"NW\",  # CompassDirection\n",
        "    \"N_\", \"E_\", \"S_\", \"W_\", \"NE_\", \"SE_\", \"SW_\", \"NW_\",  # CompassDirectionLonger\n",
        "    \"UP\", \"DOWN\", \"WAIT\", \"MORE\",           # MiscDirection and MiscAction\n",
        "    \"ADJUST\", \"APPLY\", \"ATTRIBUTES\", \"CALL\", \"CAST\", \"CHAT\", \"CLOSE\", \"DIP\", \"DROP\", \"DROPTYPE\",\n",
        "    \"EAT\", \"ENGRAVE\", \"ENHANCE\", \"ESC\", \"FIGHT\", \"FIRE\", \"FORCE\", \"INVENTORY\", \"INVENTTYPE\", \"INVOKE\",\n",
        "    \"JUMP\", \"KICK\", \"LOOK\", \"LOOT\", \"MONSTER\", \"MOVE\", \"MOVEFAR\", \"OFFER\", \"OPEN\", \"PAY\", \"PICKUP\",\n",
        "    \"PRAY\", \"PUTON\", \"QUAFF\", \"QUIVER\", \"READ\", \"REMOVE\", \"RIDE\", \"RUB\", \"RUSH\", \"RUSH2\", \"SEARCH\",\n",
        "    \"SEEARMOR\", \"SEERINGS\", \"SEETOOLS\", \"SEETRAP\", \"SEEWEAPON\", \"SHELL\", \"SIT\", \"SWAP\", \"TAKEOFF\",\n",
        "    \"TAKEOFFALL\", \"THROW\", \"TIP\", \"TURN\", \"TWOWEAPON\", \"UNTRAP\", \"VERSIONSHORT\", \"WEAR\", \"WIELD\",\n",
        "    \"WIPE\", \"ZAP\",\n",
        "    \"PLUS\", \"QUOTE\", \"DOLLAR\", \"SPACE\"         # TextCharacters\n",
        "]\n",
        "\n",
        "act_useful = [\n",
        "    \"MORE\",\n",
        "    # C-m\n",
        "    \"N\", \"E\", \"S\", \"W\", \"NE\", \"SE\", \"SW\", \"NW\",\n",
        "    # k  l  j   h  u   n   b   y\n",
        "    \"N_\", \"E_\", \"S_\", \"W_\", \"NE_\", \"SE_\", \"SW_\", \"NW_\",\n",
        "    \"UP\", \"DOWN\", \"WAIT\", \"KICK\", \"EAT\", \"SEARCH\",\n",
        "    # <   >    .   C-d    e    s\n",
        "     \"PICKUP\", \"ESC\", \"DROP\", \"LOOK\",\n",
        "    #  ,         d    :\n",
        "     \"WIELD\" ,\"PUTON\", \"REMOVE\", \"WEAR\", \"TAKEOFF\",\n",
        "    #  w    P     R     W     T\n",
        "     \"APPLY\", \"CLOSE\", \"FIRE\", \"RUSH\", \"INVENTORY\", \"MOVE\",\n",
        "    #  a    c     f    g     i     m\n",
        "     \"OPEN\", \"PAY\", \"QUAFF\",# \"READ\", \"TAKEOFFALL\", \"UNTRAP\", \"ZAP\", \"CAST\",\n",
        "    #  o    p    q\n",
        "]\n",
        "\n",
        "# act_index = [act_list.index(i) for i in act_useful]\n",
        "act_mask = [1 if action in act_useful else 0 for action in act_list]\n",
        "\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from torch import multiprocessing as mp\n",
        "    from torch import nn\n",
        "    from torch.nn import functional as F\n",
        "except ImportError:\n",
        "    logging.exception(\n",
        "        \"PyTorch not found. Please install the agent dependencies with \"\n",
        "        '`pip install \"nle[agent]\"`'\n",
        "    )\n",
        "\n",
        "import gymnasium as gym  # noqa: E402\n",
        "\n",
        "import nle  # noqa: F401, E402\n",
        "from nle import nethack  # noqa: E402\n",
        "from nle.agent import vtrace  # noqa: E402\n",
        "\n",
        "# yapf: disable\n",
        "parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
        "\n",
        "parser.add_argument(\"--env\", type=str, default=\"NetHackScore-v0\",\n",
        "                    help=\"Gym environment.\")\n",
        "parser.add_argument(\"--mode\", default=\"train\",\n",
        "                    choices=[\"train\", \"test\", \"test_render\"],\n",
        "                    help=\"Training or test mode.\")\n",
        "\n",
        "# Training settings.\n",
        "parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
        "                    help=\"Disable saving checkpoint.\")\n",
        "parser.add_argument(\"--savedir\", default=\"~/torchbeast/\",\n",
        "                    help=\"Root dir where experiment data will be saved.\")\n",
        "parser.add_argument(\"--num_actors\", default=4, type=int, metavar=\"N\",\n",
        "                    help=\"Number of actors (default: 4).\")\n",
        "parser.add_argument(\"--total_steps\", default=100000, type=int, metavar=\"T\",\n",
        "                    help=\"Total environment steps to train for.\")\n",
        "parser.add_argument(\"--total_steps_\", default=100000, type=int, metavar=\"T\",\n",
        "                    help=\"Total environment steps each time to train for.\")\n",
        "parser.add_argument(\"--batch_size\", default=8, type=int, metavar=\"B\",\n",
        "                    help=\"Learner batch size.\")\n",
        "parser.add_argument(\"--unroll_length\", default=80, type=int, metavar=\"T\",\n",
        "                    help=\"The unroll length (time dimension).\")\n",
        "parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
        "                    metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
        "parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=2, type=int,\n",
        "                    metavar=\"N\", help=\"Number learner threads.\")\n",
        "parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
        "                    help=\"Disable CUDA.\")\n",
        "parser.add_argument(\"--use_lstm\", action=\"store_true\",\n",
        "                    help=\"Use LSTM in agent model.\")\n",
        "parser.add_argument(\"--save_ttyrec_every\", default=1000, type=int,\n",
        "                    metavar=\"N\", help=\"Save ttyrec every N episodes.\")\n",
        "\n",
        "\n",
        "# Loss settings.\n",
        "parser.add_argument(\"--entropy_cost\", default=0.0006,\n",
        "                    type=float, help=\"Entropy cost/multiplier.\")\n",
        "parser.add_argument(\"--baseline_cost\", default=0.5,\n",
        "                    type=float, help=\"Baseline cost/multiplier.\")\n",
        "parser.add_argument(\"--discounting\", default=0.99,\n",
        "                    type=float, help=\"Discounting factor.\")\n",
        "parser.add_argument(\"--reward_clipping\", default=\"abs_one\",\n",
        "                    choices=[\"abs_one\", \"none\"],\n",
        "                    help=\"Reward clipping.\")\n",
        "\n",
        "# Optimizer settings.\n",
        "parser.add_argument(\"--learning_rate\", default=0.00048,\n",
        "                    type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
        "parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
        "                    help=\"RMSProp smoothing constant.\")\n",
        "parser.add_argument(\"--momentum\", default=0, type=float,\n",
        "                    help=\"RMSProp momentum.\")\n",
        "parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
        "                    help=\"RMSProp epsilon.\")\n",
        "parser.add_argument(\"--grad_norm_clipping\", default=40.0, type=float,\n",
        "                    help=\"Global gradient norm clip.\")\n",
        "# yapf: enable\n",
        "\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=(\n",
        "        \"[%(levelname)s:%(process)d %(module)s:%(lineno)d %(asctime)s] \" \"%(message)s\"\n",
        "    ),\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "\n",
        "def nested_map(f, n):\n",
        "    if isinstance(n, tuple) or isinstance(n, list):\n",
        "        return n.__class__(nested_map(f, sn) for sn in n)\n",
        "    if isinstance(n, dict):\n",
        "        return {k: nested_map(f, v) for k, v in n.items()}\n",
        "    return f(n)\n",
        "\n",
        "\n",
        "def compute_baseline_loss(advantages):\n",
        "    return 0.5 * torch.sum(advantages**2)\n",
        "\n",
        "\n",
        "def compute_entropy_loss(logits):\n",
        "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
        "    policy = F.softmax(logits, dim=-1)\n",
        "    log_policy = F.log_softmax(logits, dim=-1)\n",
        "    return torch.sum(policy * log_policy)\n",
        "\n",
        "\n",
        "def compute_policy_gradient_loss(logits, actions, advantages):\n",
        "    cross_entropy = F.nll_loss(\n",
        "        F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
        "        target=torch.flatten(actions, 0, 1),\n",
        "        reduction=\"none\",\n",
        "    )\n",
        "    cross_entropy = cross_entropy.view_as(advantages)\n",
        "    return torch.sum(cross_entropy * advantages.detach())\n",
        "\n",
        "\n",
        "def create_env(name, *args, **kwargs):\n",
        "    return gym.make(name, observation_keys=(\"glyphs\", \"blstats\", \"message\", \"inv_glyphs\", \"inv_letters\", \"inv_oclasses\", \"inv_strs\"), *args, **kwargs)  # noqa: B026\n",
        "    # \"inv_glyphs\",\"inv_letters\", \"inv_oclasses\", \"inv_strs\"\n",
        "\n",
        "def act(\n",
        "    flags,\n",
        "    actor_index: int,\n",
        "    free_queue: mp.SimpleQueue,\n",
        "    full_queue: mp.SimpleQueue,\n",
        "    model: torch.nn.Module,\n",
        "    buffers,\n",
        "    initial_agent_state_buffers,\n",
        "):\n",
        "    try:\n",
        "        logging.info(\"Actor %i started.\", actor_index)\n",
        "\n",
        "        gym_env = create_env(\n",
        "            flags.env, savedir=flags.rundir, save_ttyrec_every=flags.save_ttyrec_every\n",
        "        )\n",
        "        env = ResettingEnvironment(gym_env)\n",
        "        env_output = env.initial()\n",
        "        agent_state = model.initial_state(batch_size=1)\n",
        "        agent_output, unused_state = model(env_output, agent_state)\n",
        "        while True:\n",
        "            index = free_queue.get()\n",
        "            if index is None:\n",
        "                break\n",
        "\n",
        "            # Write old rollout end.\n",
        "            for key in env_output:\n",
        "                buffers[key][index][0, ...] = env_output[key]\n",
        "            for key in agent_output:\n",
        "                buffers[key][index][0, ...] = agent_output[key]\n",
        "            for i, tensor in enumerate(agent_state):\n",
        "                initial_agent_state_buffers[index][i][...] = tensor\n",
        "\n",
        "            # Do new rollout.\n",
        "            for t in range(flags.unroll_length):\n",
        "                with torch.no_grad():\n",
        "                    agent_output, agent_state = model(env_output, agent_state)\n",
        "\n",
        "                env_output = env.step(agent_output[\"action\"])\n",
        "\n",
        "                for key in env_output:\n",
        "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
        "                for key in agent_output:\n",
        "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
        "\n",
        "            full_queue.put(index)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        pass  # Return silently.\n",
        "    except Exception:\n",
        "        logging.error(\"Exception in worker process %i\", actor_index)\n",
        "        traceback.print_exc()\n",
        "        print()\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_batch(\n",
        "    flags,\n",
        "    free_queue: mp.SimpleQueue,\n",
        "    full_queue: mp.SimpleQueue,\n",
        "    buffers,\n",
        "    initial_agent_state_buffers,\n",
        "    lock=threading.Lock(),\n",
        "):\n",
        "    with lock:\n",
        "        indices = [full_queue.get() for _ in range(flags.batch_size)]\n",
        "    batch = {\n",
        "        key: torch.stack([buffers[key][m] for m in indices], dim=1) for key in buffers\n",
        "    }\n",
        "    initial_agent_state = (\n",
        "        torch.cat(ts, dim=1)\n",
        "        for ts in zip(*[initial_agent_state_buffers[m] for m in indices])\n",
        "    )\n",
        "    for m in indices:\n",
        "        free_queue.put(m)\n",
        "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
        "    initial_agent_state = tuple(\n",
        "        t.to(device=flags.device, non_blocking=True) for t in initial_agent_state\n",
        "    )\n",
        "    return batch, initial_agent_state\n",
        "\n",
        "\n",
        "def learn(\n",
        "    flags,\n",
        "    actor_model,\n",
        "    model,\n",
        "    batch,\n",
        "    initial_agent_state,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    lock=threading.Lock(),  # noqa: B008\n",
        "):\n",
        "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
        "    with lock:\n",
        "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
        "\n",
        "        # Take final value function slice for bootstrapping.\n",
        "        bootstrap_value = learner_outputs[\"baseline\"][-1]\n",
        "\n",
        "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
        "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
        "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
        "\n",
        "        rewards = batch[\"reward\"]\n",
        "\n",
        "\n",
        "        if flags.reward_clipping == \"abs_one\":\n",
        "            clipped_rewards = torch.clamp(rewards, -1, 1)\n",
        "        elif flags.reward_clipping == \"none\":\n",
        "            clipped_rewards = rewards\n",
        "\n",
        "        discounts = (~batch[\"done\"]).float() * flags.discounting\n",
        "\n",
        "        vtrace_returns = vtrace.from_logits(\n",
        "            behavior_policy_logits=batch[\"policy_logits\"],\n",
        "            target_policy_logits=learner_outputs[\"policy_logits\"],\n",
        "            actions=batch[\"action\"],\n",
        "            discounts=discounts,\n",
        "            rewards=clipped_rewards,\n",
        "            values=learner_outputs[\"baseline\"],\n",
        "            bootstrap_value=bootstrap_value,\n",
        "        )\n",
        "\n",
        "        pg_loss = compute_policy_gradient_loss(\n",
        "            learner_outputs[\"policy_logits\"],\n",
        "            batch[\"action\"],\n",
        "            vtrace_returns.pg_advantages,\n",
        "        )\n",
        "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
        "            vtrace_returns.vs - learner_outputs[\"baseline\"]\n",
        "        )\n",
        "        entropy_loss = flags.entropy_cost * compute_entropy_loss(\n",
        "            learner_outputs[\"policy_logits\"]\n",
        "        )\n",
        "\n",
        "        total_loss = pg_loss + baseline_loss + entropy_loss\n",
        "\n",
        "        # TODO\n",
        "        acts = learner_outputs[\"action\"]\n",
        "        elements, counts = torch.unique(acts, return_counts=True)\n",
        "        combined = list(zip([act_list[i] for i in elements.tolist()], counts.tolist()))\n",
        "        combined_ = sorted(combined, key=lambda x: x[1], reverse=True)\n",
        "        episode_returns = batch[\"episode_return\"][batch[\"done\"]]\n",
        "\n",
        "        stats = {\n",
        "            \"episode_returns\": tuple(episode_returns.cpu().numpy()[:10]),\n",
        "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
        "            \"total_loss\": total_loss.item(),\n",
        "            \"pg_loss\": pg_loss.item(),\n",
        "            \"baseline_loss\": baseline_loss.item(),\n",
        "            \"entropy_loss\": entropy_loss.item(),\n",
        "            #\"actions\": elements,\n",
        "            \"action_counts\": combined_[:30],\n",
        "            \"num_episodes\": episode_returns.shape[0]\n",
        "        }\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), flags.grad_norm_clipping)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        actor_model.load_state_dict(model.state_dict())\n",
        "        return stats\n",
        "\n",
        "\n",
        "def create_buffers(flags, observation_space, num_actions, num_overlapping_steps=1):\n",
        "    size = (flags.unroll_length + num_overlapping_steps,)\n",
        "\n",
        "    # Get specimens to infer shapes and dtypes.\n",
        "    samples = {k: torch.from_numpy(v) for k, v in observation_space.sample().items()}\n",
        "\n",
        "    specs = {\n",
        "        key: dict(size=size + sample.shape, dtype=sample.dtype)\n",
        "        for key, sample in samples.items()\n",
        "    }\n",
        "    specs.update(\n",
        "        reward=dict(size=size, dtype=torch.float32),\n",
        "        done=dict(size=size, dtype=torch.bool),\n",
        "        episode_return=dict(size=size, dtype=torch.float32),\n",
        "        episode_step=dict(size=size, dtype=torch.int32),\n",
        "        policy_logits=dict(size=size + (num_actions,), dtype=torch.float32),\n",
        "        baseline=dict(size=size, dtype=torch.float32),\n",
        "        last_action=dict(size=size, dtype=torch.int64),\n",
        "        action=dict(size=size, dtype=torch.int64),\n",
        "    )\n",
        "    buffers = {key: [] for key in specs}\n",
        "    for _ in range(flags.num_buffers):\n",
        "        for key in buffers:\n",
        "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
        "    return buffers\n",
        "\n",
        "\n",
        "def _format_observations(observation, keys=(\"glyphs\", \"blstats\", \"message\", \"inv_glyphs\", \"inv_letters\", \"inv_oclasses\", \"inv_strs\")):\n",
        "    observations = {}\n",
        "    for key in keys:\n",
        "        entry = observation[key]\n",
        "        entry = torch.from_numpy(entry)\n",
        "        entry = entry.view((1, 1) + entry.shape)  # (...) -> (T,B,...).\n",
        "        observations[key] = entry\n",
        "    return observations\n",
        "\n",
        "\n",
        "class ResettingEnvironment:\n",
        "    \"\"\"Turns a Gym environment into something that can be step()ed indefinitely.\"\"\"\n",
        "\n",
        "    def __init__(self, gym_env):\n",
        "        self.gym_env = gym_env\n",
        "        self.episode_return = None\n",
        "        self.episode_step = None\n",
        "        self.depth = 1\n",
        "        self.hungry = 1\n",
        "        self.glyphs = None\n",
        "        self.glyphs_sum = None\n",
        "        self.attr_index = [3, 4, 5, 6, 7, 8, 11, 15, 18]\n",
        "        self.attributes = None\n",
        "        self.AC = None\n",
        "        self.T = 1\n",
        "        self.frozen_init = -3\n",
        "        self.loop_init = -20\n",
        "        self.frozen_step = self.frozen_init\n",
        "        self.loop_step = self.loop_init\n",
        "\n",
        "\n",
        "\n",
        "    def initial(self):\n",
        "        initial_reward = torch.zeros(1, 1)\n",
        "        # This supports only single-tensor actions ATM.\n",
        "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
        "        self.episode_return = torch.zeros(1, 1)\n",
        "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
        "        initial_done = torch.ones(1, 1, dtype=torch.uint8)\n",
        "        obs, reset_info = self.gym_env.reset()\n",
        "        self.depth = obs['blstats'][12]\n",
        "        self.hungry = obs['blstats'][21]\n",
        "        self.glyphs = np.count_nonzero(obs['glyphs'] == 2359)\n",
        "        self.glyphs_sum = np.sum(obs['glyphs'])\n",
        "        self.attributes = sum(obs['blstats'][pos] for pos in self.attr_index)\n",
        "        self.AC = obs['blstats'][16]\n",
        "\n",
        "        self.T = obs['blstats'][20]\n",
        "        self.frozen_step = self.frozen_init\n",
        "        self.loop_step = self.loop_init\n",
        "        result = _format_observations(obs)\n",
        "        result.update(\n",
        "            reward=initial_reward,\n",
        "            done=initial_done,\n",
        "            episode_return=self.episode_return,\n",
        "            episode_step=self.episode_step,\n",
        "            last_action=initial_last_action,\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, done, truncated, unused_info = self.gym_env.step(\n",
        "            action.item()\n",
        "        )\n",
        "        self.episode_step += 1\n",
        "\n",
        "        self.episode_return += reward\n",
        "        episode_step = self.episode_step\n",
        "        episode_return = self.episode_return\n",
        "\n",
        "        ### reward shaping\n",
        "        # + gain item\n",
        "        # + gain non-crop food\n",
        "        # - CC diff\n",
        "        # + lv diff\n",
        "        # - in trap\n",
        "        #reward_ = torch.tensor(reward).view(1, 1)\n",
        "\n",
        "        # origin 4*exp 10\n",
        "        reward = np.tanh(reward)*2\n",
        "\n",
        "        # attributes 3 4 5 6 7 8 / 11 13 15 16 18 HP Gold Mana AC(negative) lvl -----------------------------------------\n",
        "        attributes = sum(observation['blstats'][pos] for pos in self.attr_index)\n",
        "        reward += max(0,attributes-self.attributes)*2\n",
        "        self.attributes = max(attributes,self.attributes)\n",
        "        # AC\n",
        "        AC = observation['blstats'][16]\n",
        "        reward += max(0,self.AC-AC)*2\n",
        "        self.AC = min(AC,self.AC)\n",
        "\n",
        "        # hungry 21\n",
        "        hungry = observation['blstats'][21]\n",
        "        reward += max(0,self.hungry-hungry)*3\n",
        "        self.hungry = hungry\n",
        "\n",
        "        # discover 2359\n",
        "        glyphs = np.count_nonzero(observation['glyphs'] == 2359)\n",
        "        glyphs_diff = np.tanh(self.glyphs-glyphs)\n",
        "        reward += max(0,glyphs_diff)\n",
        "        self.glyphs = glyphs\n",
        "\n",
        "        # depth 12\n",
        "        depth = observation['blstats'][12]\n",
        "        reward += max(0,depth-self.depth-glyphs_diff)*5\n",
        "        self.depth = max(depth,self.depth)\n",
        "\n",
        "        # time 20\n",
        "        T = observation['blstats'][20]\n",
        "        if T == self.T:\n",
        "            self.frozen_step += 1\n",
        "        else:\n",
        "            self.frozen_step = self.frozen_init\n",
        "        reward -= max(0,self.frozen_step)*0.001\n",
        "        self.T = T\n",
        "\n",
        "        # loop\n",
        "        glyphs_sum = np.sum(observation['glyphs'])\n",
        "        if glyphs_sum == self.glyphs_sum:\n",
        "            self.loop_step += 1\n",
        "        else:\n",
        "            self.loop_step = self.loop_init\n",
        "        reward -= max(0,self.frozen_step)*0.001\n",
        "        self.glyphs_sum = glyphs_sum\n",
        "\n",
        "        reward = np.tanh(reward)\n",
        "        # print(f\"reward is {reward}\"\n",
        "        ###\n",
        "\n",
        "        if done:\n",
        "            observation, reset_info = self.gym_env.reset()\n",
        "            self.episode_return = torch.zeros(1, 1)\n",
        "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
        "            self.depth = 1\n",
        "            self.hungry = 1\n",
        "            self.glyphs = np.count_nonzero(observation['glyphs'] == 2359)\n",
        "            self.glyphs_sum = np.sum(observation['glyphs'])\n",
        "            self.attributes = sum(observation['blstats'][pos] for pos in self.attr_index)\n",
        "            self.AC = observation['blstats'][16]\n",
        "            self.T = observation['blstats'][20]\n",
        "            self.frozen_step = self.frozen_init\n",
        "            self.loop_step = self.loop_init\n",
        "\n",
        "        result = _format_observations(observation)\n",
        "\n",
        "        reward = torch.tensor(reward).view(1, 1)\n",
        "        done = torch.tensor(done).view(1, 1)\n",
        "\n",
        "        result.update(\n",
        "            reward=reward,\n",
        "            #reward_=reward_,\n",
        "            done=done,\n",
        "            episode_return=episode_return,\n",
        "            episode_step=episode_step,\n",
        "            last_action=action,\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    def close(self):\n",
        "        self.gym_env.close()\n",
        "\n",
        "\n",
        "def train(flags):  # pylint: disable=too-many-branches, too-many-statements\n",
        "    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n",
        "\n",
        "    rundir = os.path.join(\n",
        "        flags.savedir, \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    )\n",
        "\n",
        "    if not os.path.exists(rundir):\n",
        "        os.makedirs(rundir)\n",
        "    logging.info(\"Logging results to %s\", rundir)\n",
        "\n",
        "    symlink = os.path.join(flags.savedir, \"latest\")\n",
        "    try:\n",
        "        if os.path.islink(symlink):\n",
        "            os.remove(symlink)\n",
        "        if not os.path.exists(symlink):\n",
        "            os.symlink(rundir, symlink)\n",
        "        logging.info(\"Symlinked log directory: %s\", symlink)\n",
        "    except OSError:\n",
        "        raise\n",
        "\n",
        "    logfile = open(os.path.join(rundir, \"logs.tsv\"), \"a\", buffering=1)\n",
        "\n",
        "    checkpointpath = os.path.join(rundir, \"model.tar\")\n",
        "\n",
        "    flags.rundir = rundir\n",
        "\n",
        "    if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
        "        flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
        "    if flags.num_actors >= flags.num_buffers:\n",
        "        raise ValueError(\"num_buffers should be larger than num_actors\")\n",
        "    if flags.num_buffers < flags.batch_size:\n",
        "        raise ValueError(\"num_buffers should be larger than batch_size\")\n",
        "\n",
        "    T = flags.unroll_length\n",
        "    B = flags.batch_size\n",
        "\n",
        "    flags.device = None\n",
        "    if not flags.disable_cuda and torch.cuda.is_available():\n",
        "        logging.info(\"Using CUDA.\")\n",
        "        flags.device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        logging.info(\"Not using CUDA.\")\n",
        "        flags.device = torch.device(\"cpu\")\n",
        "\n",
        "    env = create_env(flags.env)\n",
        "    observation_space = env.observation_space\n",
        "    action_space = env.action_space\n",
        "    del env  # End this before forking.\n",
        "\n",
        "    model = Net(observation_space, action_space.n, flags.use_lstm)\n",
        "\n",
        "    # load model #\n",
        "    if load_model:\n",
        "        ckpt = torch.load(f_dir, map_location=\"cpu\")\n",
        "        state_dict = ckpt[\"model_state_dict\"]\n",
        "\n",
        "        # if 'act_mask' in state_dict:\n",
        "        #     del state_dict['act_mask']\n",
        "\n",
        "        model.load_state_dict(state_dict,)\n",
        "        model.reset_act_mask(act_mask)\n",
        "\n",
        "\n",
        "    buffers = create_buffers(flags, observation_space, model.num_actions)\n",
        "\n",
        "    model.share_memory()\n",
        "\n",
        "    # Add initial RNN state.\n",
        "    initial_agent_state_buffers = []\n",
        "    for _ in range(flags.num_buffers):\n",
        "        state = model.initial_state(batch_size=1)\n",
        "        for t in state:\n",
        "            t.share_memory_()\n",
        "        initial_agent_state_buffers.append(state)\n",
        "\n",
        "    actor_processes = []\n",
        "    ctx = mp.get_context(\"fork\")\n",
        "    free_queue = ctx.SimpleQueue()\n",
        "    full_queue = ctx.SimpleQueue()\n",
        "\n",
        "    for i in range(flags.num_actors):\n",
        "        actor = ctx.Process(\n",
        "            target=act,\n",
        "            args=(\n",
        "                flags,\n",
        "                i,\n",
        "                free_queue,\n",
        "                full_queue,\n",
        "                model,\n",
        "                buffers,\n",
        "                initial_agent_state_buffers,\n",
        "            ),\n",
        "            name=\"Actor-%i\" % i,\n",
        "        )\n",
        "        actor.start()\n",
        "        actor_processes.append(actor)\n",
        "\n",
        "    learner_model = Net(observation_space, action_space.n, flags.use_lstm).to(\n",
        "        device=flags.device\n",
        "    )\n",
        "    learner_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.RMSprop(\n",
        "        learner_model.parameters(),\n",
        "        lr=flags.learning_rate,\n",
        "        momentum=flags.momentum,\n",
        "        eps=flags.epsilon,\n",
        "        alpha=flags.alpha,\n",
        "    )\n",
        "\n",
        "    # optimizer = torch.optim.Adam(\n",
        "    #     learner_model.parameters(),\n",
        "    #     lr=flags.learning_rate,\n",
        "    #     eps=flags.epsilon,\n",
        "    # )\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    if load_model:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
        "        last_epoch = ckpt[\"scheduler_state_dict\"][\"last_epoch\"]\n",
        "        step = last_epoch*T*B\n",
        "    else:\n",
        "        step = 0\n",
        "\n",
        "    stat_keys = [\n",
        "        \"total_loss\",\n",
        "        \"mean_episode_return\",\n",
        "        \"pg_loss\",\n",
        "        \"baseline_loss\",\n",
        "        \"entropy_loss\",\n",
        "        \"num_episodes\",\n",
        "    ]\n",
        "    logfile.write(\"# Step\\t%s\\n\" % \"\\t\".join(stat_keys))\n",
        "\n",
        "    step_, stats = 0, {}\n",
        "\n",
        "    def batch_and_learn(i, lock=threading.Lock()):\n",
        "        \"\"\"Thread target for the learning process.\"\"\"\n",
        "        nonlocal step, step_, stats\n",
        "        while step_ < flags.total_steps_:\n",
        "            batch, agent_state = get_batch(\n",
        "                flags, free_queue, full_queue, buffers, initial_agent_state_buffers\n",
        "            )\n",
        "\n",
        "            stats = learn(\n",
        "                flags, model, learner_model, batch, agent_state, optimizer, scheduler\n",
        "            )\n",
        "            with lock:\n",
        "                logfile.write(\"%i\\t\" % step)\n",
        "                logfile.write(\"\\t\".join(str(stats[k]) for k in stat_keys))\n",
        "                logfile.write(\"\\n\")\n",
        "                step += T * B\n",
        "                step_ += T * B\n",
        "\n",
        "    for m in range(flags.num_buffers):\n",
        "        free_queue.put(m)\n",
        "\n",
        "    threads = []\n",
        "    for i in range(flags.num_learner_threads):\n",
        "        thread = threading.Thread(\n",
        "            target=batch_and_learn,\n",
        "            name=\"batch-and-learn-%d\" % i,\n",
        "            args=(i,),\n",
        "            daemon=True,  # To support KeyboardInterrupt below.\n",
        "        )\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "    def checkpoint():\n",
        "        if flags.disable_checkpoint:\n",
        "            return\n",
        "        logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "                \"flags\": vars(flags),\n",
        "            },\n",
        "            checkpointpath,\n",
        "        )\n",
        "\n",
        "    timer = timeit.default_timer\n",
        "    try:\n",
        "        last_checkpoint_time = timer()\n",
        "        start_step = 0 ####\n",
        "        while step_ < flags.total_steps_:\n",
        "\n",
        "            if start_step == step_: ####\n",
        "                continue\n",
        "            else:\n",
        "                start_step = step_\n",
        "            start_time = timer()\n",
        "            time.sleep(log_time)\n",
        "\n",
        "            if timer() - last_checkpoint_time > 30 * 60:  # Save every 10 min.\n",
        "                checkpoint()\n",
        "                last_checkpoint_time = timer()\n",
        "                source_path = os.path.join(rundir, \"logs.tsv\")\n",
        "                destination_path = os.path.join(rundir, \"logs_.tsv\")\n",
        "                shutil.copy(source_path, destination_path)\n",
        "\n",
        "            #sps = (step - start_step) / (timer() - start_time)\n",
        "            sps = timer()\n",
        "            if stats.get(\"episode_returns\", None):\n",
        "                mean_return = (\n",
        "                    \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
        "                )\n",
        "            else:\n",
        "                mean_return = \"\"\n",
        "            total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
        "            logging.info(\n",
        "                \"Steps %i ( %i of %i ) @ %.1f SPS. Loss %f. %sStats:\\n%s\",\n",
        "                step_,\n",
        "                step,\n",
        "                flags.total_steps,\n",
        "                sps,\n",
        "                total_loss,\n",
        "                mean_return,\n",
        "                pprint.pformat(stats),\n",
        "            )\n",
        "    except KeyboardInterrupt:\n",
        "        logging.warning(\"Quitting.\")\n",
        "        return  # Try joining actors then quit.\n",
        "    else:\n",
        "        for thread in threads:\n",
        "            thread.join()\n",
        "        logging.info(\"Learning finished after %d steps.\", step)\n",
        "    finally:\n",
        "        for _ in range(flags.num_actors):\n",
        "            free_queue.put(None)\n",
        "        for actor in actor_processes:\n",
        "            actor.join(timeout=1)\n",
        "\n",
        "    checkpoint()\n",
        "    logfile.close()\n",
        "\n",
        "\n",
        "def test(flags, num_episodes=10):\n",
        "    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n",
        "    checkpointpath = os.path.join(flags.savedir, \"GAT_TRAN_test_4\", \"model.tar\")\n",
        "\n",
        "    gym_env = create_env(flags.env, save_ttyrec_every=flags.save_ttyrec_every)\n",
        "    env = ResettingEnvironment(gym_env)\n",
        "    model = Net(gym_env.observation_space, gym_env.action_space.n, flags.use_lstm)\n",
        "    model.eval()\n",
        "    checkpoint = torch.load(checkpointpath, map_location=\"cpu\")\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    observation = env.initial()\n",
        "    returns = []\n",
        "\n",
        "    agent_state = model.initial_state(batch_size=1)\n",
        "\n",
        "    while len(returns) < num_episodes:\n",
        "        if flags.mode == \"test_render\":\n",
        "            env.gym_env.render()\n",
        "        policy_outputs, agent_state = model(observation, agent_state)\n",
        "        observation = env.step(policy_outputs[\"action\"])\n",
        "        if observation[\"done\"].item():\n",
        "            returns.append(observation[\"episode_return\"].item())\n",
        "            logging.info(\n",
        "                \"Episode ended after %d steps. Return: %.1f\",\n",
        "                observation[\"episode_step\"].item(),\n",
        "                observation[\"episode_return\"].item(),\n",
        "            )\n",
        "    env.close()\n",
        "    logging.info(\n",
        "        \"Average returns over %i steps: %.1f\", num_episodes, sum(returns) / len(returns)\n",
        "    )\n",
        "\n",
        "\n",
        "class RandomNet(nn.Module):\n",
        "    def __init__(self, observation_shape, num_actions, use_lstm):\n",
        "        super(RandomNet, self).__init__()\n",
        "        del observation_shape, use_lstm\n",
        "        self.num_actions = num_actions\n",
        "        self.theta = torch.nn.Parameter(torch.zeros(self.num_actions))\n",
        "\n",
        "    def forward(self, inputs, core_state):\n",
        "        # print(inputs)\n",
        "        T, B, *_ = inputs[\"observation\"].shape\n",
        "        zeros = self.theta * 0\n",
        "        # set logits to 0\n",
        "        policy_logits = zeros[None, :].expand(T * B, -1)\n",
        "        # set baseline to 0\n",
        "        baseline = policy_logits.sum(dim=1).view(-1, B)\n",
        "\n",
        "        # sample random action\n",
        "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1).view(\n",
        "            T, B\n",
        "        )\n",
        "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
        "        return (\n",
        "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
        "            core_state,\n",
        "        )\n",
        "\n",
        "    def initial_state(self, batch_size):\n",
        "        return ()\n",
        "\n",
        "\n",
        "def _step_to_range(delta, num_steps):\n",
        "    \"\"\"Range of `num_steps` integers with distance `delta` centered around zero.\"\"\"\n",
        "    return delta * torch.arange(-num_steps // 2, num_steps // 2)\n",
        "\n",
        "\n",
        "class Crop(nn.Module):\n",
        "    \"\"\"Helper class for NetHackNet below.\"\"\"\n",
        "\n",
        "    def __init__(self, height, width, height_target, width_target):\n",
        "        super(Crop, self).__init__()\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.width_target = width_target\n",
        "        self.height_target = height_target\n",
        "        width_grid = _step_to_range(2 / (self.width - 1), self.width_target)[\n",
        "            None, :\n",
        "        ].expand(self.height_target, -1)\n",
        "        height_grid = _step_to_range(2 / (self.height - 1), height_target)[\n",
        "            :, None\n",
        "        ].expand(-1, self.width_target)\n",
        "\n",
        "        # \"clone\" necessary, https://github.com/pytorch/pytorch/issues/34880\n",
        "        self.register_buffer(\"width_grid\", width_grid.clone())\n",
        "        self.register_buffer(\"height_grid\", height_grid.clone())\n",
        "\n",
        "    def forward(self, inputs, coordinates):\n",
        "        \"\"\"Calculates centered crop around given x,y coordinates.\n",
        "        Args:\n",
        "           inputs [B x H x W]\n",
        "           coordinates [B x 2] x,y coordinates\n",
        "        Returns:\n",
        "           [B x H' x W'] inputs cropped and centered around x,y coordinates.\n",
        "        \"\"\"\n",
        "        assert inputs.shape[1] == self.height\n",
        "        assert inputs.shape[2] == self.width\n",
        "\n",
        "        inputs = inputs[:, None, :, :].float()\n",
        "\n",
        "        x = coordinates[:, 0]+1\n",
        "        y = coordinates[:, 1]+1\n",
        "\n",
        "        x_shift = 2 / (self.width - 1) * (x.float() - self.width // 2)\n",
        "        y_shift = 2 / (self.height - 1) * (y.float() - self.height // 2)\n",
        "\n",
        "        grid = torch.stack(\n",
        "            [\n",
        "                self.width_grid[None, :, :] + x_shift[:, None, None],\n",
        "                self.height_grid[None, :, :] + y_shift[:, None, None],\n",
        "            ],\n",
        "            dim=3,\n",
        "        )\n",
        "\n",
        "        # TODO: only cast to int if original tensor was int\n",
        "        return (\n",
        "            torch.round(F.grid_sample(inputs, grid, align_corners=True))\n",
        "            .squeeze(1)\n",
        "            .long()\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BswoL49WBxye"
      },
      "source": [
        "# relational"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki7s-mVIB2pK"
      },
      "outputs": [],
      "source": [
        "# 'chars': Box(0, 255, (21, 79), uint8), 'colors': Box(0, 15, (21, 79), uint8), 'specials': Box(0, 255, (21, 79), uint8),\n",
        "# 'inv_glyphs': Box(0, 5976, (55,), int16), 'inv_letters': Box(0, 127, (55,), uint8), 'inv_oclasses': Box(0, 18, (55,), uint8), 'inv_strs': Box(0, 255, (55, 80), uint8),\n",
        "# 'chars','colors','specials'\n",
        "from torch.nn import Linear, LayerNorm\n",
        "import numpy as np\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "class NetHackNet_GAT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_shape,\n",
        "        num_actions,\n",
        "        use_lstm,\n",
        "        embedding_dim=32,\n",
        "        crop_dim=13,\n",
        "        num_layers=5,\n",
        "    ):\n",
        "        super(NetHackNet_GAT, self).__init__()\n",
        "\n",
        "        self.act_mask = nn.Parameter((torch.tensor(act_mask, dtype=torch.float32)-1)*1e9, requires_grad=False)\n",
        "\n",
        "\n",
        "        BLSTAT_NORMALIZATION_STATS = [[\n",
        "            1.0 / 79.0, # hero col\n",
        "            1.0 / 21, # hero row\n",
        "            0.0, # strength pct\n",
        "            1.0 / 10, # strength\n",
        "            1.0 / 10, # dexterity\n",
        "            1.0 / 10, # constitution\n",
        "            1.0 / 10, # intelligence\n",
        "            1.0 / 10, # wisdom\n",
        "            1.0 / 10, # charisma\n",
        "            0.0,      # score\n",
        "            1.0 / 10, # hitpoints\n",
        "            1.0 / 10, # max hitpoints\n",
        "            1.0, # depth\n",
        "            1.0 / 1000, # gold\n",
        "            1.0 / 10, # energy\n",
        "            1.0 / 10, # max energy\n",
        "            1.0 / 10, # armor class\n",
        "            0.0, # monster level\n",
        "            1.0 / 10, # experience level\n",
        "            1.0 / 100, # experience points\n",
        "            1.0 / 1000, # time\n",
        "            1.0, # hunger_state\n",
        "            1.0 / 10, # carrying capacity\n",
        "            0.0, # dungeon number\n",
        "            0.0, # level number\n",
        "            0.0, # condition bits\n",
        "            0.0, # character alignment\n",
        "            ]]\n",
        "        self.blstats_scale = nn.Parameter(torch.tensor(BLSTAT_NORMALIZATION_STATS, dtype=torch.float32))\n",
        "        self.BLSTAT_CLIP_RANGE = (0, 5)\n",
        "\n",
        "        self.glyph_shape = observation_shape[\"glyphs\"].shape\n",
        "        self.blstats_size = observation_shape[\"blstats\"].shape[0]\n",
        "        self.message_size = observation_shape[\"message\"].shape[0]\n",
        "        self.inv_shape = observation_shape[\"inv_strs\"].shape\n",
        "        self.timer = timeit.default_timer\n",
        "        self.t = self.timer()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.base_model = False # not(use_lstm)\n",
        "        self.use_message = True\n",
        "        self.use_inv = True\n",
        "\n",
        "        self.H = self.glyph_shape[0]\n",
        "        self.W = self.glyph_shape[1]\n",
        "\n",
        "        self.k_dim = embedding_dim  # glyph_dim\n",
        "        self.s_dim = 128 # 128    blstats_dim\n",
        "        self.att_fc_h_dim = 256\n",
        "        self.att_fc_out_dim = 512   # 1024\n",
        "        self.h_dim = 512   # 1024\n",
        "        self.cnn_out_dim = self.s_dim   # 128\n",
        "\n",
        "        self.crop_dim = crop_dim\n",
        "\n",
        "\n",
        "        #--------------inv emb---------------#\n",
        "\n",
        "        if self.use_inv:\n",
        "            self.inv_embed = nn.Embedding(nethack.MAX_GLYPH+1, self.k_dim)\n",
        "\n",
        "            self.inv_emb_norm = nn.LayerNorm(self.k_dim)\n",
        "\n",
        "            self.embed_inv = nn.Sequential(\n",
        "                nn.Linear(self.k_dim+self.inv_shape[-1]+1+19, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, self.s_dim),\n",
        "                nn.ReLU(),\n",
        "                )\n",
        "            self.inv_norm = nn.LayerNorm(self.s_dim)\n",
        "\n",
        "        #--------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "        #--------------CNN crop---------------#\n",
        "        K = embedding_dim  # number of input filters\n",
        "        F = 3  # filter dimensions\n",
        "        S = 1  # stride\n",
        "        P = 1  # padding\n",
        "        M = 128  # number of intermediate filters   16\n",
        "        Y = self.cnn_out_dim  # number of output filters  8\n",
        "        L = num_layers  # number of convnet layers\n",
        "\n",
        "        self.crop = Crop(self.H, self.W, self.crop_dim, self.crop_dim)\n",
        "        in_channels = [K] + [M] * (L - 1)\n",
        "        out_channels = [M] * (L - 1) + [Y]\n",
        "        def interleave(xs, ys):\n",
        "            return [val for pair in zip(xs, ys) for val in pair]\n",
        "        self.embed = nn.Embedding(nethack.MAX_GLYPH, self.k_dim)\n",
        "        conv_extract_crop = [\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels[i],\n",
        "                out_channels=out_channels[i],\n",
        "                kernel_size=(F, F),\n",
        "                stride=S,\n",
        "                padding=P,\n",
        "            )\n",
        "            for i in range(L)\n",
        "        ]\n",
        "\n",
        "        self.extract_crop_representation = nn.Sequential(\n",
        "            *interleave(conv_extract_crop, [nn.ELU()] * len(conv_extract_crop))\n",
        "        )\n",
        "        self.CNN_residual_norm = nn.LayerNorm(Y)\n",
        "\n",
        "        # self.conv_pool = nn.MaxPool2d(self.crop_dim,5)\n",
        "        # self.CNN_residual_norm2 = nn.LayerNorm(Y)\n",
        "        #--------------------------------------#\n",
        "\n",
        "        #---------------baseCNN----------------#\n",
        "        if self.base_model:\n",
        "            in_channels_ = [self.cnn_out_dim] + [32] * 4\n",
        "            out_channels_ = [32] * 4 + [self.cnn_out_dim]\n",
        "            base_conv = [\n",
        "                nn.Conv2d(\n",
        "                    in_channels=in_channels_[i],\n",
        "                    out_channels=out_channels_[i],\n",
        "                    kernel_size=(3, 3),\n",
        "                    stride=1,\n",
        "                    padding=1,\n",
        "                )\n",
        "                for i in range(len(in_channels_))\n",
        "            ]\n",
        "\n",
        "            self.base_conv_representation = nn.Sequential(\n",
        "                *interleave(base_conv, [nn.ELU()] * len(base_conv))\n",
        "            )\n",
        "            out_dim = self.att_fc_out_dim + self.s_dim\n",
        "        #--------------------------------------#\n",
        "\n",
        "        #----------------ATT-------------------#\n",
        "        else:\n",
        "            # encoder_layer = nn.TransformerEncoderLayer(d_model=self.cnn_out_dim, nhead=2, batch_first=True)\n",
        "            # self.attention = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "            self.attention = nn.MultiheadAttention(embed_dim=self.cnn_out_dim, num_heads=2, dropout=0.1, batch_first=True)\n",
        "            self.attention_mlp = nn.Sequential(\n",
        "                nn.Linear(self.cnn_out_dim, self.att_fc_h_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.att_fc_h_dim, self.cnn_out_dim),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "            self.att_norm1 = nn.LayerNorm(self.cnn_out_dim)\n",
        "            self.att_norm2 = nn.LayerNorm(self.cnn_out_dim)\n",
        "            self.att_norm3 = nn.LayerNorm(self.cnn_out_dim)\n",
        "            self.att_norm4 = nn.LayerNorm(self.cnn_out_dim)\n",
        "            self.dropout1 = nn.Dropout(0.1)\n",
        "            self.dropout2 = nn.Dropout(0.1)\n",
        "\n",
        "            out_dim = self.att_fc_out_dim\n",
        "            self.att_fc = nn.Sequential(\n",
        "                nn.Linear(self.cnn_out_dim, self.att_fc_h_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.att_fc_h_dim, self.att_fc_out_dim),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "\n",
        "        #--------------------------------------#\n",
        "\n",
        "        if self.use_message:\n",
        "            self.embed_message = nn.Sequential(\n",
        "                nn.Linear(self.message_size, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, self.s_dim),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "            self.message_norm = nn.LayerNorm(self.s_dim)\n",
        "\n",
        "        self.action_embed = nn.Embedding(self.num_actions, self.s_dim)\n",
        "        self.embed_action = nn.Sequential(\n",
        "            nn.Linear(self.s_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.s_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.embed_blstats = nn.Sequential(\n",
        "            nn.Linear(self.blstats_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.s_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.att_norm = nn.LayerNorm(self.att_fc_out_dim)\n",
        "        self.blstats_norm = nn.LayerNorm(self.s_dim)\n",
        "        self.action_norm = nn.LayerNorm(self.s_dim)\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(out_dim, self.h_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.h_dim, self.h_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.core = nn.LSTM(self.h_dim, self.h_dim, num_layers=1)\n",
        "\n",
        "        self.policy = nn.Linear(self.h_dim, self.num_actions)\n",
        "        self.baseline = nn.Linear(self.h_dim, 1)\n",
        "\n",
        "    def initial_state(self, batch_size=1):\n",
        "        return tuple(\n",
        "            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)\n",
        "            for _ in range(2)\n",
        "        )\n",
        "    def reset_act_mask(self,a_mask=None):\n",
        "        if a_mask is None:\n",
        "            self.act_mask = None\n",
        "        else:\n",
        "            self.act_mask = nn.Parameter((torch.tensor(a_mask, dtype=torch.float32)-1)*1e9, requires_grad=False)\n",
        "\n",
        "    def _select(self, embed, x):\n",
        "        # Work around slow backward pass of nn.Embedding, see\n",
        "        # https://github.com/pytorch/pytorch/issues/24912\n",
        "        out = embed.weight.index_select(0, x.reshape(-1))\n",
        "        return out.reshape(x.shape + (-1,))\n",
        "\n",
        "    def forward(self, env_outputs, core_state):\n",
        "        # print(env_outputs)\n",
        "        # time.sleep(100)\n",
        "\n",
        "        # -- [T x B x H x W]\n",
        "        glyphs = env_outputs[\"glyphs\"]\n",
        "        T, B, *_ = glyphs.shape\n",
        "\n",
        "        # -- [T x B x F]\n",
        "        blstats = env_outputs[\"blstats\"]\n",
        "        # -- [B' x F]\n",
        "        blstats = blstats.view(T * B, -1).float()\n",
        "        coordinates = blstats[:, :2]\n",
        "\n",
        "        # -- [T x B x 1]\n",
        "        last_actions = env_outputs[\"last_action\"]\n",
        "\n",
        "        #----------blstats+action---------#\n",
        "        # -- [B' x 1]\n",
        "        last_actions = last_actions.view(T * B, -1)\n",
        "        # -- [B' x emb]\n",
        "        action_emb_ = self.action_embed(last_actions)\n",
        "        action_emb = self.embed_action(action_emb_)\n",
        "\n",
        "        ## scale\n",
        "        blstats = blstats * self.blstats_scale\n",
        "        blstats = torch.clamp(blstats, min=self.BLSTAT_CLIP_RANGE[0], max=self.BLSTAT_CLIP_RANGE[1])\n",
        "\n",
        "        ##\n",
        "\n",
        "        # -- [B' x 27]\n",
        "        #blstats = torch.log1p(torch.nn.functional.relu(blstats))\n",
        "\n",
        "\n",
        "        # -- [B' x 1+27]\n",
        "        #blstats_action = torch.cat([action_emb.squeeze(1),blstats],dim=1)\n",
        "\n",
        "        # -- [B' x K''']\n",
        "        blstats_emb = self.embed_blstats(blstats)\n",
        "\n",
        "        assert blstats_emb.shape[0] == T * B\n",
        "        #----------------------------------#\n",
        "\n",
        "        #----------glyphs CNN-------------#\n",
        "        # -- [B' x H x W]\n",
        "        glyphs = torch.flatten(glyphs, 0, 1)  # Merge time and batch.\n",
        "        # -- [B' x H x W]\n",
        "        glyphs = glyphs.long()\n",
        "        # -- [B' x H' x W']\n",
        "        crop = self.crop(glyphs, coordinates)\n",
        "        # -- [B' x H' x W' x K]\n",
        "        crop_emb = self._select(self.embed, crop)\n",
        "        # CNN crop model.\n",
        "        # -- [B' x K x W' x H']\n",
        "        crop_emb = crop_emb.transpose(1, 3)  # -- TODO: slow?\n",
        "        # -- [B' x K' x W' x H']\n",
        "        crop_rep = self.extract_crop_representation(crop_emb)\n",
        "        #--------------------------------------#\n",
        "\n",
        "        ## full\n",
        "        # glyphs_emb = self._select(self.embed, glyphs)\n",
        "        # glyphs_emb = glyphs_emb.transpose(1, 3)\n",
        "        # glyphs_rep_  = self.extract_crop_representation(glyphs_emb)\n",
        "        # glyphs_rep = self.conv_pool(glyphs_rep_)\n",
        "        ##\n",
        "\n",
        "        # crop_rep_ += crop_emb\n",
        "        # crop_rep_ = crop_rep_.transpose(1, 3)\n",
        "        # crop_rep = self.CNN_residual_norm(crop_rep_)\n",
        "        # # -- [B' x K' x W' x H']\n",
        "        # crop_rep = crop_rep.transpose(1, 3)\n",
        "\n",
        "\n",
        "        #-------------base CNN----------------#\n",
        "        if self.base_model:\n",
        "            # # -- [B' x K' x W' x H']\n",
        "            observation_att = self.base_conv_representation(crop_rep)\n",
        "            observation_att = observation_att.view(T * B, -1, self.cnn_out_dim)\n",
        "            observation_rep_, max_ind = torch.max(observation_att,dim=1)\n",
        "            # -- [B' x K']\n",
        "            observation_rep = self.att_fc(observation_rep_)\n",
        "\n",
        "            reps =[self.att_norm(observation_rep)]\n",
        "            reps.append(self.blstats_norm(blstats_emb))\n",
        "            reps.append(self.message_norm(message_emb))\n",
        "\n",
        "            st = torch.cat(reps, dim=1)\n",
        "            # -- [B x K]\n",
        "            st = self.fc(st)\n",
        "\n",
        "        #--------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #----------------ATT-------------------#\n",
        "        else:\n",
        "            # -- [B' x W'H' x K']\n",
        "            crop_rep = crop_rep.reshape(T * B, -1, self.cnn_out_dim)\n",
        "            assert crop_rep.shape[0] == T * B\n",
        "            # glyphs_rep = glyphs_rep.reshape(T * B, -1, self.cnn_out_dim)\n",
        "            # assert glyphs_rep.shape[0] == T * B\n",
        "\n",
        "            reps = [self.CNN_residual_norm(crop_rep)]\n",
        "            # reps.append(self.CNN_residual_norm2(glyphs_rep))\n",
        "            reps.append(self.blstats_norm(blstats_emb).unsqueeze(1))\n",
        "            reps.append(self.action_norm(action_emb))\n",
        "\n",
        "            if self.use_inv:\n",
        "                inv_glyphs = env_outputs[\"inv_glyphs\"]\n",
        "                inv_glyphs_emb = self.inv_embed(inv_glyphs.long())\n",
        "                inv_glyphs_emb = self.inv_emb_norm(inv_glyphs_emb)\n",
        "                inv_letters = env_outputs[\"inv_letters\"]/127\n",
        "                inv_letters = inv_letters.unsqueeze(-1)\n",
        "                inv_oclasses = torch.nn.functional.one_hot(env_outputs[\"inv_oclasses\"].long(),19)\n",
        "                inv_strs = env_outputs[\"inv_strs\"]/255\n",
        "\n",
        "                inv_emb_ = torch.cat((inv_glyphs_emb, inv_letters, inv_oclasses, inv_strs), dim=-1)\n",
        "                inv_emb = self.embed_inv(inv_emb_)\n",
        "                inv_emb = self.inv_norm(inv_emb)\n",
        "                inv_emb = torch.flatten(inv_emb, 0, 1)\n",
        "                reps.append(inv_emb)\n",
        "\n",
        "            if self.use_message:\n",
        "                # -- [T x B x F]\n",
        "                message = env_outputs[\"message\"]\n",
        "                # -- [B' x F]\n",
        "                message = message.view(T * B, -1).float()\n",
        "                message_emb = self.embed_message(message/255)\n",
        "                reps.append(self.message_norm(message_emb).unsqueeze(1))\n",
        "\n",
        "            # -- [B' x W'H'+3 x K']\n",
        "            crop_rep_ = torch.cat(reps, dim=1)\n",
        "\n",
        "            # -- [B' x W'H' x K']\n",
        "            #observation_att = self.attention(crop_rep_)\n",
        "\n",
        "            observation_att_, att_w = self.attention(crop_rep_, crop_rep_, crop_rep_)\n",
        "            observation_att = self.att_norm1(self.dropout1(observation_att_) + crop_rep_)\n",
        "            observation_att_mlp = self.att_norm2(self.attention_mlp(observation_att)+observation_att)\n",
        "            #observation_att_2, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n",
        "            observation_att = self.att_norm3(self.dropout2(observation_att_mlp) + observation_att_)\n",
        "            observation_att_out = self.att_norm4(self.attention_mlp(observation_att)+observation_att)\n",
        "            ## back\n",
        "            # observation_att_, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n",
        "            # observation_att_ = self.att_norm1(self.dropout1(observation_att_) + crop_rep_)\n",
        "            # #observation_att_2, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n",
        "            # observation_att = self.att_norm2(self.dropout2(observation_att_) + crop_rep_)\n",
        "\n",
        "\n",
        "\n",
        "            # -- [B' x K']\n",
        "            observation_rep_, max_ind = torch.max(observation_att_out,dim=1)\n",
        "            # -- [B' x K']\n",
        "            observation_rep = self.att_fc(observation_rep_)\n",
        "            st = self.att_norm(observation_rep)\n",
        "        #--------------------------------------#\n",
        "\n",
        "\n",
        "        #----------------LSTM-------------------#\n",
        "        core_input = st.view(T, B, -1)\n",
        "        core_output_list = []\n",
        "        notdone = (~env_outputs[\"done\"]).float()\n",
        "        for input, nd in zip(core_input.unbind(), notdone.unbind()):\n",
        "            # Reset core state to zero whenever an episode ended.\n",
        "            # Make `done` broadcastable with (num_layers, B, hidden_size)\n",
        "            # states:\n",
        "            nd = nd.view(1, -1, 1)\n",
        "            core_state = tuple(nd * s for s in core_state)\n",
        "            output, core_state = self.core(input.unsqueeze(0), core_state)\n",
        "            core_output_list.append(output)\n",
        "        # -- [T x B x K']\n",
        "        core_output_ = torch.cat(core_output_list)\n",
        "\n",
        "        # -- [B' x K']\n",
        "        core_output = torch.flatten(core_output_, 0, 1)\n",
        "\n",
        "        #---------------------------------------#\n",
        "\n",
        "        # -- [B' x A]\n",
        "        policy_logits = self.policy(core_output)\n",
        "        # -- [B' x A]\n",
        "        baseline = self.baseline(core_output)\n",
        "\n",
        "        if self.act_mask is not None:\n",
        "            policy_logits += self.act_mask\n",
        "\n",
        "        if self.training:\n",
        "            ##\n",
        "            #policy_logits_ = policy_logits.clone()\n",
        "            #policy_logits_[:, 1:17] *= (1 / 16)\n",
        "            # print(f\"logits/16 is {policy_logits_}\")\n",
        "            ##\n",
        "            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
        "        else:\n",
        "            # Don't sample when testing.\n",
        "            action = torch.argmax(policy_logits, dim=1)\n",
        "\n",
        "        # if last_actions.shape[0] == 1:\n",
        "        #     if last_actions[0] == 21 or last_actions[0] == 18:\n",
        "        #         if torch.rand(1) < 0.6:\n",
        "        #             action[0] = 8\n",
        "\n",
        "\n",
        "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
        "        baseline = baseline.view(T, B)\n",
        "        action = action.view(T, B)\n",
        "        return (\n",
        "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
        "            core_state,\n",
        "        )\n",
        "\n",
        "def main(flags):\n",
        "    if flags.mode == \"train\":\n",
        "        train(flags)\n",
        "    else:\n",
        "        test(flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTuLRQpqPLri"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3s5ipuzCK-x"
      },
      "source": [
        "TODO:\n",
        "action mask\n",
        "exp replay actor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcPs79dQFSbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639772d0-c902-4e47-8aa0-0e6623456cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "                   ('FIRE', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('INVENTORY', 8),\n",
            "                   ('RUSH', 7),\n",
            "                   ('SE_', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('PICKUP', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 9700.8505859375,\n",
            " 'entropy_loss': -1.543116807937622,\n",
            " 'episode_returns': (359.0,),\n",
            " 'mean_episode_return': 359.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -87.9194564819336,\n",
            " 'total_loss': 9611.3876953125}\n",
            "INFO:root:Steps 12001280 ( 92016640 of 10000000000 ) @ 23819.0 SPS. Loss 9163.535156. Return per episode: 154.0. Stats:\n",
            "{'action_counts': [('W', 475),\n",
            "                   ('E', 414),\n",
            "                   ('S', 358),\n",
            "                   ('SEARCH', 357),\n",
            "                   ('N', 266),\n",
            "                   ('E_', 187),\n",
            "                   ('S_', 120),\n",
            "                   ('W_', 106),\n",
            "                   ('N_', 57),\n",
            "                   ('EAT', 35),\n",
            "                   ('SW', 32),\n",
            "                   ('SE', 28),\n",
            "                   ('NW_', 25),\n",
            "                   ('NW', 20),\n",
            "                   ('NE', 17),\n",
            "                   ('DOWN', 16),\n",
            "                   ('INVENTORY', 13),\n",
            "                   ('FIRE', 10),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('WAIT', 2),\n",
            "                   ('PICKUP', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('MORE', 1),\n",
            "                   ('PAY', 1),\n",
            "                   ('PUTON', 1)],\n",
            " 'baseline_loss': 9057.123046875,\n",
            " 'entropy_loss': -1.700971245765686,\n",
            " 'episode_returns': (154.0,),\n",
            " 'mean_episode_return': 154.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 108.11311340332031,\n",
            " 'total_loss': 9163.53515625}\n",
            "INFO:root:Steps 12052480 ( 92067840 of 10000000000 ) @ 23919.1 SPS. Loss 7857.925293. Return per episode: 285.0. Stats:\n",
            "{'action_counts': [('E', 635),\n",
            "                   ('W', 396),\n",
            "                   ('S', 377),\n",
            "                   ('SEARCH', 260),\n",
            "                   ('N', 257),\n",
            "                   ('E_', 157),\n",
            "                   ('N_', 139),\n",
            "                   ('W_', 124),\n",
            "                   ('S_', 53),\n",
            "                   ('SW', 29),\n",
            "                   ('NW_', 24),\n",
            "                   ('EAT', 24),\n",
            "                   ('SE', 22),\n",
            "                   ('NE', 19),\n",
            "                   ('INVENTORY', 11),\n",
            "                   ('FIRE', 10),\n",
            "                   ('DOWN', 9),\n",
            "                   ('NW', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('KICK', 3),\n",
            "                   ('SW_', 2)],\n",
            " 'baseline_loss': 7699.451171875,\n",
            " 'entropy_loss': -1.7429275512695312,\n",
            " 'episode_returns': (310.0, 248.0, 500.0, 82.0),\n",
            " 'mean_episode_return': 285.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 160.2174835205078,\n",
            " 'total_loss': 7857.92529296875}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 12103680 ( 92119040 of 10000000000 ) @ 24020.2 SPS. Loss 11293.961914. Return per episode: 523.7. Stats:\n",
            "{'action_counts': [('W', 649),\n",
            "                   ('E', 471),\n",
            "                   ('S', 330),\n",
            "                   ('SEARCH', 311),\n",
            "                   ('N', 293),\n",
            "                   ('W_', 150),\n",
            "                   ('E_', 118),\n",
            "                   ('N_', 82),\n",
            "                   ('SW', 23),\n",
            "                   ('S_', 23),\n",
            "                   ('DOWN', 19),\n",
            "                   ('EAT', 18),\n",
            "                   ('KICK', 17),\n",
            "                   ('NE', 13),\n",
            "                   ('SE', 13),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 8),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 11124.0791015625,\n",
            " 'entropy_loss': -1.0780341625213623,\n",
            " 'episode_returns': (531.0, 682.0, 358.0),\n",
            " 'mean_episode_return': 523.6666870117188,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 170.96124267578125,\n",
            " 'total_loss': 11293.9619140625}\n",
            "INFO:root:Steps 12149760 ( 92165120 of 10000000000 ) @ 24120.3 SPS. Loss 9379.914062. Return per episode: 374.5. Stats:\n",
            "{'action_counts': [('W', 496),\n",
            "                   ('E', 424),\n",
            "                   ('SEARCH', 358),\n",
            "                   ('S', 337),\n",
            "                   ('N', 283),\n",
            "                   ('W_', 204),\n",
            "                   ('E_', 162),\n",
            "                   ('N_', 129),\n",
            "                   ('S_', 43),\n",
            "                   ('NW', 25),\n",
            "                   ('NW_', 23),\n",
            "                   ('NE', 21),\n",
            "                   ('SW', 17),\n",
            "                   ('SE', 10),\n",
            "                   ('DOWN', 9),\n",
            "                   ('EAT', 6),\n",
            "                   ('KICK', 4),\n",
            "                   ('RUSH', 3),\n",
            "                   ('FIRE', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 9457.412109375,\n",
            " 'entropy_loss': -1.455401062965393,\n",
            " 'episode_returns': (347.0, 402.0),\n",
            " 'mean_episode_return': 374.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -76.04254913330078,\n",
            " 'total_loss': 9379.9140625}\n",
            "INFO:root:Steps 12200960 ( 92216320 of 10000000000 ) @ 24220.4 SPS. Loss 9499.769531. Return per episode: 602.0. Stats:\n",
            "{'action_counts': [('W', 533),\n",
            "                   ('E', 484),\n",
            "                   ('N', 339),\n",
            "                   ('S', 321),\n",
            "                   ('SEARCH', 191),\n",
            "                   ('E_', 167),\n",
            "                   ('W_', 165),\n",
            "                   ('N_', 127),\n",
            "                   ('S_', 83),\n",
            "                   ('NW_', 27),\n",
            "                   ('SE', 26),\n",
            "                   ('SW', 22),\n",
            "                   ('NW', 17),\n",
            "                   ('EAT', 16),\n",
            "                   ('DOWN', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('FIRE', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('QUAFF', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 9870.9794921875,\n",
            " 'entropy_loss': -1.6161895990371704,\n",
            " 'episode_returns': (1133.0, 358.0, 315.0),\n",
            " 'mean_episode_return': 602.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -369.59375,\n",
            " 'total_loss': 9499.76953125}\n",
            "INFO:root:Steps 12252160 ( 92267520 of 10000000000 ) @ 24320.5 SPS. Loss 7752.092285. Return per episode: 298.0. Stats:\n",
            "{'action_counts': [('W', 438),\n",
            "                   ('E', 436),\n",
            "                   ('S', 383),\n",
            "                   ('N', 299),\n",
            "                   ('E_', 259),\n",
            "                   ('W_', 187),\n",
            "                   ('SEARCH', 168),\n",
            "                   ('N_', 140),\n",
            "                   ('S_', 107),\n",
            "                   ('EAT', 37),\n",
            "                   ('NW_', 31),\n",
            "                   ('SE', 15),\n",
            "                   ('NE', 11),\n",
            "                   ('DOWN', 11),\n",
            "                   ('RUSH', 9),\n",
            "                   ('FIRE', 8),\n",
            "                   ('SW', 6),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('KICK', 5),\n",
            "                   ('NW', 4),\n",
            "                   ('NE_', 1)],\n",
            " 'baseline_loss': 7881.169921875,\n",
            " 'entropy_loss': -1.5970479249954224,\n",
            " 'episode_returns': (298.0,),\n",
            " 'mean_episode_return': 298.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -127.48051452636719,\n",
            " 'total_loss': 7752.09228515625}\n",
            "INFO:root:Steps 12300800 ( 92316160 of 10000000000 ) @ 24420.6 SPS. Loss 8230.965820. Return per episode: 291.0. Stats:\n",
            "{'action_counts': [('W', 415),\n",
            "                   ('SEARCH', 392),\n",
            "                   ('E', 377),\n",
            "                   ('S', 313),\n",
            "                   ('E_', 272),\n",
            "                   ('N', 247),\n",
            "                   ('W_', 174),\n",
            "                   ('N_', 166),\n",
            "                   ('S_', 100),\n",
            "                   ('NW_', 25),\n",
            "                   ('SE', 18),\n",
            "                   ('NE', 15),\n",
            "                   ('EAT', 10),\n",
            "                   ('DOWN', 9),\n",
            "                   ('SW', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('KICK', 5),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('NW', 2),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 8420.25,\n",
            " 'entropy_loss': -1.7554775476455688,\n",
            " 'episode_returns': (277.0, 304.0, 209.0, 348.0, 317.0),\n",
            " 'mean_episode_return': 291.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -187.52822875976562,\n",
            " 'total_loss': 8230.9658203125}\n",
            "INFO:root:Steps 12352000 ( 92367360 of 10000000000 ) @ 24520.7 SPS. Loss 11347.802734. Return per episode: 337.2. Stats:\n",
            "{'action_counts': [('E', 513),\n",
            "                   ('W', 483),\n",
            "                   ('N', 409),\n",
            "                   ('S', 340),\n",
            "                   ('SEARCH', 178),\n",
            "                   ('W_', 162),\n",
            "                   ('E_', 143),\n",
            "                   ('S_', 89),\n",
            "                   ('N_', 82),\n",
            "                   ('NE', 26),\n",
            "                   ('SE', 23),\n",
            "                   ('NW', 21),\n",
            "                   ('SW', 19),\n",
            "                   ('NW_', 16),\n",
            "                   ('DOWN', 12),\n",
            "                   ('EAT', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('FIRE', 8),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('SW_', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 11150.572265625,\n",
            " 'entropy_loss': -1.3167970180511475,\n",
            " 'episode_returns': (420.0, 224.0, 524.0, 181.0),\n",
            " 'mean_episode_return': 337.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 198.546875,\n",
            " 'total_loss': 11347.802734375}\n",
            "INFO:root:Steps 12400640 ( 92416000 of 10000000000 ) @ 24620.8 SPS. Loss 8961.254883. Return per episode: 369.4. Stats:\n",
            "{'action_counts': [('E', 666),\n",
            "                   ('W', 467),\n",
            "                   ('S', 367),\n",
            "                   ('N', 258),\n",
            "                   ('SEARCH', 188),\n",
            "                   ('W_', 178),\n",
            "                   ('E_', 99),\n",
            "                   ('N_', 83),\n",
            "                   ('S_', 77),\n",
            "                   ('EAT', 26),\n",
            "                   ('SE', 24),\n",
            "                   ('DOWN', 24),\n",
            "                   ('NW_', 20),\n",
            "                   ('SW', 19),\n",
            "                   ('KICK', 18),\n",
            "                   ('NW', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('FIRE', 9),\n",
            "                   ('RUSH', 7),\n",
            "                   ('INVENTORY', 5)],\n",
            " 'baseline_loss': 9446.484375,\n",
            " 'entropy_loss': -1.3609092235565186,\n",
            " 'episode_returns': (331.0, 110.0, 874.0, 169.0, 363.0),\n",
            " 'mean_episode_return': 369.3999938964844,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -483.8684997558594,\n",
            " 'total_loss': 8961.2548828125}\n",
            "INFO:root:Steps 12449280 ( 92464640 of 10000000000 ) @ 24721.0 SPS. Loss 18471.574219. Return per episode: 487.0. Stats:\n",
            "{'action_counts': [('W', 596),\n",
            "                   ('E', 557),\n",
            "                   ('S', 368),\n",
            "                   ('SEARCH', 260),\n",
            "                   ('N', 217),\n",
            "                   ('W_', 114),\n",
            "                   ('E_', 93),\n",
            "                   ('S_', 80),\n",
            "                   ('N_', 75),\n",
            "                   ('SW', 40),\n",
            "                   ('NE', 35),\n",
            "                   ('NW', 34),\n",
            "                   ('SE', 24),\n",
            "                   ('DOWN', 18),\n",
            "                   ('NW_', 16),\n",
            "                   ('EAT', 13),\n",
            "                   ('KICK', 6),\n",
            "                   ('FIRE', 3),\n",
            "                   ('RUSH', 3),\n",
            "                   ('CLOSE', 2),\n",
            "                   ('PICKUP', 2),\n",
            "                   ('ESC', 1),\n",
            "                   ('QUAFF', 1),\n",
            "                   ('WEAR', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 18827.66015625,\n",
            " 'entropy_loss': -1.3358829021453857,\n",
            " 'episode_returns': (625.0, 91.0, 654.0, 876.0, 387.0, 568.0, 208.0),\n",
            " 'mean_episode_return': 487.0000305175781,\n",
            " 'num_episodes': 7,\n",
            " 'pg_loss': -354.75030517578125,\n",
            " 'total_loss': 18471.57421875}\n",
            "INFO:root:Steps 12500480 ( 92515840 of 10000000000 ) @ 24821.1 SPS. Loss 12806.975586. Return per episode: 278.7. Stats:\n",
            "{'action_counts': [('E', 622),\n",
            "                   ('W', 471),\n",
            "                   ('SEARCH', 307),\n",
            "                   ('N', 245),\n",
            "                   ('S', 245),\n",
            "                   ('E_', 137),\n",
            "                   ('N_', 133),\n",
            "                   ('W_', 116),\n",
            "                   ('S_', 103),\n",
            "                   ('NW_', 44),\n",
            "                   ('EAT', 34),\n",
            "                   ('NE', 23),\n",
            "                   ('SE', 17),\n",
            "                   ('DOWN', 13),\n",
            "                   ('SW', 10),\n",
            "                   ('KICK', 10),\n",
            "                   ('NW', 7),\n",
            "                   ('FIRE', 7),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('RUSH', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('SE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 12503.8759765625,\n",
            " 'entropy_loss': -1.8253567218780518,\n",
            " 'episode_returns': (290.0, 410.0, 136.0),\n",
            " 'mean_episode_return': 278.66668701171875,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 304.9247131347656,\n",
            " 'total_loss': 12806.9755859375}\n",
            "INFO:root:Steps 12551680 ( 92567040 of 10000000000 ) @ 24921.2 SPS. Loss 5534.239746. Return per episode: 539.3. Stats:\n",
            "{'action_counts': [('E', 500),\n",
            "                   ('W', 473),\n",
            "                   ('SEARCH', 410),\n",
            "                   ('S', 259),\n",
            "                   ('N', 209),\n",
            "                   ('W_', 186),\n",
            "                   ('E_', 158),\n",
            "                   ('N_', 156),\n",
            "                   ('S_', 90),\n",
            "                   ('NW_', 29),\n",
            "                   ('SE', 19),\n",
            "                   ('EAT', 19),\n",
            "                   ('NE', 10),\n",
            "                   ('SW', 9),\n",
            "                   ('NW', 9),\n",
            "                   ('DOWN', 9),\n",
            "                   ('KICK', 4),\n",
            "                   ('RUSH', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 5749.56640625,\n",
            " 'entropy_loss': -1.6614717245101929,\n",
            " 'episode_returns': (830.0, 560.0, 228.0),\n",
            " 'mean_episode_return': 539.3333740234375,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -213.66506958007812,\n",
            " 'total_loss': 5534.23974609375}\n",
            "INFO:root:Steps 12600320 ( 92615680 of 10000000000 ) @ 25021.3 SPS. Loss 7023.338867. Return per episode: 324.0. Stats:\n",
            "{'action_counts': [('E', 678),\n",
            "                   ('W', 555),\n",
            "                   ('S', 316),\n",
            "                   ('SEARCH', 311),\n",
            "                   ('N', 197),\n",
            "                   ('E_', 108),\n",
            "                   ('W_', 94),\n",
            "                   ('N_', 83),\n",
            "                   ('S_', 39),\n",
            "                   ('EAT', 29),\n",
            "                   ('NE', 27),\n",
            "                   ('NW_', 23),\n",
            "                   ('SE', 19),\n",
            "                   ('SW', 19),\n",
            "                   ('DOWN', 13),\n",
            "                   ('RUSH', 13),\n",
            "                   ('FIRE', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('NW', 7),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('WAIT', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 7114.673828125,\n",
            " 'entropy_loss': -1.5734620094299316,\n",
            " 'episode_returns': (571.0, 222.0, 179.0),\n",
            " 'mean_episode_return': 324.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -89.76184844970703,\n",
            " 'total_loss': 7023.3388671875}\n",
            "INFO:root:Steps 12648960 ( 92664320 of 10000000000 ) @ 25121.4 SPS. Loss 8999.208008. Return per episode: 246.0. Stats:\n",
            "{'action_counts': [('E', 558),\n",
            "                   ('W', 474),\n",
            "                   ('S', 341),\n",
            "                   ('SEARCH', 323),\n",
            "                   ('N', 204),\n",
            "                   ('W_', 169),\n",
            "                   ('E_', 126),\n",
            "                   ('N_', 125),\n",
            "                   ('S_', 119),\n",
            "                   ('SE', 29),\n",
            "                   ('SW', 21),\n",
            "                   ('NW_', 13),\n",
            "                   ('EAT', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('NE', 7),\n",
            "                   ('NW', 5),\n",
            "                   ('RUSH', 3),\n",
            "                   ('FIRE', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 8868.943359375,\n",
            " 'entropy_loss': -1.4915896654129028,\n",
            " 'episode_returns': (246.0,),\n",
            " 'mean_episode_return': 246.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 131.75587463378906,\n",
            " 'total_loss': 8999.2080078125}\n",
            "INFO:root:Steps 12700160 ( 92715520 of 10000000000 ) @ 25221.5 SPS. Loss 10074.443359. Stats:\n",
            "{'action_counts': [('E', 628),\n",
            "                   ('W', 487),\n",
            "                   ('SEARCH', 310),\n",
            "                   ('S', 301),\n",
            "                   ('N', 207),\n",
            "                   ('N_', 140),\n",
            "                   ('S_', 121),\n",
            "                   ('E_', 114),\n",
            "                   ('W_', 86),\n",
            "                   ('EAT', 41),\n",
            "                   ('SE', 31),\n",
            "                   ('DOWN', 18),\n",
            "                   ('NE', 17),\n",
            "                   ('SW', 16),\n",
            "                   ('RUSH', 16),\n",
            "                   ('KICK', 9),\n",
            "                   ('FIRE', 7),\n",
            "                   ('NW_', 5),\n",
            "                   ('NW', 4),\n",
            "                   ('INVENTORY', 2)],\n",
            " 'baseline_loss': 9976.5234375,\n",
            " 'entropy_loss': -1.5054001808166504,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': 99.42568969726562,\n",
            " 'total_loss': 10074.443359375}\n",
            "INFO:root:Steps 12748800 ( 92764160 of 10000000000 ) @ 25321.6 SPS. Loss 7903.793457. Return per episode: 817.0. Stats:\n",
            "{'action_counts': [('W', 482),\n",
            "                   ('E', 458),\n",
            "                   ('S', 293),\n",
            "                   ('SEARCH', 254),\n",
            "                   ('N', 231),\n",
            "                   ('W_', 182),\n",
            "                   ('S_', 157),\n",
            "                   ('E_', 155),\n",
            "                   ('N_', 123),\n",
            "                   ('SW', 43),\n",
            "                   ('EAT', 37),\n",
            "                   ('SE', 33),\n",
            "                   ('NW_', 25),\n",
            "                   ('NE', 18),\n",
            "                   ('INVENTORY', 18),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('RUSH', 11),\n",
            "                   ('FIRE', 4),\n",
            "                   ('KICK', 3),\n",
            "                   ('SW_', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 7669.2236328125,\n",
            " 'entropy_loss': -2.1057863235473633,\n",
            " 'episode_returns': (1210.0, 424.0),\n",
            " 'mean_episode_return': 817.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 236.67593383789062,\n",
            " 'total_loss': 7903.79345703125}\n",
            "INFO:root:Steps 12797440 ( 92812800 of 10000000000 ) @ 25421.7 SPS. Loss 7625.212402. Return per episode: 596.0. Stats:\n",
            "{'action_counts': [('E', 726),\n",
            "                   ('W', 321),\n",
            "                   ('S', 313),\n",
            "                   ('N', 294),\n",
            "                   ('SEARCH', 245),\n",
            "                   ('E_', 178),\n",
            "                   ('N_', 130),\n",
            "                   ('S_', 92),\n",
            "                   ('W_', 80),\n",
            "                   ('NE', 42),\n",
            "                   ('KICK', 28),\n",
            "                   ('SE', 26),\n",
            "                   ('DOWN', 24),\n",
            "                   ('EAT', 17),\n",
            "                   ('NW', 12),\n",
            "                   ('SW', 10),\n",
            "                   ('NW_', 10),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('FIRE', 2),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('PAY', 1),\n",
            "                   ('RUSH', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 7552.4892578125,\n",
            " 'entropy_loss': -1.5875569581985474,\n",
            " 'episode_returns': (761.0, 431.0),\n",
            " 'mean_episode_return': 596.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 74.31047821044922,\n",
            " 'total_loss': 7625.21240234375}\n",
            "INFO:root:Steps 12846080 ( 92861440 of 10000000000 ) @ 25521.8 SPS. Loss 9331.375977. Return per episode: 435.0. Stats:\n",
            "{'action_counts': [('W', 459),\n",
            "                   ('E', 429),\n",
            "                   ('S', 357),\n",
            "                   ('SEARCH', 259),\n",
            "                   ('N', 254),\n",
            "                   ('W_', 236),\n",
            "                   ('S_', 187),\n",
            "                   ('N_', 144),\n",
            "                   ('E_', 88),\n",
            "                   ('SW', 49),\n",
            "                   ('DOWN', 16),\n",
            "                   ('NW_', 15),\n",
            "                   ('SE', 14),\n",
            "                   ('NW', 14),\n",
            "                   ('KICK', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('EAT', 9),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 9617.435546875,\n",
            " 'entropy_loss': -1.4933266639709473,\n",
            " 'episode_returns': (801.0, 219.0, 285.0),\n",
            " 'mean_episode_return': 435.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -284.56671142578125,\n",
            " 'total_loss': 9331.3759765625}\n",
            "INFO:root:Steps 12894720 ( 92910080 of 10000000000 ) @ 25621.9 SPS. Loss 9525.100586. Return per episode: 435.0. Stats:\n",
            "{'action_counts': [('E', 628),\n",
            "                   ('W', 366),\n",
            "                   ('N', 329),\n",
            "                   ('S', 307),\n",
            "                   ('SEARCH', 278),\n",
            "                   ('E_', 134),\n",
            "                   ('W_', 130),\n",
            "                   ('S_', 120),\n",
            "                   ('N_', 111),\n",
            "                   ('SE', 40),\n",
            "                   ('SW', 27),\n",
            "                   ('NE', 18),\n",
            "                   ('EAT', 16),\n",
            "                   ('NW', 12),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('DOWN', 8),\n",
            "                   ('NW_', 7),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 3),\n",
            "                   ('OPEN', 2)],\n",
            " 'baseline_loss': 9456.07421875,\n",
            " 'entropy_loss': -1.5959280729293823,\n",
            " 'episode_returns': (620.0, 278.0, 407.0),\n",
            " 'mean_episode_return': 435.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 70.62205505371094,\n",
            " 'total_loss': 9525.1005859375}\n",
            "INFO:root:Steps 12943360 ( 92958720 of 10000000000 ) @ 25722.0 SPS. Loss 7156.990234. Return per episode: 219.0. Stats:\n",
            "{'action_counts': [('E', 575),\n",
            "                   ('N', 326),\n",
            "                   ('SEARCH', 316),\n",
            "                   ('W', 299),\n",
            "                   ('S', 251),\n",
            "                   ('N_', 175),\n",
            "                   ('E_', 159),\n",
            "                   ('S_', 120),\n",
            "                   ('W_', 109),\n",
            "                   ('EAT', 53),\n",
            "                   ('SE', 28),\n",
            "                   ('SW', 24),\n",
            "                   ('NW', 21),\n",
            "                   ('NW_', 19),\n",
            "                   ('NE', 16),\n",
            "                   ('KICK', 15),\n",
            "                   ('INVENTORY', 13),\n",
            "                   ('FIRE', 11),\n",
            "                   ('DOWN', 10),\n",
            "                   ('RUSH', 9),\n",
            "                   ('CLOSE', 2),\n",
            "                   ('PICKUP', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('MORE', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 7144.9921875,\n",
            " 'entropy_loss': -1.8562129735946655,\n",
            " 'episode_returns': (260.0, 178.0),\n",
            " 'mean_episode_return': 219.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 13.854450225830078,\n",
            " 'total_loss': 7156.990234375}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 12994560 ( 93009920 of 10000000000 ) @ 25822.6 SPS. Loss 3870.346436. Return per episode: 310.0. Stats:\n",
            "{'action_counts': [('E', 553),\n",
            "                   ('S', 328),\n",
            "                   ('N', 275),\n",
            "                   ('SEARCH', 256),\n",
            "                   ('N_', 228),\n",
            "                   ('W', 212),\n",
            "                   ('S_', 206),\n",
            "                   ('W_', 161),\n",
            "                   ('E_', 124),\n",
            "                   ('EAT', 69),\n",
            "                   ('SE', 37),\n",
            "                   ('SW', 27),\n",
            "                   ('FIRE', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('NW', 13),\n",
            "                   ('NW_', 11),\n",
            "                   ('KICK', 8),\n",
            "                   ('DOWN', 6),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 4058.61376953125,\n",
            " 'entropy_loss': -1.9240621328353882,\n",
            " 'episode_returns': (381.0, 239.0),\n",
            " 'mean_episode_return': 310.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -186.34329223632812,\n",
            " 'total_loss': 3870.346435546875}\n",
            "INFO:root:Steps 13043200 ( 93058560 of 10000000000 ) @ 25922.7 SPS. Loss 8270.934570. Return per episode: 148.0. Stats:\n",
            "{'action_counts': [('W', 560),\n",
            "                   ('E', 533),\n",
            "                   ('S', 346),\n",
            "                   ('N', 229),\n",
            "                   ('W_', 176),\n",
            "                   ('SEARCH', 163),\n",
            "                   ('N_', 132),\n",
            "                   ('S_', 127),\n",
            "                   ('E_', 126),\n",
            "                   ('EAT', 36),\n",
            "                   ('SW', 32),\n",
            "                   ('SE', 18),\n",
            "                   ('NW', 15),\n",
            "                   ('DOWN', 14),\n",
            "                   ('KICK', 14),\n",
            "                   ('NW_', 10),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 2),\n",
            "                   ('UP', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 8263.859375,\n",
            " 'entropy_loss': -1.5237915515899658,\n",
            " 'episode_returns': (148.0,),\n",
            " 'mean_episode_return': 148.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 8.598587036132812,\n",
            " 'total_loss': 8270.9345703125}\n",
            "INFO:root:Steps 13091840 ( 93107200 of 10000000000 ) @ 26022.8 SPS. Loss 11109.997070. Return per episode: 385.2. Stats:\n",
            "{'action_counts': [('E', 667),\n",
            "                   ('W', 533),\n",
            "                   ('S', 358),\n",
            "                   ('N', 303),\n",
            "                   ('E_', 175),\n",
            "                   ('W_', 118),\n",
            "                   ('SEARCH', 84),\n",
            "                   ('N_', 62),\n",
            "                   ('SW', 59),\n",
            "                   ('S_', 44),\n",
            "                   ('EAT', 40),\n",
            "                   ('SE', 33),\n",
            "                   ('DOWN', 19),\n",
            "                   ('NE', 16),\n",
            "                   ('NW', 12),\n",
            "                   ('FIRE', 10),\n",
            "                   ('NW_', 7),\n",
            "                   ('KICK', 7),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('UP', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 11171.294921875,\n",
            " 'entropy_loss': -1.2995717525482178,\n",
            " 'episode_returns': (533.0, 422.0, 592.0, 224.0, 155.0),\n",
            " 'mean_episode_return': 385.20001220703125,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -59.99837875366211,\n",
            " 'total_loss': 11109.9970703125}\n",
            "INFO:root:Steps 13140480 ( 93155840 of 10000000000 ) @ 26122.9 SPS. Loss 11692.379883. Return per episode: 454.0. Stats:\n",
            "{'action_counts': [('W', 479),\n",
            "                   ('E', 474),\n",
            "                   ('E_', 314),\n",
            "                   ('N', 284),\n",
            "                   ('S', 275),\n",
            "                   ('SEARCH', 193),\n",
            "                   ('W_', 163),\n",
            "                   ('N_', 140),\n",
            "                   ('S_', 92),\n",
            "                   ('SW', 22),\n",
            "                   ('DOWN', 20),\n",
            "                   ('SE', 18),\n",
            "                   ('KICK', 17),\n",
            "                   ('NE', 16),\n",
            "                   ('NW', 16),\n",
            "                   ('NW_', 13),\n",
            "                   ('EAT', 12),\n",
            "                   ('FIRE', 5),\n",
            "                   ('WAIT', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('SW_', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 11468.734375,\n",
            " 'entropy_loss': -1.4742767810821533,\n",
            " 'episode_returns': (337.0, 637.0, 227.0, 615.0),\n",
            " 'mean_episode_return': 454.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 225.12026977539062,\n",
            " 'total_loss': 11692.3798828125}\n",
            "INFO:root:Steps 13191680 ( 93207040 of 10000000000 ) @ 26223.0 SPS. Loss 9567.732422. Return per episode: 648.0. Stats:\n",
            "{'action_counts': [('W', 607),\n",
            "                   ('E', 527),\n",
            "                   ('S', 316),\n",
            "                   ('N', 303),\n",
            "                   ('SEARCH', 230),\n",
            "                   ('E_', 175),\n",
            "                   ('W_', 115),\n",
            "                   ('N_', 104),\n",
            "                   ('S_', 64),\n",
            "                   ('SW', 22),\n",
            "                   ('SE', 17),\n",
            "                   ('DOWN', 17),\n",
            "                   ('KICK', 16),\n",
            "                   ('NW', 13),\n",
            "                   ('NW_', 13),\n",
            "                   ('EAT', 7),\n",
            "                   ('NE', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('RUSH', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 9372.2861328125,\n",
            " 'entropy_loss': -1.1836574077606201,\n",
            " 'episode_returns': (463.0, 833.0),\n",
            " 'mean_episode_return': 648.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 196.63031005859375,\n",
            " 'total_loss': 9567.732421875}\n",
            "INFO:root:Steps 13237760 ( 93253120 of 10000000000 ) @ 26323.1 SPS. Loss 10819.301758. Return per episode: 602.2. Stats:\n",
            "{'action_counts': [('E', 560),\n",
            "                   ('W', 420),\n",
            "                   ('S', 331),\n",
            "                   ('E_', 280),\n",
            "                   ('N', 199),\n",
            "                   ('W_', 190),\n",
            "                   ('SEARCH', 183),\n",
            "                   ('N_', 118),\n",
            "                   ('S_', 93),\n",
            "                   ('SW', 43),\n",
            "                   ('SE', 36),\n",
            "                   ('EAT', 26),\n",
            "                   ('DOWN', 17),\n",
            "                   ('NE', 13),\n",
            "                   ('NW_', 12),\n",
            "                   ('FIRE', 11),\n",
            "                   ('NW', 9),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('SW_', 1)],\n",
            " 'baseline_loss': 11271.5576171875,\n",
            " 'entropy_loss': -1.5121184587478638,\n",
            " 'episode_returns': (497.0, 555.0, 1057.0, 300.0),\n",
            " 'mean_episode_return': 602.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -450.74407958984375,\n",
            " 'total_loss': 10819.3017578125}\n",
            "INFO:root:Steps 13288960 ( 93304320 of 10000000000 ) @ 26423.2 SPS. Loss 7184.394043. Return per episode: 453.5. Stats:\n",
            "{'action_counts': [('W', 511),\n",
            "                   ('S', 448),\n",
            "                   ('E', 387),\n",
            "                   ('N', 334),\n",
            "                   ('W_', 192),\n",
            "                   ('E_', 160),\n",
            "                   ('SEARCH', 126),\n",
            "                   ('N_', 114),\n",
            "                   ('S_', 97),\n",
            "                   ('SW', 59),\n",
            "                   ('SE', 38),\n",
            "                   ('EAT', 15),\n",
            "                   ('KICK', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('FIRE', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('NW_', 7),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('APPLY', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 7050.06982421875,\n",
            " 'entropy_loss': -1.5233755111694336,\n",
            " 'episode_returns': (442.0, 465.0),\n",
            " 'mean_episode_return': 453.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 135.8475341796875,\n",
            " 'total_loss': 7184.39404296875}\n",
            "INFO:root:Steps 13337600 ( 93352960 of 10000000000 ) @ 26523.3 SPS. Loss 9109.454102. Return per episode: 180.0. Stats:\n",
            "{'action_counts': [('E', 533),\n",
            "                   ('W', 462),\n",
            "                   ('S', 398),\n",
            "                   ('SEARCH', 270),\n",
            "                   ('E_', 228),\n",
            "                   ('N', 170),\n",
            "                   ('W_', 123),\n",
            "                   ('N_', 91),\n",
            "                   ('S_', 58),\n",
            "                   ('SW', 38),\n",
            "                   ('EAT', 35),\n",
            "                   ('KICK', 30),\n",
            "                   ('SE', 29),\n",
            "                   ('NW', 19),\n",
            "                   ('FIRE', 19),\n",
            "                   ('RUSH', 16),\n",
            "                   ('NE', 13),\n",
            "                   ('NW_', 10),\n",
            "                   ('DOWN', 8),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('MOVE', 1),\n",
            "                   ('PAY', 1),\n",
            "                   ('QUAFF', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 8971.9111328125,\n",
            " 'entropy_loss': -1.524632453918457,\n",
            " 'episode_returns': (180.0,),\n",
            " 'mean_episode_return': 180.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 139.067626953125,\n",
            " 'total_loss': 9109.4541015625}\n",
            "INFO:root:Steps 13383680 ( 93399040 of 10000000000 ) @ 26623.5 SPS. Loss 10469.666016. Return per episode: 410.0. Stats:\n",
            "{'action_counts': [('E', 527),\n",
            "                   ('W', 410),\n",
            "                   ('S', 339),\n",
            "                   ('N', 312),\n",
            "                   ('SEARCH', 287),\n",
            "                   ('E_', 218),\n",
            "                   ('W_', 149),\n",
            "                   ('N_', 82),\n",
            "                   ('S_', 51),\n",
            "                   ('SW', 50),\n",
            "                   ('SE', 22),\n",
            "                   ('EAT', 19),\n",
            "                   ('DOWN', 17),\n",
            "                   ('NW_', 15),\n",
            "                   ('NW', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('INVENTORY', 11),\n",
            "                   ('RUSH', 11),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 5),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 10872.232421875,\n",
            " 'entropy_loss': -1.2735015153884888,\n",
            " 'episode_returns': (628.0, 625.0, 231.0, 156.0),\n",
            " 'mean_episode_return': 410.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -401.29248046875,\n",
            " 'total_loss': 10469.666015625}\n",
            "INFO:root:Steps 13434880 ( 93450240 of 10000000000 ) @ 26723.6 SPS. Loss 8066.433105. Return per episode: 442.3. Stats:\n",
            "{'action_counts': [('E', 478),\n",
            "                   ('W', 444),\n",
            "                   ('N', 297),\n",
            "                   ('S', 294),\n",
            "                   ('E_', 266),\n",
            "                   ('W_', 230),\n",
            "                   ('SEARCH', 218),\n",
            "                   ('N_', 110),\n",
            "                   ('S_', 64),\n",
            "                   ('SW', 34),\n",
            "                   ('NE', 18),\n",
            "                   ('SE', 17),\n",
            "                   ('DOWN', 16),\n",
            "                   ('EAT', 16),\n",
            "                   ('KICK', 14),\n",
            "                   ('NW_', 13),\n",
            "                   ('NW', 11),\n",
            "                   ('RUSH', 11),\n",
            "                   ('FIRE', 6),\n",
            "                   ('SW_', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 8016.20654296875,\n",
            " 'entropy_loss': -1.4118443727493286,\n",
            " 'episode_returns': (350.0, 769.0, 208.0),\n",
            " 'mean_episode_return': 442.3333435058594,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 51.638206481933594,\n",
            " 'total_loss': 8066.43310546875}\n",
            "INFO:root:Steps 13483520 ( 93498880 of 10000000000 ) @ 26823.7 SPS. Loss 7755.383301. Return per episode: 426.0. Stats:\n",
            "{'action_counts': [('W', 532),\n",
            "                   ('E', 498),\n",
            "                   ('S', 303),\n",
            "                   ('N', 289),\n",
            "                   ('SEARCH', 226),\n",
            "                   ('E_', 182),\n",
            "                   ('W_', 174),\n",
            "                   ('SE', 74),\n",
            "                   ('S_', 73),\n",
            "                   ('N_', 54),\n",
            "                   ('SW', 33),\n",
            "                   ('DOWN', 23),\n",
            "                   ('EAT', 19),\n",
            "                   ('NW_', 16),\n",
            "                   ('NW', 15),\n",
            "                   ('NE', 13),\n",
            "                   ('RUSH', 12),\n",
            "                   ('KICK', 9),\n",
            "                   ('FIRE', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('NE_', 1),\n",
            "                   ('APPLY', 1)],\n",
            " 'baseline_loss': 7935.7119140625,\n",
            " 'entropy_loss': -1.496591567993164,\n",
            " 'episode_returns': (426.0,),\n",
            " 'mean_episode_return': 426.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -178.83218383789062,\n",
            " 'total_loss': 7755.38330078125}\n",
            "INFO:root:Steps 13532160 ( 93547520 of 10000000000 ) @ 26923.8 SPS. Loss 13234.411133. Return per episode: 400.0. Stats:\n",
            "{'action_counts': [('E', 601),\n",
            "                   ('W', 446),\n",
            "                   ('N', 370),\n",
            "                   ('S', 323),\n",
            "                   ('SEARCH', 265),\n",
            "                   ('E_', 190),\n",
            "                   ('W_', 111),\n",
            "                   ('S_', 69),\n",
            "                   ('N_', 44),\n",
            "                   ('SW', 33),\n",
            "                   ('SE', 23),\n",
            "                   ('NW', 16),\n",
            "                   ('DOWN', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('EAT', 13),\n",
            "                   ('NW_', 11),\n",
            "                   ('KICK', 7),\n",
            "                   ('RUSH', 3),\n",
            "                   ('FIRE', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 13141.486328125,\n",
            " 'entropy_loss': -1.1651540994644165,\n",
            " 'episode_returns': (235.0, 566.0, 221.0, 312.0, 666.0),\n",
            " 'mean_episode_return': 400.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 94.08998107910156,\n",
            " 'total_loss': 13234.4111328125}\n",
            "INFO:root:Steps 13580800 ( 93596160 of 10000000000 ) @ 27023.9 SPS. Loss 10237.257812. Return per episode: 410.0. Stats:\n",
            "{'action_counts': [('E', 487),\n",
            "                   ('W', 468),\n",
            "                   ('SEARCH', 348),\n",
            "                   ('N', 307),\n",
            "                   ('S', 258),\n",
            "                   ('W_', 176),\n",
            "                   ('S_', 158),\n",
            "                   ('E_', 146),\n",
            "                   ('N_', 89),\n",
            "                   ('SW', 30),\n",
            "                   ('EAT', 22),\n",
            "                   ('SE', 19),\n",
            "                   ('NW', 13),\n",
            "                   ('RUSH', 11),\n",
            "                   ('NE', 6),\n",
            "                   ('NW_', 6),\n",
            "                   ('DOWN', 6),\n",
            "                   ('KICK', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('UP', 2),\n",
            "                   ('SE_', 1)],\n",
            " 'baseline_loss': 10673.8720703125,\n",
            " 'entropy_loss': -1.4783611297607422,\n",
            " 'episode_returns': (212.0, 212.0, 332.0, 884.0),\n",
            " 'mean_episode_return': 410.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -435.1357421875,\n",
            " 'total_loss': 10237.2578125}\n",
            "INFO:root:Steps 13629440 ( 93644800 of 10000000000 ) @ 27124.0 SPS. Loss 6811.823242. Stats:\n",
            "{'action_counts': [('E', 559),\n",
            "                   ('W', 502),\n",
            "                   ('N', 486),\n",
            "                   ('S', 357),\n",
            "                   ('W_', 128),\n",
            "                   ('E_', 111),\n",
            "                   ('S_', 105),\n",
            "                   ('SEARCH', 92),\n",
            "                   ('N_', 77),\n",
            "                   ('SW', 37),\n",
            "                   ('SE', 24),\n",
            "                   ('KICK', 21),\n",
            "                   ('NW', 18),\n",
            "                   ('NE', 14),\n",
            "                   ('DOWN', 10),\n",
            "                   ('EAT', 7),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 6903.8583984375,\n",
            " 'entropy_loss': -1.1840400695800781,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -90.85103607177734,\n",
            " 'total_loss': 6811.8232421875}\n",
            "INFO:root:Steps 13678080 ( 93693440 of 10000000000 ) @ 27224.1 SPS. Loss 8279.630859. Return per episode: 496.0. Stats:\n",
            "{'action_counts': [('W', 602),\n",
            "                   ('E', 428),\n",
            "                   ('N', 288),\n",
            "                   ('E_', 239),\n",
            "                   ('SEARCH', 236),\n",
            "                   ('S_', 207),\n",
            "                   ('S', 203),\n",
            "                   ('W_', 143),\n",
            "                   ('N_', 104),\n",
            "                   ('SW', 29),\n",
            "                   ('SE', 20),\n",
            "                   ('EAT', 12),\n",
            "                   ('NE', 10),\n",
            "                   ('NW_', 8),\n",
            "                   ('DOWN', 8),\n",
            "                   ('KICK', 6),\n",
            "                   ('NW', 5),\n",
            "                   ('RUSH', 5),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 8251.25,\n",
            " 'entropy_loss': -1.4113566875457764,\n",
            " 'episode_returns': (496.0,),\n",
            " 'mean_episode_return': 496.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 29.79175567626953,\n",
            " 'total_loss': 8279.630859375}\n",
            "INFO:root:Steps 13726720 ( 93742080 of 10000000000 ) @ 27324.2 SPS. Loss 7990.300781. Return per episode: 937.0. Stats:\n",
            "{'action_counts': [('W', 673),\n",
            "                   ('N', 359),\n",
            "                   ('E_', 305),\n",
            "                   ('E', 304),\n",
            "                   ('SEARCH', 252),\n",
            "                   ('S', 191),\n",
            "                   ('W_', 142),\n",
            "                   ('N_', 131),\n",
            "                   ('S_', 91),\n",
            "                   ('NW', 21),\n",
            "                   ('SE', 14),\n",
            "                   ('NE', 13),\n",
            "                   ('SW', 13),\n",
            "                   ('KICK', 11),\n",
            "                   ('DOWN', 10),\n",
            "                   ('EAT', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('NW_', 7),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 7950.6640625,\n",
            " 'entropy_loss': -1.58711576461792,\n",
            " 'episode_returns': (937.0,),\n",
            " 'mean_episode_return': 937.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 41.22377014160156,\n",
            " 'total_loss': 7990.30078125}\n",
            "INFO:root:Steps 13777920 ( 93793280 of 10000000000 ) @ 27424.3 SPS. Loss 8467.703125. Return per episode: 406.0. Stats:\n",
            "{'action_counts': [('W', 603),\n",
            "                   ('E', 485),\n",
            "                   ('SEARCH', 327),\n",
            "                   ('N', 287),\n",
            "                   ('S', 230),\n",
            "                   ('E_', 178),\n",
            "                   ('S_', 111),\n",
            "                   ('W_', 100),\n",
            "                   ('N_', 80),\n",
            "                   ('SE', 24),\n",
            "                   ('SW', 23),\n",
            "                   ('EAT', 21),\n",
            "                   ('DOWN', 18),\n",
            "                   ('NE', 17),\n",
            "                   ('NW', 16),\n",
            "                   ('RUSH', 11),\n",
            "                   ('KICK', 10),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('FIRE', 5),\n",
            "                   ('NW_', 4),\n",
            "                   ('SE_', 2),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 8719.5947265625,\n",
            " 'entropy_loss': -1.3333699703216553,\n",
            " 'episode_returns': (406.0,),\n",
            " 'mean_episode_return': 406.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -250.5587615966797,\n",
            " 'total_loss': 8467.703125}\n",
            "INFO:root:Steps 13826560 ( 93841920 of 10000000000 ) @ 27524.4 SPS. Loss 9111.094727. Stats:\n",
            "{'action_counts': [('W', 686),\n",
            "                   ('E', 465),\n",
            "                   ('N', 304),\n",
            "                   ('S', 304),\n",
            "                   ('SEARCH', 199),\n",
            "                   ('E_', 187),\n",
            "                   ('N_', 98),\n",
            "                   ('W_', 97),\n",
            "                   ('S_', 82),\n",
            "                   ('SW', 27),\n",
            "                   ('SE', 24),\n",
            "                   ('EAT', 22),\n",
            "                   ('NW_', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('KICK', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('RUSH', 4),\n",
            "                   ('APPLY', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 8785.537109375,\n",
            " 'entropy_loss': -1.3562768697738647,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': 326.91400146484375,\n",
            " 'total_loss': 9111.0947265625}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 13875200 ( 93890560 of 10000000000 ) @ 27624.8 SPS. Loss 9659.248047. Stats:\n",
            "{'action_counts': [('W', 579),\n",
            "                   ('E', 486),\n",
            "                   ('N', 335),\n",
            "                   ('SEARCH', 301),\n",
            "                   ('S', 231),\n",
            "                   ('E_', 191),\n",
            "                   ('W_', 122),\n",
            "                   ('N_', 83),\n",
            "                   ('S_', 79),\n",
            "                   ('SW', 33),\n",
            "                   ('NE', 27),\n",
            "                   ('SE', 22),\n",
            "                   ('EAT', 15),\n",
            "                   ('NW_', 14),\n",
            "                   ('RUSH', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('DOWN', 11),\n",
            "                   ('KICK', 5),\n",
            "                   ('FIRE', 3)],\n",
            " 'baseline_loss': 9785.4677734375,\n",
            " 'entropy_loss': -1.3727164268493652,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -124.84681701660156,\n",
            " 'total_loss': 9659.248046875}\n",
            "INFO:root:Steps 13926400 ( 93941760 of 10000000000 ) @ 27725.0 SPS. Loss 8932.461914. Return per episode: 562.3. Stats:\n",
            "{'action_counts': [('W', 582),\n",
            "                   ('E', 490),\n",
            "                   ('N', 317),\n",
            "                   ('S', 295),\n",
            "                   ('E_', 210),\n",
            "                   ('W_', 140),\n",
            "                   ('SEARCH', 140),\n",
            "                   ('S_', 101),\n",
            "                   ('N_', 90),\n",
            "                   ('SE', 32),\n",
            "                   ('EAT', 31),\n",
            "                   ('SW', 27),\n",
            "                   ('KICK', 24),\n",
            "                   ('INVENTORY', 17),\n",
            "                   ('NE', 16),\n",
            "                   ('NW_', 10),\n",
            "                   ('DOWN', 9),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW', 7),\n",
            "                   ('FIRE', 4),\n",
            "                   ('NE_', 2),\n",
            "                   ('WAIT', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('PUTON', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 8858.244140625,\n",
            " 'entropy_loss': -1.7512001991271973,\n",
            " 'episode_returns': (514.0, 1156.0, 17.0),\n",
            " 'mean_episode_return': 562.3333740234375,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 75.96830749511719,\n",
            " 'total_loss': 8932.4619140625}\n",
            "INFO:root:Steps 13975040 ( 93990400 of 10000000000 ) @ 27825.1 SPS. Loss 12477.860352. Return per episode: 678.8. Stats:\n",
            "{'action_counts': [('W', 605),\n",
            "                   ('E', 414),\n",
            "                   ('N', 370),\n",
            "                   ('S', 332),\n",
            "                   ('E_', 179),\n",
            "                   ('SEARCH', 174),\n",
            "                   ('W_', 159),\n",
            "                   ('S_', 109),\n",
            "                   ('N_', 80),\n",
            "                   ('EAT', 32),\n",
            "                   ('SE', 23),\n",
            "                   ('SW', 21),\n",
            "                   ('DOWN', 11),\n",
            "                   ('KICK', 10),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW_', 7),\n",
            "                   ('NW', 6),\n",
            "                   ('FIRE', 6),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('NE', 5),\n",
            "                   ('OPEN', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 12547.24609375,\n",
            " 'entropy_loss': -1.2782909870147705,\n",
            " 'episode_returns': (1108.0, 954.0, 501.0, 152.0),\n",
            " 'mean_episode_return': 678.75,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -68.10763549804688,\n",
            " 'total_loss': 12477.8603515625}\n",
            "INFO:root:Steps 14023680 ( 94039040 of 10000000000 ) @ 27925.2 SPS. Loss 4902.796387. Return per episode: 152.0. Stats:\n",
            "{'action_counts': [('W', 512),\n",
            "                   ('E', 418),\n",
            "                   ('N', 287),\n",
            "                   ('SEARCH', 283),\n",
            "                   ('E_', 276),\n",
            "                   ('S', 232),\n",
            "                   ('W_', 155),\n",
            "                   ('N_', 103),\n",
            "                   ('S_', 91),\n",
            "                   ('EAT', 39),\n",
            "                   ('SE', 24),\n",
            "                   ('SW', 22),\n",
            "                   ('INVENTORY', 21),\n",
            "                   ('NE', 19),\n",
            "                   ('KICK', 17),\n",
            "                   ('DOWN', 15),\n",
            "                   ('RUSH', 15),\n",
            "                   ('NW_', 10),\n",
            "                   ('FIRE', 8),\n",
            "                   ('NW', 7),\n",
            "                   ('OPEN', 2),\n",
            "                   ('WAIT', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 5085.0556640625,\n",
            " 'entropy_loss': -1.7407537698745728,\n",
            " 'episode_returns': (148.0, 156.0),\n",
            " 'mean_episode_return': 152.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -180.518310546875,\n",
            " 'total_loss': 4902.79638671875}\n",
            "INFO:root:Steps 14072320 ( 94087680 of 10000000000 ) @ 28025.3 SPS. Loss 9190.983398. Return per episode: 561.0. Stats:\n",
            "{'action_counts': [('W', 586),\n",
            "                   ('N', 437),\n",
            "                   ('E', 398),\n",
            "                   ('S', 281),\n",
            "                   ('E_', 177),\n",
            "                   ('SEARCH', 155),\n",
            "                   ('S_', 97),\n",
            "                   ('W_', 93),\n",
            "                   ('N_', 62),\n",
            "                   ('SW', 50),\n",
            "                   ('NE', 41),\n",
            "                   ('SE', 33),\n",
            "                   ('NW', 30),\n",
            "                   ('EAT', 21),\n",
            "                   ('KICK', 21),\n",
            "                   ('DOWN', 18),\n",
            "                   ('NW_', 17),\n",
            "                   ('INVENTORY', 16),\n",
            "                   ('RUSH', 15),\n",
            "                   ('FIRE', 5),\n",
            "                   ('APPLY', 2),\n",
            "                   ('PAY', 2),\n",
            "                   ('WAIT', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 8803.3134765625,\n",
            " 'entropy_loss': -1.6959089040756226,\n",
            " 'episode_returns': (820.0, 250.0, 613.0),\n",
            " 'mean_episode_return': 561.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 389.36669921875,\n",
            " 'total_loss': 9190.9833984375}\n",
            "INFO:root:Steps 14123520 ( 94138880 of 10000000000 ) @ 28125.4 SPS. Loss 9662.157227. Return per episode: 616.7. Stats:\n",
            "{'action_counts': [('W', 482),\n",
            "                   ('E', 459),\n",
            "                   ('N', 432),\n",
            "                   ('S', 297),\n",
            "                   ('E_', 168),\n",
            "                   ('SEARCH', 157),\n",
            "                   ('S_', 138),\n",
            "                   ('W_', 112),\n",
            "                   ('N_', 74),\n",
            "                   ('KICK', 48),\n",
            "                   ('EAT', 29),\n",
            "                   ('SE', 28),\n",
            "                   ('SW', 28),\n",
            "                   ('NE', 26),\n",
            "                   ('NW', 17),\n",
            "                   ('DOWN', 17),\n",
            "                   ('RUSH', 14),\n",
            "                   ('NW_', 13),\n",
            "                   ('INVENTORY', 13),\n",
            "                   ('FIRE', 4),\n",
            "                   ('UP', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 10013.015625,\n",
            " 'entropy_loss': -1.6286780834197998,\n",
            " 'episode_returns': (523.0, 576.0, 751.0),\n",
            " 'mean_episode_return': 616.6666870117188,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -349.2296142578125,\n",
            " 'total_loss': 9662.1572265625}\n",
            "INFO:root:Steps 14172160 ( 94187520 of 10000000000 ) @ 28225.5 SPS. Loss 7281.916992. Return per episode: 788.5. Stats:\n",
            "{'action_counts': [('W', 690),\n",
            "                   ('N', 381),\n",
            "                   ('E', 292),\n",
            "                   ('S', 274),\n",
            "                   ('SEARCH', 252),\n",
            "                   ('E_', 185),\n",
            "                   ('S_', 151),\n",
            "                   ('W_', 94),\n",
            "                   ('N_', 83),\n",
            "                   ('NW', 34),\n",
            "                   ('SW', 28),\n",
            "                   ('NE', 21),\n",
            "                   ('EAT', 18),\n",
            "                   ('KICK', 15),\n",
            "                   ('SE', 14),\n",
            "                   ('DOWN', 9),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW_', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('UP', 2),\n",
            "                   ('LOOK', 1)],\n",
            " 'baseline_loss': 7339.70703125,\n",
            " 'entropy_loss': -1.5232696533203125,\n",
            " 'episode_returns': (851.0, 726.0),\n",
            " 'mean_episode_return': 788.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -56.266719818115234,\n",
            " 'total_loss': 7281.9169921875}\n",
            "INFO:root:Steps 14223360 ( 94238720 of 10000000000 ) @ 28325.6 SPS. Loss 8349.053711. Return per episode: 426.2. Stats:\n",
            "{'action_counts': [('W', 475),\n",
            "                   ('E', 371),\n",
            "                   ('N', 337),\n",
            "                   ('E_', 309),\n",
            "                   ('S', 299),\n",
            "                   ('W_', 236),\n",
            "                   ('SEARCH', 202),\n",
            "                   ('S_', 143),\n",
            "                   ('N_', 83),\n",
            "                   ('SW', 22),\n",
            "                   ('KICK', 14),\n",
            "                   ('SE', 12),\n",
            "                   ('DOWN', 11),\n",
            "                   ('NW', 9),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('FIRE', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('EAT', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('SW_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 8817.6123046875,\n",
            " 'entropy_loss': -1.497388482093811,\n",
            " 'episode_returns': (91.0, 1231.0, 73.0, 310.0),\n",
            " 'mean_episode_return': 426.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -467.06134033203125,\n",
            " 'total_loss': 8349.0537109375}\n",
            "INFO:root:Steps 14274560 ( 94289920 of 10000000000 ) @ 28425.7 SPS. Loss 17018.304688. Return per episode: 340.6. Stats:\n",
            "{'action_counts': [('W', 637),\n",
            "                   ('E', 543),\n",
            "                   ('N', 384),\n",
            "                   ('SEARCH', 257),\n",
            "                   ('S', 233),\n",
            "                   ('E_', 124),\n",
            "                   ('S_', 119),\n",
            "                   ('W_', 85),\n",
            "                   ('N_', 57),\n",
            "                   ('KICK', 23),\n",
            "                   ('DOWN', 22),\n",
            "                   ('NW', 18),\n",
            "                   ('NE', 15),\n",
            "                   ('SW', 15),\n",
            "                   ('SE', 11),\n",
            "                   ('EAT', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('NW_', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('QUAFF', 1),\n",
            "                   ('TAKEOFF', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 16343.23828125,\n",
            " 'entropy_loss': -1.0884881019592285,\n",
            " 'episode_returns': (108.0, 541.0, 222.0, 464.0, 368.0),\n",
            " 'mean_episode_return': 340.6000061035156,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 676.1551513671875,\n",
            " 'total_loss': 17018.3046875}\n",
            "INFO:root:Steps 14320640 ( 94336000 of 10000000000 ) @ 28525.8 SPS. Loss 5990.745605. Return per episode: 260.3. Stats:\n",
            "{'action_counts': [('W', 536),\n",
            "                   ('N', 457),\n",
            "                   ('E', 381),\n",
            "                   ('S', 270),\n",
            "                   ('E_', 254),\n",
            "                   ('SEARCH', 170),\n",
            "                   ('W_', 164),\n",
            "                   ('S_', 92),\n",
            "                   ('N_', 57),\n",
            "                   ('NW', 39),\n",
            "                   ('KICK', 30),\n",
            "                   ('SW', 27),\n",
            "                   ('NE', 21),\n",
            "                   ('NW_', 14),\n",
            "                   ('SE', 13),\n",
            "                   ('DOWN', 10),\n",
            "                   ('FIRE', 7),\n",
            "                   ('EAT', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('MOVE', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 5918.076171875,\n",
            " 'entropy_loss': -1.3887112140655518,\n",
            " 'episode_returns': (272.0, 180.0, 329.0),\n",
            " 'mean_episode_return': 260.3333435058594,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 74.05815124511719,\n",
            " 'total_loss': 5990.74560546875}\n",
            "INFO:root:Steps 14369280 ( 94384640 of 10000000000 ) @ 28625.9 SPS. Loss 7596.365234. Return per episode: 508.5. Stats:\n",
            "{'action_counts': [('E', 624),\n",
            "                   ('W', 499),\n",
            "                   ('N', 296),\n",
            "                   ('S', 283),\n",
            "                   ('E_', 225),\n",
            "                   ('W_', 198),\n",
            "                   ('SEARCH', 164),\n",
            "                   ('S_', 61),\n",
            "                   ('N_', 59),\n",
            "                   ('EAT', 29),\n",
            "                   ('SE', 21),\n",
            "                   ('KICK', 16),\n",
            "                   ('SW', 14),\n",
            "                   ('INVENTORY', 14),\n",
            "                   ('DOWN', 13),\n",
            "                   ('RUSH', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 5),\n",
            "                   ('FIRE', 5),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('APPLY', 1)],\n",
            " 'baseline_loss': 7417.6669921875,\n",
            " 'entropy_loss': -1.3535337448120117,\n",
            " 'episode_returns': (631.0, 386.0),\n",
            " 'mean_episode_return': 508.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 180.0517578125,\n",
            " 'total_loss': 7596.365234375}\n",
            "INFO:root:Steps 14423040 ( 94438400 of 10000000000 ) @ 28726.0 SPS. Loss 8088.126953. Return per episode: 422.0. Stats:\n",
            "{'action_counts': [('W', 615),\n",
            "                   ('E_', 293),\n",
            "                   ('S', 291),\n",
            "                   ('E', 283),\n",
            "                   ('N', 255),\n",
            "                   ('SEARCH', 246),\n",
            "                   ('W_', 184),\n",
            "                   ('N_', 108),\n",
            "                   ('S_', 98),\n",
            "                   ('SE', 51),\n",
            "                   ('NE', 24),\n",
            "                   ('SW', 23),\n",
            "                   ('KICK', 20),\n",
            "                   ('NW', 17),\n",
            "                   ('EAT', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW_', 7),\n",
            "                   ('FIRE', 5),\n",
            "                   ('LOOK', 3),\n",
            "                   ('WAIT', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 8178.46630859375,\n",
            " 'entropy_loss': -2.052241086959839,\n",
            " 'episode_returns': (554.0, 290.0),\n",
            " 'mean_episode_return': 422.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -88.28687286376953,\n",
            " 'total_loss': 8088.126953125}\n",
            "INFO:root:Steps 14469120 ( 94484480 of 10000000000 ) @ 28826.1 SPS. Loss 13202.568359. Return per episode: 644.6. Stats:\n",
            "{'action_counts': [('W', 486),\n",
            "                   ('E', 416),\n",
            "                   ('N', 296),\n",
            "                   ('S', 290),\n",
            "                   ('E_', 241),\n",
            "                   ('SEARCH', 222),\n",
            "                   ('W_', 168),\n",
            "                   ('N_', 120),\n",
            "                   ('S_', 104),\n",
            "                   ('SE', 63),\n",
            "                   ('SW', 28),\n",
            "                   ('NW_', 26),\n",
            "                   ('NE', 21),\n",
            "                   ('EAT', 21),\n",
            "                   ('DOWN', 19),\n",
            "                   ('KICK', 15),\n",
            "                   ('NW', 11),\n",
            "                   ('RUSH', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('SW_', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 12704.796875,\n",
            " 'entropy_loss': -1.679721474647522,\n",
            " 'episode_returns': (221.0, 340.0, 182.0, 2073.0, 407.0),\n",
            " 'mean_episode_return': 644.6000366210938,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 499.450927734375,\n",
            " 'total_loss': 13202.568359375}\n",
            "INFO:root:Steps 14517760 ( 94533120 of 10000000000 ) @ 28926.2 SPS. Loss 7075.081055. Return per episode: 409.0. Stats:\n",
            "{'action_counts': [('E', 532),\n",
            "                   ('W', 374),\n",
            "                   ('S', 327),\n",
            "                   ('N', 263),\n",
            "                   ('E_', 193),\n",
            "                   ('W_', 191),\n",
            "                   ('SEARCH', 164),\n",
            "                   ('N_', 128),\n",
            "                   ('S_', 95),\n",
            "                   ('SE', 77),\n",
            "                   ('SW', 58),\n",
            "                   ('NE', 35),\n",
            "                   ('EAT', 24),\n",
            "                   ('NW', 22),\n",
            "                   ('NW_', 22),\n",
            "                   ('INVENTORY', 12),\n",
            "                   ('RUSH', 12),\n",
            "                   ('DOWN', 8),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('NE_', 3),\n",
            "                   ('SE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 6863.90576171875,\n",
            " 'entropy_loss': -1.908070683479309,\n",
            " 'episode_returns': (440.0, 378.0),\n",
            " 'mean_episode_return': 409.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 213.08331298828125,\n",
            " 'total_loss': 7075.0810546875}\n",
            "INFO:root:Steps 14568960 ( 94584320 of 10000000000 ) @ 29026.4 SPS. Loss 9799.577148. Return per episode: 498.7. Stats:\n",
            "{'action_counts': [('W', 527),\n",
            "                   ('E', 455),\n",
            "                   ('S', 309),\n",
            "                   ('W_', 219),\n",
            "                   ('E_', 209),\n",
            "                   ('N', 198),\n",
            "                   ('SEARCH', 174),\n",
            "                   ('N_', 143),\n",
            "                   ('S_', 111),\n",
            "                   ('SW', 79),\n",
            "                   ('NW_', 25),\n",
            "                   ('SE', 22),\n",
            "                   ('NW', 18),\n",
            "                   ('DOWN', 15),\n",
            "                   ('EAT', 15),\n",
            "                   ('NE', 14),\n",
            "                   ('KICK', 9),\n",
            "                   ('FIRE', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('WAIT', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('PUTON', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 9568.8310546875,\n",
            " 'entropy_loss': -1.7222850322723389,\n",
            " 'episode_returns': (184.0, 750.0, 562.0),\n",
            " 'mean_episode_return': 498.66668701171875,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 232.46885681152344,\n",
            " 'total_loss': 9799.5771484375}\n",
            "INFO:root:Steps 14617600 ( 94632960 of 10000000000 ) @ 29126.5 SPS. Loss 10053.208984. Return per episode: 408.5. Stats:\n",
            "{'action_counts': [('W', 508),\n",
            "                   ('E', 458),\n",
            "                   ('S', 323),\n",
            "                   ('N', 264),\n",
            "                   ('SEARCH', 203),\n",
            "                   ('E_', 192),\n",
            "                   ('W_', 157),\n",
            "                   ('N_', 132),\n",
            "                   ('S_', 92),\n",
            "                   ('SW', 75),\n",
            "                   ('SE', 31),\n",
            "                   ('EAT', 30),\n",
            "                   ('NE', 22),\n",
            "                   ('NW', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('KICK', 10),\n",
            "                   ('FIRE', 9),\n",
            "                   ('RUSH', 8),\n",
            "                   ('NW_', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('OPEN', 3),\n",
            "                   ('SW_', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 10251.8984375,\n",
            " 'entropy_loss': -1.7166125774383545,\n",
            " 'episode_returns': (379.0, 438.0),\n",
            " 'mean_episode_return': 408.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -196.97286987304688,\n",
            " 'total_loss': 10053.208984375}\n",
            "INFO:root:Steps 14666240 ( 94681600 of 10000000000 ) @ 29226.6 SPS. Loss 6579.880371. Return per episode: 1057.0. Stats:\n",
            "{'action_counts': [('E', 519),\n",
            "                   ('S', 411),\n",
            "                   ('W', 370),\n",
            "                   ('N', 288),\n",
            "                   ('SEARCH', 265),\n",
            "                   ('E_', 196),\n",
            "                   ('W_', 167),\n",
            "                   ('N_', 120),\n",
            "                   ('S_', 68),\n",
            "                   ('SE', 42),\n",
            "                   ('SW', 28),\n",
            "                   ('NE', 18),\n",
            "                   ('EAT', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW_', 11),\n",
            "                   ('NW', 6),\n",
            "                   ('FIRE', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 6963.02294921875,\n",
            " 'entropy_loss': -1.474287748336792,\n",
            " 'episode_returns': (1057.0,),\n",
            " 'mean_episode_return': 1057.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -381.6686706542969,\n",
            " 'total_loss': 6579.88037109375}\n",
            "INFO:root:Steps 14714880 ( 94730240 of 10000000000 ) @ 29326.7 SPS. Loss 10266.309570. Return per episode: 398.2. Stats:\n",
            "{'action_counts': [('E', 595),\n",
            "                   ('S', 426),\n",
            "                   ('W', 372),\n",
            "                   ('N', 365),\n",
            "                   ('SEARCH', 177),\n",
            "                   ('W_', 157),\n",
            "                   ('E_', 141),\n",
            "                   ('N_', 121),\n",
            "                   ('S_', 53),\n",
            "                   ('SE', 37),\n",
            "                   ('SW', 29),\n",
            "                   ('NE', 25),\n",
            "                   ('NW', 17),\n",
            "                   ('DOWN', 14),\n",
            "                   ('EAT', 9),\n",
            "                   ('KICK', 7),\n",
            "                   ('NW_', 4),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 9996.7744140625,\n",
            " 'entropy_loss': -1.2271298170089722,\n",
            " 'episode_returns': (127.0, 749.0, 478.0, 298.0, 339.0),\n",
            " 'mean_episode_return': 398.20001220703125,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 270.7628173828125,\n",
            " 'total_loss': 10266.3095703125}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 14766080 ( 94781440 of 10000000000 ) @ 29427.6 SPS. Loss 11241.301758. Return per episode: 550.0. Stats:\n",
            "{'action_counts': [('W', 633),\n",
            "                   ('E', 449),\n",
            "                   ('S', 352),\n",
            "                   ('N', 284),\n",
            "                   ('SEARCH', 217),\n",
            "                   ('E_', 159),\n",
            "                   ('W_', 110),\n",
            "                   ('N_', 99),\n",
            "                   ('S_', 88),\n",
            "                   ('SW', 46),\n",
            "                   ('EAT', 31),\n",
            "                   ('KICK', 25),\n",
            "                   ('DOWN', 20),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 10),\n",
            "                   ('SE', 9),\n",
            "                   ('NE', 6),\n",
            "                   ('FIRE', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('UP', 1)],\n",
            " 'baseline_loss': 11028.341796875,\n",
            " 'entropy_loss': -1.249490737915039,\n",
            " 'episode_returns': (550.0,),\n",
            " 'mean_episode_return': 550.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 214.20944213867188,\n",
            " 'total_loss': 11241.3017578125}\n",
            "INFO:root:Steps 14814720 ( 94830080 of 10000000000 ) @ 29527.7 SPS. Loss 7467.951660. Return per episode: 337.0. Stats:\n",
            "{'action_counts': [('W', 688),\n",
            "                   ('E', 467),\n",
            "                   ('S', 416),\n",
            "                   ('N', 201),\n",
            "                   ('E_', 194),\n",
            "                   ('W_', 169),\n",
            "                   ('SEARCH', 137),\n",
            "                   ('N_', 59),\n",
            "                   ('S_', 54),\n",
            "                   ('SW', 51),\n",
            "                   ('KICK', 23),\n",
            "                   ('SE', 19),\n",
            "                   ('INVENTORY', 17),\n",
            "                   ('DOWN', 11),\n",
            "                   ('RUSH', 11),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 9),\n",
            "                   ('FIRE', 9),\n",
            "                   ('EAT', 8),\n",
            "                   ('NE', 6),\n",
            "                   ('NE_', 1)],\n",
            " 'baseline_loss': 7759.9423828125,\n",
            " 'entropy_loss': -1.415479063987732,\n",
            " 'episode_returns': (337.0,),\n",
            " 'mean_episode_return': 337.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -290.5751953125,\n",
            " 'total_loss': 7467.95166015625}\n",
            "INFO:root:Steps 14865920 ( 94881280 of 10000000000 ) @ 29627.8 SPS. Loss 6342.092773. Return per episode: 146.0. Stats:\n",
            "{'action_counts': [('W', 774),\n",
            "                   ('S', 310),\n",
            "                   ('E_', 294),\n",
            "                   ('N', 250),\n",
            "                   ('W_', 229),\n",
            "                   ('E', 228),\n",
            "                   ('SEARCH', 146),\n",
            "                   ('N_', 141),\n",
            "                   ('S_', 73),\n",
            "                   ('SW', 34),\n",
            "                   ('EAT', 18),\n",
            "                   ('NE', 11),\n",
            "                   ('NW_', 11),\n",
            "                   ('DOWN', 11),\n",
            "                   ('SE', 9),\n",
            "                   ('NW', 9),\n",
            "                   ('KICK', 6),\n",
            "                   ('RUSH', 3),\n",
            "                   ('SE_', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 6442.92236328125,\n",
            " 'entropy_loss': -1.5310032367706299,\n",
            " 'episode_returns': (146.0,),\n",
            " 'mean_episode_return': 146.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -99.29873657226562,\n",
            " 'total_loss': 6342.0927734375}\n",
            "INFO:root:Steps 14914560 ( 94929920 of 10000000000 ) @ 29727.9 SPS. Loss 8829.645508. Return per episode: 294.0. Stats:\n",
            "{'action_counts': [('W', 545),\n",
            "                   ('E', 446),\n",
            "                   ('S', 400),\n",
            "                   ('SEARCH', 319),\n",
            "                   ('N', 216),\n",
            "                   ('E_', 198),\n",
            "                   ('W_', 146),\n",
            "                   ('N_', 101),\n",
            "                   ('S_', 46),\n",
            "                   ('SW', 36),\n",
            "                   ('SE', 23),\n",
            "                   ('EAT', 20),\n",
            "                   ('NW', 15),\n",
            "                   ('NE', 9),\n",
            "                   ('NW_', 9),\n",
            "                   ('DOWN', 9),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 7),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 9245.01953125,\n",
            " 'entropy_loss': -1.5010274648666382,\n",
            " 'episode_returns': (264.0, 324.0),\n",
            " 'mean_episode_return': 294.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -413.8727722167969,\n",
            " 'total_loss': 8829.6455078125}\n",
            "INFO:root:Steps 14963200 ( 94978560 of 10000000000 ) @ 29828.0 SPS. Loss 11078.571289. Return per episode: 370.8. Stats:\n",
            "{'action_counts': [('W', 564),\n",
            "                   ('E', 555),\n",
            "                   ('S', 492),\n",
            "                   ('N', 218),\n",
            "                   ('E_', 187),\n",
            "                   ('SEARCH', 176),\n",
            "                   ('W_', 148),\n",
            "                   ('N_', 77),\n",
            "                   ('S_', 48),\n",
            "                   ('SW', 21),\n",
            "                   ('EAT', 17),\n",
            "                   ('DOWN', 14),\n",
            "                   ('FIRE', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('SE', 7),\n",
            "                   ('NW', 5),\n",
            "                   ('NW_', 4),\n",
            "                   ('NE', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('MOVE', 1),\n",
            "                   ('REMOVE', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 11644.37890625,\n",
            " 'entropy_loss': -1.170398473739624,\n",
            " 'episode_returns': (78.0, 584.0, 304.0, 648.0, 240.0),\n",
            " 'mean_episode_return': 370.8000183105469,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -564.6380615234375,\n",
            " 'total_loss': 11078.5712890625}\n",
            "INFO:root:Steps 15014400 ( 95029760 of 10000000000 ) @ 29928.1 SPS. Loss 8784.198242. Return per episode: 432.0. Stats:\n",
            "{'action_counts': [('E', 493),\n",
            "                   ('W', 491),\n",
            "                   ('S', 411),\n",
            "                   ('E_', 291),\n",
            "                   ('N', 268),\n",
            "                   ('SEARCH', 196),\n",
            "                   ('N_', 155),\n",
            "                   ('W_', 104),\n",
            "                   ('S_', 63),\n",
            "                   ('SW', 18),\n",
            "                   ('EAT', 12),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 10),\n",
            "                   ('DOWN', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('NE', 6),\n",
            "                   ('NW_', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('RUSH', 3),\n",
            "                   ('FIRE', 2),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 8958.0859375,\n",
            " 'entropy_loss': -1.2410115003585815,\n",
            " 'episode_returns': (627.0, 677.0, 162.0, 464.0, 230.0),\n",
            " 'mean_episode_return': 432.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -172.64694213867188,\n",
            " 'total_loss': 8784.1982421875}\n",
            "INFO:root:Steps 15063040 ( 95078400 of 10000000000 ) @ 30028.2 SPS. Loss 10512.441406. Return per episode: 696.0. Stats:\n",
            "{'action_counts': [('W', 591),\n",
            "                   ('N', 329),\n",
            "                   ('E', 301),\n",
            "                   ('S', 291),\n",
            "                   ('E_', 224),\n",
            "                   ('SEARCH', 182),\n",
            "                   ('N_', 177),\n",
            "                   ('W_', 149),\n",
            "                   ('S_', 119),\n",
            "                   ('EAT', 40),\n",
            "                   ('NW', 35),\n",
            "                   ('NW_', 23),\n",
            "                   ('SW', 19),\n",
            "                   ('RUSH', 17),\n",
            "                   ('SE', 15),\n",
            "                   ('INVENTORY', 14),\n",
            "                   ('NE', 12),\n",
            "                   ('FIRE', 7),\n",
            "                   ('KICK', 7),\n",
            "                   ('DOWN', 5),\n",
            "                   ('MORE', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('CLOSE', 1)],\n",
            " 'baseline_loss': 10058.986328125,\n",
            " 'entropy_loss': -1.7295945882797241,\n",
            " 'episode_returns': (696.0,),\n",
            " 'mean_episode_return': 696.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 455.1849670410156,\n",
            " 'total_loss': 10512.44140625}\n",
            "INFO:root:Steps 15114240 ( 95129600 of 10000000000 ) @ 30128.3 SPS. Loss 7079.321289. Return per episode: 1042.0. Stats:\n",
            "{'action_counts': [('W', 618),\n",
            "                   ('N', 369),\n",
            "                   ('S', 367),\n",
            "                   ('E', 312),\n",
            "                   ('E_', 203),\n",
            "                   ('SEARCH', 201),\n",
            "                   ('N_', 116),\n",
            "                   ('S_', 114),\n",
            "                   ('W_', 87),\n",
            "                   ('SW', 33),\n",
            "                   ('EAT', 33),\n",
            "                   ('SE', 27),\n",
            "                   ('INVENTORY', 14),\n",
            "                   ('RUSH', 12),\n",
            "                   ('NE', 11),\n",
            "                   ('KICK', 10),\n",
            "                   ('NW_', 9),\n",
            "                   ('FIRE', 8),\n",
            "                   ('NW', 7),\n",
            "                   ('DOWN', 7),\n",
            "                   ('UP', 1),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 7225.509765625,\n",
            " 'entropy_loss': -1.5323166847229004,\n",
            " 'episode_returns': (1042.0,),\n",
            " 'mean_episode_return': 1042.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -144.6564178466797,\n",
            " 'total_loss': 7079.3212890625}\n",
            "INFO:root:Steps 15160320 ( 95175680 of 10000000000 ) @ 30228.4 SPS. Loss 5234.007324. Return per episode: 472.5. Stats:\n",
            "{'action_counts': [('W', 673),\n",
            "                   ('E', 369),\n",
            "                   ('S', 324),\n",
            "                   ('N', 263),\n",
            "                   ('E_', 246),\n",
            "                   ('SEARCH', 170),\n",
            "                   ('W_', 129),\n",
            "                   ('S_', 111),\n",
            "                   ('N_', 88),\n",
            "                   ('EAT', 40),\n",
            "                   ('SW', 32),\n",
            "                   ('NE', 18),\n",
            "                   ('SE', 17),\n",
            "                   ('RUSH', 16),\n",
            "                   ('INVENTORY', 15),\n",
            "                   ('NW_', 11),\n",
            "                   ('FIRE', 10),\n",
            "                   ('NW', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('DOWN', 6),\n",
            "                   ('WAIT', 2),\n",
            "                   ('MOVE', 1),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 5412.30517578125,\n",
            " 'entropy_loss': -1.5890544652938843,\n",
            " 'episode_returns': (560.0, 573.0, 497.0, 260.0),\n",
            " 'mean_episode_return': 472.5,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -176.70901489257812,\n",
            " 'total_loss': 5234.00732421875}\n",
            "INFO:root:Steps 15211520 ( 95226880 of 10000000000 ) @ 30328.5 SPS. Loss 8498.099609. Return per episode: 266.0. Stats:\n",
            "{'action_counts': [('W', 709),\n",
            "                   ('N', 435),\n",
            "                   ('E', 402),\n",
            "                   ('S', 280),\n",
            "                   ('SEARCH', 219),\n",
            "                   ('E_', 151),\n",
            "                   ('W_', 103),\n",
            "                   ('S_', 69),\n",
            "                   ('N_', 54),\n",
            "                   ('NW', 27),\n",
            "                   ('NE', 26),\n",
            "                   ('SW', 23),\n",
            "                   ('DOWN', 19),\n",
            "                   ('SE', 10),\n",
            "                   ('KICK', 10),\n",
            "                   ('NW_', 9),\n",
            "                   ('EAT', 9),\n",
            "                   ('RUSH', 3),\n",
            "                   ('FIRE', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 8247.3984375,\n",
            " 'entropy_loss': -1.18342924118042,\n",
            " 'episode_returns': (213.0, 319.0),\n",
            " 'mean_episode_return': 266.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 251.88497924804688,\n",
            " 'total_loss': 8498.099609375}\n",
            "INFO:root:Steps 15262720 ( 95278080 of 10000000000 ) @ 30428.6 SPS. Loss 16396.175781. Return per episode: 378.0. Stats:\n",
            "{'action_counts': [('W', 479),\n",
            "                   ('E', 455),\n",
            "                   ('S', 343),\n",
            "                   ('N', 318),\n",
            "                   ('SEARCH', 278),\n",
            "                   ('E_', 207),\n",
            "                   ('W_', 177),\n",
            "                   ('N_', 98),\n",
            "                   ('S_', 71),\n",
            "                   ('NE', 21),\n",
            "                   ('SE', 17),\n",
            "                   ('EAT', 17),\n",
            "                   ('NW', 16),\n",
            "                   ('KICK', 15),\n",
            "                   ('SW', 14),\n",
            "                   ('NW_', 14),\n",
            "                   ('DOWN', 10),\n",
            "                   ('FIRE', 5),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 16145.01953125,\n",
            " 'entropy_loss': -1.3429412841796875,\n",
            " 'episode_returns': (554.0, 514.0, 66.0),\n",
            " 'mean_episode_return': 378.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 252.4999542236328,\n",
            " 'total_loss': 16396.17578125}\n",
            "INFO:root:Steps 15308800 ( 95324160 of 10000000000 ) @ 30528.7 SPS. Loss 3954.768555. Return per episode: 492.0. Stats:\n",
            "{'action_counts': [('W', 489),\n",
            "                   ('E', 424),\n",
            "                   ('N', 296),\n",
            "                   ('S', 271),\n",
            "                   ('SEARCH', 238),\n",
            "                   ('E_', 209),\n",
            "                   ('W_', 205),\n",
            "                   ('N_', 107),\n",
            "                   ('S_', 106),\n",
            "                   ('SW', 49),\n",
            "                   ('SE', 29),\n",
            "                   ('NW', 25),\n",
            "                   ('NW_', 22),\n",
            "                   ('EAT', 22),\n",
            "                   ('NE', 19),\n",
            "                   ('KICK', 19),\n",
            "                   ('DOWN', 12),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('SE_', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('TAKEOFF', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 4127.1923828125,\n",
            " 'entropy_loss': -1.7680199146270752,\n",
            " 'episode_returns': (492.0,),\n",
            " 'mean_episode_return': 492.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -170.6556396484375,\n",
            " 'total_loss': 3954.7685546875}\n",
            "INFO:root:Steps 15360000 ( 95375360 of 10000000000 ) @ 30628.8 SPS. Loss 4884.523926. Return per episode: 153.5. Stats:\n",
            "{'action_counts': [('W', 580),\n",
            "                   ('E', 349),\n",
            "                   ('S', 306),\n",
            "                   ('N', 246),\n",
            "                   ('SEARCH', 245),\n",
            "                   ('E_', 232),\n",
            "                   ('W_', 112),\n",
            "                   ('S_', 104),\n",
            "                   ('N_', 80),\n",
            "                   ('SE', 54),\n",
            "                   ('SW', 49),\n",
            "                   ('NE', 42),\n",
            "                   ('NW_', 37),\n",
            "                   ('NW', 28),\n",
            "                   ('INVENTORY', 26),\n",
            "                   ('KICK', 19),\n",
            "                   ('EAT', 16),\n",
            "                   ('FIRE', 11),\n",
            "                   ('RUSH', 10),\n",
            "                   ('DOWN', 6),\n",
            "                   ('SE_', 3),\n",
            "                   ('DROP', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('PAY', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 5209.5419921875,\n",
            " 'entropy_loss': -2.0427868366241455,\n",
            " 'episode_returns': (256.0, 51.0),\n",
            " 'mean_episode_return': 153.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -322.97515869140625,\n",
            " 'total_loss': 4884.52392578125}\n",
            "INFO:root:Steps 15411200 ( 95426560 of 10000000000 ) @ 30728.9 SPS. Loss 7197.077148. Return per episode: 409.5. Stats:\n",
            "{'action_counts': [('E', 609),\n",
            "                   ('W', 508),\n",
            "                   ('S', 324),\n",
            "                   ('N', 260),\n",
            "                   ('SEARCH', 197),\n",
            "                   ('E_', 188),\n",
            "                   ('W_', 147),\n",
            "                   ('N_', 89),\n",
            "                   ('S_', 57),\n",
            "                   ('NE', 36),\n",
            "                   ('SE', 30),\n",
            "                   ('SW', 27),\n",
            "                   ('NW', 17),\n",
            "                   ('EAT', 14),\n",
            "                   ('NW_', 12),\n",
            "                   ('DOWN', 9),\n",
            "                   ('FIRE', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 6),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('PUTON', 1),\n",
            "                   ('QUAFF', 1),\n",
            "                   ('REMOVE', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 7111.06884765625,\n",
            " 'entropy_loss': -1.5297285318374634,\n",
            " 'episode_returns': (50.0, 769.0),\n",
            " 'mean_episode_return': 409.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 87.5382080078125,\n",
            " 'total_loss': 7197.0771484375}\n",
            "INFO:root:Steps 15457280 ( 95472640 of 10000000000 ) @ 30829.0 SPS. Loss 9156.105469. Return per episode: 489.2. Stats:\n",
            "{'action_counts': [('W', 589),\n",
            "                   ('E', 457),\n",
            "                   ('S', 350),\n",
            "                   ('N', 294),\n",
            "                   ('W_', 197),\n",
            "                   ('E_', 192),\n",
            "                   ('SEARCH', 149),\n",
            "                   ('N_', 115),\n",
            "                   ('S_', 57),\n",
            "                   ('SW', 31),\n",
            "                   ('NW_', 22),\n",
            "                   ('EAT', 19),\n",
            "                   ('DOWN', 16),\n",
            "                   ('NW', 14),\n",
            "                   ('INVENTORY', 14),\n",
            "                   ('NE', 13),\n",
            "                   ('SE', 12),\n",
            "                   ('RUSH', 10),\n",
            "                   ('FIRE', 4),\n",
            "                   ('KICK', 4),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 9068.970703125,\n",
            " 'entropy_loss': -1.3942657709121704,\n",
            " 'episode_returns': (400.0, 1012.0, 290.0, 424.0, 320.0),\n",
            " 'mean_episode_return': 489.20001220703125,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 88.52899169921875,\n",
            " 'total_loss': 9156.10546875}\n",
            "INFO:root:Steps 15505920 ( 95521280 of 10000000000 ) @ 30929.1 SPS. Loss 6066.840332. Return per episode: 318.5. Stats:\n",
            "{'action_counts': [('E', 510),\n",
            "                   ('W', 371),\n",
            "                   ('S', 344),\n",
            "                   ('SEARCH', 332),\n",
            "                   ('N', 322),\n",
            "                   ('E_', 167),\n",
            "                   ('W_', 141),\n",
            "                   ('N_', 75),\n",
            "                   ('S_', 72),\n",
            "                   ('SW', 49),\n",
            "                   ('EAT', 46),\n",
            "                   ('SE', 21),\n",
            "                   ('NE', 19),\n",
            "                   ('NW', 19),\n",
            "                   ('NW_', 19),\n",
            "                   ('KICK', 18),\n",
            "                   ('DOWN', 10),\n",
            "                   ('RUSH', 8),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('FIRE', 6),\n",
            "                   ('NE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('PUTON', 1)],\n",
            " 'baseline_loss': 6181.9033203125,\n",
            " 'entropy_loss': -1.5622334480285645,\n",
            " 'episode_returns': (370.0, 267.0),\n",
            " 'mean_episode_return': 318.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -113.5011215209961,\n",
            " 'total_loss': 6066.84033203125}\n",
            "INFO:root:Steps 15557120 ( 95572480 of 10000000000 ) @ 31029.2 SPS. Loss 12284.943359. Return per episode: 304.0. Stats:\n",
            "{'action_counts': [('E', 480),\n",
            "                   ('W', 369),\n",
            "                   ('N', 346),\n",
            "                   ('S', 294),\n",
            "                   ('E_', 255),\n",
            "                   ('W_', 231),\n",
            "                   ('N_', 161),\n",
            "                   ('SEARCH', 150),\n",
            "                   ('S_', 56),\n",
            "                   ('NE', 44),\n",
            "                   ('SE', 35),\n",
            "                   ('SW', 27),\n",
            "                   ('NW', 24),\n",
            "                   ('NW_', 21),\n",
            "                   ('EAT', 19),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('RUSH', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('DOWN', 8),\n",
            "                   ('FIRE', 8),\n",
            "                   ('UP', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('LOOK', 1)],\n",
            " 'baseline_loss': 12193.310546875,\n",
            " 'entropy_loss': -1.6792047023773193,\n",
            " 'episode_returns': (219.0, 459.0, 234.0),\n",
            " 'mean_episode_return': 304.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 93.31230926513672,\n",
            " 'total_loss': 12284.943359375}\n",
            "INFO:root:Steps 15608320 ( 95623680 of 10000000000 ) @ 31129.3 SPS. Loss 10276.650391. Return per episode: 337.6. Stats:\n",
            "{'action_counts': [('W', 502),\n",
            "                   ('E', 421),\n",
            "                   ('N', 412),\n",
            "                   ('S', 369),\n",
            "                   ('SEARCH', 233),\n",
            "                   ('W_', 200),\n",
            "                   ('E_', 152),\n",
            "                   ('N_', 90),\n",
            "                   ('S_', 57),\n",
            "                   ('NE', 25),\n",
            "                   ('SW', 23),\n",
            "                   ('SE', 20),\n",
            "                   ('NW', 18),\n",
            "                   ('NW_', 10),\n",
            "                   ('EAT', 10),\n",
            "                   ('DOWN', 5),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('RUSH', 3),\n",
            "                   ('KICK', 2)],\n",
            " 'baseline_loss': 10453.169921875,\n",
            " 'entropy_loss': -1.3580878973007202,\n",
            " 'episode_returns': (272.0, 369.0, 383.0, 330.0, 334.0),\n",
            " 'mean_episode_return': 337.6000061035156,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -175.160888671875,\n",
            " 'total_loss': 10276.650390625}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 15656960 ( 95672320 of 10000000000 ) @ 31230.6 SPS. Loss 8742.339844. Return per episode: 594.7. Stats:\n",
            "{'action_counts': [('E', 466),\n",
            "                   ('SEARCH', 439),\n",
            "                   ('W', 396),\n",
            "                   ('S', 304),\n",
            "                   ('N', 279),\n",
            "                   ('W_', 191),\n",
            "                   ('E_', 151),\n",
            "                   ('N_', 84),\n",
            "                   ('S_', 66),\n",
            "                   ('NE', 47),\n",
            "                   ('NW', 44),\n",
            "                   ('SE', 19),\n",
            "                   ('SW', 18),\n",
            "                   ('NW_', 16),\n",
            "                   ('EAT', 11),\n",
            "                   ('KICK', 9),\n",
            "                   ('DOWN', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('FIRE', 2),\n",
            "                   ('UP', 1),\n",
            "                   ('MORE', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('PICKUP', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 8568.7421875,\n",
            " 'entropy_loss': -1.6501498222351074,\n",
            " 'episode_returns': (118.0, 618.0, 1048.0),\n",
            " 'mean_episode_return': 594.6666870117188,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 175.24827575683594,\n",
            " 'total_loss': 8742.33984375}\n",
            "INFO:root:Steps 15705600 ( 95720960 of 10000000000 ) @ 31330.7 SPS. Loss 9348.494141. Stats:\n",
            "{'action_counts': [('E', 467),\n",
            "                   ('W', 428),\n",
            "                   ('S', 366),\n",
            "                   ('N', 344),\n",
            "                   ('E_', 250),\n",
            "                   ('SEARCH', 182),\n",
            "                   ('W_', 163),\n",
            "                   ('N_', 152),\n",
            "                   ('S_', 81),\n",
            "                   ('SE', 26),\n",
            "                   ('SW', 22),\n",
            "                   ('NE', 20),\n",
            "                   ('NW', 18),\n",
            "                   ('DOWN', 12),\n",
            "                   ('KICK', 11),\n",
            "                   ('NW_', 8),\n",
            "                   ('EAT', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('SW_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 9137.24609375,\n",
            " 'entropy_loss': -1.3329650163650513,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': 212.5806427001953,\n",
            " 'total_loss': 9348.494140625}\n",
            "INFO:root:Steps 15759360 ( 95774720 of 10000000000 ) @ 31430.8 SPS. Loss 4054.796143. Stats:\n",
            "{'action_counts': [('E', 414),\n",
            "                   ('N', 361),\n",
            "                   ('W', 293),\n",
            "                   ('W_', 290),\n",
            "                   ('S', 281),\n",
            "                   ('E_', 271),\n",
            "                   ('SEARCH', 223),\n",
            "                   ('N_', 158),\n",
            "                   ('S_', 100),\n",
            "                   ('NW', 33),\n",
            "                   ('NE', 30),\n",
            "                   ('SW', 27),\n",
            "                   ('SE', 17),\n",
            "                   ('NW_', 17),\n",
            "                   ('EAT', 12),\n",
            "                   ('DOWN', 11),\n",
            "                   ('KICK', 8),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('NE_', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 4340.91796875,\n",
            " 'entropy_loss': -1.6559743881225586,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -284.46588134765625,\n",
            " 'total_loss': 4054.796142578125}\n",
            "INFO:root:Steps 15805440 ( 95820800 of 10000000000 ) @ 31530.9 SPS. Loss 9682.745117. Return per episode: 532.0. Stats:\n",
            "{'action_counts': [('E', 411),\n",
            "                   ('SEARCH', 396),\n",
            "                   ('W', 313),\n",
            "                   ('S', 289),\n",
            "                   ('N', 279),\n",
            "                   ('E_', 268),\n",
            "                   ('W_', 253),\n",
            "                   ('N_', 120),\n",
            "                   ('S_', 69),\n",
            "                   ('EAT', 24),\n",
            "                   ('NE', 22),\n",
            "                   ('SW', 21),\n",
            "                   ('SE', 17),\n",
            "                   ('NW_', 17),\n",
            "                   ('KICK', 15),\n",
            "                   ('RUSH', 13),\n",
            "                   ('DOWN', 10),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('NW', 7),\n",
            "                   ('FIRE', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 10012.1171875,\n",
            " 'entropy_loss': -1.6855231523513794,\n",
            " 'episode_returns': (532.0,),\n",
            " 'mean_episode_return': 532.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -327.6863708496094,\n",
            " 'total_loss': 9682.7451171875}\n",
            "INFO:root:Steps 15854080 ( 95869440 of 10000000000 ) @ 31631.0 SPS. Loss 10229.052734. Return per episode: 130.5. Stats:\n",
            "{'action_counts': [('W', 442),\n",
            "                   ('E', 340),\n",
            "                   ('S', 331),\n",
            "                   ('E_', 294),\n",
            "                   ('W_', 263),\n",
            "                   ('SEARCH', 258),\n",
            "                   ('N', 240),\n",
            "                   ('N_', 152),\n",
            "                   ('S_', 122),\n",
            "                   ('EAT', 20),\n",
            "                   ('SW', 18),\n",
            "                   ('NW_', 17),\n",
            "                   ('NE', 15),\n",
            "                   ('SE', 15),\n",
            "                   ('NW', 9),\n",
            "                   ('DOWN', 9),\n",
            "                   ('KICK', 5),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('APPLY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 9776.908203125,\n",
            " 'entropy_loss': -1.6028094291687012,\n",
            " 'episode_returns': (0.0, 261.0),\n",
            " 'mean_episode_return': 130.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 453.7471618652344,\n",
            " 'total_loss': 10229.052734375}\n",
            "INFO:root:Steps 15905280 ( 95920640 of 10000000000 ) @ 31731.1 SPS. Loss 8735.406250. Return per episode: 240.5. Stats:\n",
            "{'action_counts': [('E', 493),\n",
            "                   ('W', 474),\n",
            "                   ('S', 461),\n",
            "                   ('N', 280),\n",
            "                   ('SEARCH', 235),\n",
            "                   ('E_', 201),\n",
            "                   ('W_', 151),\n",
            "                   ('N_', 93),\n",
            "                   ('S_', 74),\n",
            "                   ('DOWN', 21),\n",
            "                   ('SW', 17),\n",
            "                   ('EAT', 11),\n",
            "                   ('NE', 8),\n",
            "                   ('NW', 8),\n",
            "                   ('KICK', 7),\n",
            "                   ('SE', 5),\n",
            "                   ('NW_', 5),\n",
            "                   ('FIRE', 5),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('RUSH', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('PICKUP', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 8867.318359375,\n",
            " 'entropy_loss': -1.3906071186065674,\n",
            " 'episode_returns': (176.0, 305.0),\n",
            " 'mean_episode_return': 240.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -130.52142333984375,\n",
            " 'total_loss': 8735.40625}\n",
            "INFO:root:Steps 15953920 ( 95969280 of 10000000000 ) @ 31831.2 SPS. Loss 12500.748047. Return per episode: 198.0. Stats:\n",
            "{'action_counts': [('E', 497),\n",
            "                   ('W', 478),\n",
            "                   ('S', 415),\n",
            "                   ('N', 344),\n",
            "                   ('SEARCH', 222),\n",
            "                   ('E_', 172),\n",
            "                   ('W_', 144),\n",
            "                   ('N_', 118),\n",
            "                   ('S_', 67),\n",
            "                   ('KICK', 17),\n",
            "                   ('SW', 16),\n",
            "                   ('NW', 14),\n",
            "                   ('EAT', 14),\n",
            "                   ('SE', 13),\n",
            "                   ('DOWN', 7),\n",
            "                   ('NE', 6),\n",
            "                   ('NW_', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('RUSH', 3)],\n",
            " 'baseline_loss': 12394.615234375,\n",
            " 'entropy_loss': -1.1824803352355957,\n",
            " 'episode_returns': (104.0, 292.0),\n",
            " 'mean_episode_return': 198.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 107.31517028808594,\n",
            " 'total_loss': 12500.748046875}\n",
            "INFO:root:Steps 16002560 ( 96017920 of 10000000000 ) @ 31931.3 SPS. Loss 10229.833984. Return per episode: 515.5. Stats:\n",
            "{'action_counts': [('E', 543),\n",
            "                   ('W', 496),\n",
            "                   ('S', 379),\n",
            "                   ('SEARCH', 352),\n",
            "                   ('N', 231),\n",
            "                   ('W_', 174),\n",
            "                   ('E_', 127),\n",
            "                   ('S_', 71),\n",
            "                   ('N_', 66),\n",
            "                   ('SW', 22),\n",
            "                   ('NW_', 15),\n",
            "                   ('EAT', 14),\n",
            "                   ('SE', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NE', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW', 7),\n",
            "                   ('FIRE', 7),\n",
            "                   ('UP', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 10114.6796875,\n",
            " 'entropy_loss': -1.1239410638809204,\n",
            " 'episode_returns': (502.0, 529.0),\n",
            " 'mean_episode_return': 515.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 116.27843475341797,\n",
            " 'total_loss': 10229.833984375}\n",
            "INFO:root:Steps 16053760 ( 96069120 of 10000000000 ) @ 32031.4 SPS. Loss 10422.051758. Return per episode: 302.0. Stats:\n",
            "{'action_counts': [('E', 514),\n",
            "                   ('W', 439),\n",
            "                   ('S', 355),\n",
            "                   ('N', 337),\n",
            "                   ('SEARCH', 310),\n",
            "                   ('W_', 176),\n",
            "                   ('E_', 152),\n",
            "                   ('N_', 114),\n",
            "                   ('S_', 69),\n",
            "                   ('EAT', 14),\n",
            "                   ('DOWN', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('NE', 11),\n",
            "                   ('NW_', 10),\n",
            "                   ('SW', 9),\n",
            "                   ('SE', 6),\n",
            "                   ('NW', 6),\n",
            "                   ('RUSH', 5),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 10342.296875,\n",
            " 'entropy_loss': -1.171102523803711,\n",
            " 'episode_returns': (165.0, 295.0, 446.0),\n",
            " 'mean_episode_return': 302.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 80.92559814453125,\n",
            " 'total_loss': 10422.0517578125}\n",
            "INFO:root:Steps 16102400 ( 96117760 of 10000000000 ) @ 32131.5 SPS. Loss 7447.899902. Return per episode: 132.0. Stats:\n",
            "{'action_counts': [('W', 562),\n",
            "                   ('E', 450),\n",
            "                   ('S', 376),\n",
            "                   ('N', 315),\n",
            "                   ('W_', 188),\n",
            "                   ('E_', 167),\n",
            "                   ('SEARCH', 154),\n",
            "                   ('N_', 101),\n",
            "                   ('S_', 70),\n",
            "                   ('SE', 32),\n",
            "                   ('SW', 26),\n",
            "                   ('NW', 26),\n",
            "                   ('EAT', 23),\n",
            "                   ('NE', 16),\n",
            "                   ('NW_', 13),\n",
            "                   ('DOWN', 13),\n",
            "                   ('KICK', 8),\n",
            "                   ('FIRE', 7),\n",
            "                   ('RUSH', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('CLOSE', 2),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 7219.8564453125,\n",
            " 'entropy_loss': -1.5819398164749146,\n",
            " 'episode_returns': (132.0,),\n",
            " 'mean_episode_return': 132.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 229.62539672851562,\n",
            " 'total_loss': 7447.89990234375}\n",
            "INFO:root:Steps 16151040 ( 96166400 of 10000000000 ) @ 32231.6 SPS. Loss 8351.683594. Return per episode: 377.0. Stats:\n",
            "{'action_counts': [('E', 626),\n",
            "                   ('W', 449),\n",
            "                   ('S', 403),\n",
            "                   ('N', 291),\n",
            "                   ('SEARCH', 180),\n",
            "                   ('W_', 168),\n",
            "                   ('E_', 123),\n",
            "                   ('N_', 71),\n",
            "                   ('S_', 68),\n",
            "                   ('SW', 35),\n",
            "                   ('EAT', 31),\n",
            "                   ('SE', 27),\n",
            "                   ('NW', 19),\n",
            "                   ('NW_', 18),\n",
            "                   ('RUSH', 12),\n",
            "                   ('DOWN', 10),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('FIRE', 7),\n",
            "                   ('NE', 6),\n",
            "                   ('KICK', 4),\n",
            "                   ('WAIT', 1),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 8358.7509765625,\n",
            " 'entropy_loss': -1.548093318939209,\n",
            " 'episode_returns': (346.0, 408.0),\n",
            " 'mean_episode_return': 377.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -5.51934814453125,\n",
            " 'total_loss': 8351.68359375}\n",
            "INFO:root:Steps 16199680 ( 96215040 of 10000000000 ) @ 32331.8 SPS. Loss 8897.829102. Return per episode: 156.0. Stats:\n",
            "{'action_counts': [('W', 600),\n",
            "                   ('E', 527),\n",
            "                   ('N', 375),\n",
            "                   ('S', 302),\n",
            "                   ('SEARCH', 195),\n",
            "                   ('E_', 130),\n",
            "                   ('W_', 117),\n",
            "                   ('S_', 88),\n",
            "                   ('N_', 84),\n",
            "                   ('NE', 30),\n",
            "                   ('NW', 28),\n",
            "                   ('SW', 16),\n",
            "                   ('DOWN', 14),\n",
            "                   ('NW_', 13),\n",
            "                   ('SE', 12),\n",
            "                   ('EAT', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('RUSH', 2),\n",
            "                   ('LOOK', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 8686.775390625,\n",
            " 'entropy_loss': -1.3930943012237549,\n",
            " 'episode_returns': (156.0,),\n",
            " 'mean_episode_return': 156.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 212.44735717773438,\n",
            " 'total_loss': 8897.8291015625}\n",
            "INFO:root:Steps 16250880 ( 96266240 of 10000000000 ) @ 32431.9 SPS. Loss 9120.224609. Return per episode: 482.0. Stats:\n",
            "{'action_counts': [('W', 507),\n",
            "                   ('E', 453),\n",
            "                   ('S', 424),\n",
            "                   ('N', 308),\n",
            "                   ('SEARCH', 203),\n",
            "                   ('W_', 177),\n",
            "                   ('E_', 132),\n",
            "                   ('N_', 85),\n",
            "                   ('SW', 43),\n",
            "                   ('S_', 38),\n",
            "                   ('NW', 37),\n",
            "                   ('SE', 27),\n",
            "                   ('EAT', 27),\n",
            "                   ('NW_', 23),\n",
            "                   ('NE', 17),\n",
            "                   ('RUSH', 16),\n",
            "                   ('KICK', 12),\n",
            "                   ('INVENTORY', 11),\n",
            "                   ('DOWN', 9),\n",
            "                   ('FIRE', 5),\n",
            "                   ('NE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('DROP', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('PAY', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 9092.40234375,\n",
            " 'entropy_loss': -1.658599853515625,\n",
            " 'episode_returns': (300.0, 197.0, 731.0, 700.0),\n",
            " 'mean_episode_return': 482.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 29.480514526367188,\n",
            " 'total_loss': 9120.224609375}\n",
            "INFO:root:Steps 16299520 ( 96314880 of 10000000000 ) @ 32532.0 SPS. Loss 5336.991211. Return per episode: 366.9. Stats:\n",
            "{'action_counts': [('E', 618),\n",
            "                   ('W', 497),\n",
            "                   ('S', 394),\n",
            "                   ('N', 242),\n",
            "                   ('W_', 233),\n",
            "                   ('E_', 178),\n",
            "                   ('SEARCH', 124),\n",
            "                   ('N_', 83),\n",
            "                   ('S_', 57),\n",
            "                   ('SE', 32),\n",
            "                   ('SW', 21),\n",
            "                   ('NE', 15),\n",
            "                   ('NW', 12),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW_', 11),\n",
            "                   ('EAT', 11),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 5),\n",
            "                   ('FIRE', 4),\n",
            "                   ('UP', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 5918.34228515625,\n",
            " 'entropy_loss': -1.4093846082687378,\n",
            " 'episode_returns': (551.0, 397.0, 250.0, 390.0, 314.0, 310.0, 356.0),\n",
            " 'mean_episode_return': 366.8571472167969,\n",
            " 'num_episodes': 7,\n",
            " 'pg_loss': -579.9417114257812,\n",
            " 'total_loss': 5336.9912109375}\n",
            "INFO:root:Steps 16348160 ( 96363520 of 10000000000 ) @ 32632.1 SPS. Loss 9636.500977. Stats:\n",
            "{'action_counts': [('W', 451),\n",
            "                   ('E', 376),\n",
            "                   ('S', 356),\n",
            "                   ('N', 290),\n",
            "                   ('W_', 274),\n",
            "                   ('SEARCH', 256),\n",
            "                   ('E_', 235),\n",
            "                   ('N_', 124),\n",
            "                   ('S_', 67),\n",
            "                   ('EAT', 21),\n",
            "                   ('SE', 19),\n",
            "                   ('SW', 17),\n",
            "                   ('INVENTORY', 15),\n",
            "                   ('NW', 12),\n",
            "                   ('NW_', 11),\n",
            "                   ('NE', 9),\n",
            "                   ('RUSH', 8),\n",
            "                   ('DOWN', 6),\n",
            "                   ('FIRE', 6),\n",
            "                   ('KICK', 4),\n",
            "                   ('UP', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 9648.1982421875,\n",
            " 'entropy_loss': -1.456454873085022,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -10.240997314453125,\n",
            " 'total_loss': 9636.5009765625}\n",
            "INFO:root:Steps 16396800 ( 96412160 of 10000000000 ) @ 32732.2 SPS. Loss 9904.740234. Return per episode: 460.0. Stats:\n",
            "{'action_counts': [('E', 607),\n",
            "                   ('N', 407),\n",
            "                   ('S', 299),\n",
            "                   ('W', 296),\n",
            "                   ('SEARCH', 268),\n",
            "                   ('W_', 251),\n",
            "                   ('E_', 173),\n",
            "                   ('N_', 98),\n",
            "                   ('S_', 67),\n",
            "                   ('NE', 14),\n",
            "                   ('DOWN', 13),\n",
            "                   ('SE', 10),\n",
            "                   ('EAT', 10),\n",
            "                   ('KICK', 10),\n",
            "                   ('SW', 9),\n",
            "                   ('NW', 7),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 10070.9482421875,\n",
            " 'entropy_loss': -1.3332304954528809,\n",
            " 'episode_returns': (460.0,),\n",
            " 'mean_episode_return': 460.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -164.8748321533203,\n",
            " 'total_loss': 9904.740234375}\n",
            "INFO:root:Steps 16445440 ( 96460800 of 10000000000 ) @ 32832.3 SPS. Loss 11365.418945. Return per episode: 375.0. Stats:\n",
            "{'action_counts': [('E', 572),\n",
            "                   ('S', 342),\n",
            "                   ('W', 333),\n",
            "                   ('W_', 277),\n",
            "                   ('N', 259),\n",
            "                   ('E_', 229),\n",
            "                   ('SEARCH', 215),\n",
            "                   ('N_', 121),\n",
            "                   ('S_', 70),\n",
            "                   ('EAT', 26),\n",
            "                   ('NW_', 23),\n",
            "                   ('SW', 21),\n",
            "                   ('SE', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('NW', 14),\n",
            "                   ('DOWN', 10),\n",
            "                   ('RUSH', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('KICK', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('ESC', 1)],\n",
            " 'baseline_loss': 11385.66796875,\n",
            " 'entropy_loss': -1.4640557765960693,\n",
            " 'episode_returns': (433.0, 317.0),\n",
            " 'mean_episode_return': 375.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -18.785057067871094,\n",
            " 'total_loss': 11365.4189453125}\n",
            "INFO:root:Steps 16496640 ( 96512000 of 10000000000 ) @ 32932.4 SPS. Loss 2932.837646. Return per episode: 512.5. Stats:\n",
            "{'action_counts': [('W', 436),\n",
            "                   ('E', 399),\n",
            "                   ('S', 352),\n",
            "                   ('N', 335),\n",
            "                   ('SEARCH', 269),\n",
            "                   ('W_', 196),\n",
            "                   ('E_', 180),\n",
            "                   ('N_', 118),\n",
            "                   ('S_', 70),\n",
            "                   ('SW', 40),\n",
            "                   ('EAT', 33),\n",
            "                   ('NW', 31),\n",
            "                   ('NE', 20),\n",
            "                   ('INVENTORY', 17),\n",
            "                   ('KICK', 13),\n",
            "                   ('SE', 11),\n",
            "                   ('DOWN', 11),\n",
            "                   ('NW_', 10),\n",
            "                   ('RUSH', 9),\n",
            "                   ('FIRE', 5),\n",
            "                   ('CLOSE', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('APPLY', 1)],\n",
            " 'baseline_loss': 3331.3935546875,\n",
            " 'entropy_loss': -1.7578479051589966,\n",
            " 'episode_returns': (747.0, 278.0),\n",
            " 'mean_episode_return': 512.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -396.79803466796875,\n",
            " 'total_loss': 2932.837646484375}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 16547840 ( 96563200 of 10000000000 ) @ 33033.6 SPS. Loss 10518.968750. Return per episode: 126.0. Stats:\n",
            "{'action_counts': [('W', 422),\n",
            "                   ('E', 415),\n",
            "                   ('S', 390),\n",
            "                   ('N', 316),\n",
            "                   ('W_', 262),\n",
            "                   ('E_', 217),\n",
            "                   ('SEARCH', 173),\n",
            "                   ('N_', 131),\n",
            "                   ('S_', 96),\n",
            "                   ('SW', 31),\n",
            "                   ('SE', 21),\n",
            "                   ('NW_', 17),\n",
            "                   ('NW', 15),\n",
            "                   ('NE', 13),\n",
            "                   ('EAT', 12),\n",
            "                   ('DOWN', 9),\n",
            "                   ('KICK', 7),\n",
            "                   ('RUSH', 7),\n",
            "                   ('FIRE', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 9950.4794921875,\n",
            " 'entropy_loss': -1.501586675643921,\n",
            " 'episode_returns': (126.0,),\n",
            " 'mean_episode_return': 126.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 569.9916381835938,\n",
            " 'total_loss': 10518.96875}\n",
            "INFO:root:Steps 16596480 ( 96611840 of 10000000000 ) @ 33133.7 SPS. Loss 7987.098145. Stats:\n",
            "{'action_counts': [('E', 459),\n",
            "                   ('N', 450),\n",
            "                   ('S', 347),\n",
            "                   ('W', 302),\n",
            "                   ('E_', 218),\n",
            "                   ('SEARCH', 189),\n",
            "                   ('W_', 137),\n",
            "                   ('N_', 126),\n",
            "                   ('S_', 123),\n",
            "                   ('NW', 39),\n",
            "                   ('SW', 31),\n",
            "                   ('EAT', 22),\n",
            "                   ('NE', 21),\n",
            "                   ('INVENTORY', 21),\n",
            "                   ('SE', 19),\n",
            "                   ('NW_', 17),\n",
            "                   ('DOWN', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 8),\n",
            "                   ('FIRE', 5),\n",
            "                   ('UP', 1),\n",
            "                   ('MORE', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 8072.8212890625,\n",
            " 'entropy_loss': -1.906665563583374,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -83.81631469726562,\n",
            " 'total_loss': 7987.09814453125}\n",
            "INFO:root:Steps 16645120 ( 96660480 of 10000000000 ) @ 33233.8 SPS. Loss 9270.263672. Return per episode: 589.2. Stats:\n",
            "{'action_counts': [('E', 402),\n",
            "                   ('W', 375),\n",
            "                   ('N', 371),\n",
            "                   ('S', 317),\n",
            "                   ('E_', 280),\n",
            "                   ('N_', 159),\n",
            "                   ('SEARCH', 158),\n",
            "                   ('S_', 142),\n",
            "                   ('W_', 127),\n",
            "                   ('SW', 84),\n",
            "                   ('NW', 34),\n",
            "                   ('SE', 28),\n",
            "                   ('NE', 21),\n",
            "                   ('NW_', 19),\n",
            "                   ('EAT', 13),\n",
            "                   ('KICK', 7),\n",
            "                   ('RUSH', 6),\n",
            "                   ('DOWN', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('PICKUP', 3),\n",
            "                   ('NE_', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('UP', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 9597.783203125,\n",
            " 'entropy_loss': -1.7705904245376587,\n",
            " 'episode_returns': (990.0, 394.0, 768.0, 205.0),\n",
            " 'mean_episode_return': 589.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -325.74884033203125,\n",
            " 'total_loss': 9270.263671875}\n",
            "INFO:root:Steps 16693760 ( 96709120 of 10000000000 ) @ 33333.9 SPS. Loss 8837.877930. Return per episode: 225.0. Stats:\n",
            "{'action_counts': [('W', 466),\n",
            "                   ('E', 421),\n",
            "                   ('N', 420),\n",
            "                   ('S', 292),\n",
            "                   ('SEARCH', 216),\n",
            "                   ('E_', 162),\n",
            "                   ('W_', 151),\n",
            "                   ('S_', 134),\n",
            "                   ('N_', 95),\n",
            "                   ('SW', 32),\n",
            "                   ('NW', 31),\n",
            "                   ('EAT', 23),\n",
            "                   ('DOWN', 18),\n",
            "                   ('RUSH', 18),\n",
            "                   ('NW_', 16),\n",
            "                   ('SE', 14),\n",
            "                   ('NE', 13),\n",
            "                   ('INVENTORY', 13),\n",
            "                   ('FIRE', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('APPLY', 1)],\n",
            " 'baseline_loss': 8478.2421875,\n",
            " 'entropy_loss': -1.545870304107666,\n",
            " 'episode_returns': (225.0,),\n",
            " 'mean_episode_return': 225.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 361.1820068359375,\n",
            " 'total_loss': 8837.8779296875}\n",
            "INFO:root:Steps 16744960 ( 96760320 of 10000000000 ) @ 33434.0 SPS. Loss 4525.485840. Return per episode: 259.0. Stats:\n",
            "{'action_counts': [('W', 539),\n",
            "                   ('E', 382),\n",
            "                   ('S', 373),\n",
            "                   ('N', 331),\n",
            "                   ('W_', 269),\n",
            "                   ('E_', 230),\n",
            "                   ('N_', 105),\n",
            "                   ('SEARCH', 101),\n",
            "                   ('S_', 64),\n",
            "                   ('SE', 39),\n",
            "                   ('NW', 29),\n",
            "                   ('SW', 25),\n",
            "                   ('NW_', 23),\n",
            "                   ('DOWN', 9),\n",
            "                   ('EAT', 9),\n",
            "                   ('KICK', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('RUSH', 4),\n",
            "                   ('MORE', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 4572.6708984375,\n",
            " 'entropy_loss': -1.6672264337539673,\n",
            " 'episode_returns': (221.0, 360.0, 326.0, 246.0, 349.0, 52.0),\n",
            " 'mean_episode_return': 259.0,\n",
            " 'num_episodes': 6,\n",
            " 'pg_loss': -45.51821517944336,\n",
            " 'total_loss': 4525.48583984375}\n",
            "/usr/local/lib/python3.10/dist-packages/gymnasium/wrappers/common.py:295: UserWarning: Not in moveloop after 1000 tries, aborting (ttyrec: None).\n",
            "  return self.env.reset(seed=seed, options=options)\n",
            "INFO:root:Steps 16793600 ( 96808960 of 10000000000 ) @ 33534.1 SPS. Loss 6463.040039. Return per episode: 947.0. Stats:\n",
            "{'action_counts': [('W', 585),\n",
            "                   ('E', 435),\n",
            "                   ('N', 314),\n",
            "                   ('S', 261),\n",
            "                   ('E_', 241),\n",
            "                   ('SEARCH', 181),\n",
            "                   ('W_', 165),\n",
            "                   ('N_', 107),\n",
            "                   ('S_', 80),\n",
            "                   ('SW', 42),\n",
            "                   ('SE', 21),\n",
            "                   ('INVENTORY', 18),\n",
            "                   ('RUSH', 18),\n",
            "                   ('NW', 17),\n",
            "                   ('EAT', 16),\n",
            "                   ('FIRE', 14),\n",
            "                   ('NW_', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('DOWN', 11),\n",
            "                   ('NE', 6),\n",
            "                   ('APPLY', 2),\n",
            "                   ('SE_', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 6655.96240234375,\n",
            " 'entropy_loss': -1.587956428527832,\n",
            " 'episode_returns': (352.0, 900.0, 12.0, 2524.0),\n",
            " 'mean_episode_return': 947.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -191.33453369140625,\n",
            " 'total_loss': 6463.0400390625}\n",
            "INFO:root:Steps 16842240 ( 96857600 of 10000000000 ) @ 33634.2 SPS. Loss 9996.278320. Return per episode: 179.3. Stats:\n",
            "{'action_counts': [('W', 498),\n",
            "                   ('E', 355),\n",
            "                   ('S', 350),\n",
            "                   ('N', 272),\n",
            "                   ('SEARCH', 272),\n",
            "                   ('E_', 250),\n",
            "                   ('W_', 162),\n",
            "                   ('N_', 149),\n",
            "                   ('S_', 115),\n",
            "                   ('SE', 23),\n",
            "                   ('SW', 22),\n",
            "                   ('EAT', 21),\n",
            "                   ('NW', 19),\n",
            "                   ('NW_', 16),\n",
            "                   ('RUSH', 11),\n",
            "                   ('FIRE', 7),\n",
            "                   ('DOWN', 6),\n",
            "                   ('KICK', 6),\n",
            "                   ('NE', 3),\n",
            "                   ('SW_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 10306.859375,\n",
            " 'entropy_loss': -1.5068873167037964,\n",
            " 'episode_returns': (251.0, 103.0, 184.0),\n",
            " 'mean_episode_return': 179.33334350585938,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -309.07403564453125,\n",
            " 'total_loss': 9996.2783203125}\n",
            "INFO:root:Steps 16890880 ( 96906240 of 10000000000 ) @ 33734.3 SPS. Loss 9156.151367. Return per episode: 406.2. Stats:\n",
            "{'action_counts': [('W', 599),\n",
            "                   ('E', 443),\n",
            "                   ('S', 309),\n",
            "                   ('N', 298),\n",
            "                   ('E_', 229),\n",
            "                   ('W_', 173),\n",
            "                   ('SEARCH', 168),\n",
            "                   ('N_', 112),\n",
            "                   ('S_', 69),\n",
            "                   ('SE', 26),\n",
            "                   ('SW', 26),\n",
            "                   ('EAT', 20),\n",
            "                   ('KICK', 18),\n",
            "                   ('NW', 16),\n",
            "                   ('NW_', 15),\n",
            "                   ('FIRE', 10),\n",
            "                   ('DOWN', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('NE', 5),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('ESC', 1),\n",
            "                   ('REMOVE', 1),\n",
            "                   ('WIELD', 1)],\n",
            " 'baseline_loss': 9371.15625,\n",
            " 'entropy_loss': -1.5291001796722412,\n",
            " 'episode_returns': (621.0, 401.0, 296.0, 307.0),\n",
            " 'mean_episode_return': 406.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -213.475341796875,\n",
            " 'total_loss': 9156.1513671875}\n",
            "INFO:root:Steps 16942080 ( 96957440 of 10000000000 ) @ 33834.4 SPS. Loss 11477.874023. Return per episode: 36.0. Stats:\n",
            "{'action_counts': [('E', 557),\n",
            "                   ('W', 542),\n",
            "                   ('N', 349),\n",
            "                   ('S', 288),\n",
            "                   ('E_', 233),\n",
            "                   ('SEARCH', 178),\n",
            "                   ('W_', 157),\n",
            "                   ('N_', 108),\n",
            "                   ('S_', 50),\n",
            "                   ('DOWN', 20),\n",
            "                   ('NW_', 13),\n",
            "                   ('SW', 11),\n",
            "                   ('KICK', 11),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('EAT', 6),\n",
            "                   ('RUSH', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('APPLY', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 11141.228515625,\n",
            " 'entropy_loss': -1.2059872150421143,\n",
            " 'episode_returns': (36.0,),\n",
            " 'mean_episode_return': 36.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 337.8516540527344,\n",
            " 'total_loss': 11477.8740234375}\n",
            "INFO:root:Steps 16988160 ( 97003520 of 10000000000 ) @ 33934.5 SPS. Loss 11720.182617. Return per episode: 527.5. Stats:\n",
            "{'action_counts': [('W', 690),\n",
            "                   ('E', 540),\n",
            "                   ('N', 296),\n",
            "                   ('S', 231),\n",
            "                   ('E_', 225),\n",
            "                   ('SEARCH', 219),\n",
            "                   ('W_', 112),\n",
            "                   ('N_', 72),\n",
            "                   ('NW', 32),\n",
            "                   ('S_', 27),\n",
            "                   ('NE', 24),\n",
            "                   ('SE', 23),\n",
            "                   ('DOWN', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('NW_', 11),\n",
            "                   ('EAT', 11),\n",
            "                   ('SW', 9),\n",
            "                   ('FIRE', 4),\n",
            "                   ('RUSH', 3),\n",
            "                   ('SE_', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 11482.994140625,\n",
            " 'entropy_loss': -1.2995662689208984,\n",
            " 'episode_returns': (382.0, 418.0, 627.0, 683.0),\n",
            " 'mean_episode_return': 527.5,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 238.48846435546875,\n",
            " 'total_loss': 11720.1826171875}\n",
            "INFO:root:Steps 17039360 ( 97054720 of 10000000000 ) @ 34034.6 SPS. Loss 9421.806641. Return per episode: 1019.0. Stats:\n",
            "{'action_counts': [('W', 621),\n",
            "                   ('E', 505),\n",
            "                   ('S', 423),\n",
            "                   ('N', 285),\n",
            "                   ('SEARCH', 199),\n",
            "                   ('E_', 161),\n",
            "                   ('W_', 96),\n",
            "                   ('N_', 87),\n",
            "                   ('S_', 87),\n",
            "                   ('SW', 21),\n",
            "                   ('SE', 17),\n",
            "                   ('NW_', 14),\n",
            "                   ('NW', 13),\n",
            "                   ('NE', 10),\n",
            "                   ('DOWN', 8),\n",
            "                   ('KICK', 6),\n",
            "                   ('EAT', 4),\n",
            "                   ('UP', 2),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 9565.7080078125,\n",
            " 'entropy_loss': -1.1355724334716797,\n",
            " 'episode_returns': (1019.0,),\n",
            " 'mean_episode_return': 1019.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -142.76571655273438,\n",
            " 'total_loss': 9421.806640625}\n",
            "INFO:root:Steps 17088000 ( 97103360 of 10000000000 ) @ 34134.7 SPS. Loss 9050.595703. Return per episode: 214.7. Stats:\n",
            "{'action_counts': [('W', 694),\n",
            "                   ('E', 464),\n",
            "                   ('S', 289),\n",
            "                   ('N', 261),\n",
            "                   ('E_', 204),\n",
            "                   ('SEARCH', 202),\n",
            "                   ('W_', 124),\n",
            "                   ('N_', 81),\n",
            "                   ('S_', 68),\n",
            "                   ('NW_', 32),\n",
            "                   ('SW', 29),\n",
            "                   ('EAT', 26),\n",
            "                   ('SE', 21),\n",
            "                   ('NW', 20),\n",
            "                   ('DOWN', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('RUSH', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('KICK', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 8538.4296875,\n",
            " 'entropy_loss': -1.5443223714828491,\n",
            " 'episode_returns': (104.0, 321.0, 219.0),\n",
            " 'mean_episode_return': 214.6666717529297,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 513.710205078125,\n",
            " 'total_loss': 9050.595703125}\n",
            "INFO:root:Steps 17136640 ( 97152000 of 10000000000 ) @ 34234.8 SPS. Loss 11245.657227. Return per episode: 450.7. Stats:\n",
            "{'action_counts': [('W', 626),\n",
            "                   ('E', 403),\n",
            "                   ('S', 340),\n",
            "                   ('N', 272),\n",
            "                   ('SEARCH', 201),\n",
            "                   ('E_', 193),\n",
            "                   ('W_', 191),\n",
            "                   ('S_', 113),\n",
            "                   ('N_', 101),\n",
            "                   ('SE', 23),\n",
            "                   ('SW', 22),\n",
            "                   ('EAT', 16),\n",
            "                   ('NW_', 15),\n",
            "                   ('NE', 10),\n",
            "                   ('DOWN', 9),\n",
            "                   ('NW', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('SE_', 1)],\n",
            " 'baseline_loss': 10971.0859375,\n",
            " 'entropy_loss': -1.421550989151001,\n",
            " 'episode_returns': (352.0, 185.0, 348.0, 641.0, 838.0, 340.0),\n",
            " 'mean_episode_return': 450.66668701171875,\n",
            " 'num_episodes': 6,\n",
            " 'pg_loss': 275.993408203125,\n",
            " 'total_loss': 11245.6572265625}\n",
            "INFO:root:Steps 17185280 ( 97200640 of 10000000000 ) @ 34334.9 SPS. Loss 6478.972168. Return per episode: 232.5. Stats:\n",
            "{'action_counts': [('W', 516),\n",
            "                   ('E', 447),\n",
            "                   ('N', 363),\n",
            "                   ('S', 319),\n",
            "                   ('E_', 257),\n",
            "                   ('SEARCH', 180),\n",
            "                   ('W_', 178),\n",
            "                   ('N_', 100),\n",
            "                   ('S_', 40),\n",
            "                   ('SW', 22),\n",
            "                   ('NW_', 22),\n",
            "                   ('SE', 19),\n",
            "                   ('EAT', 17),\n",
            "                   ('NW', 16),\n",
            "                   ('DOWN', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('KICK', 11),\n",
            "                   ('RUSH', 8),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('FIRE', 6),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 6477.181640625,\n",
            " 'entropy_loss': -1.5990664958953857,\n",
            " 'episode_returns': (244.0, 221.0),\n",
            " 'mean_episode_return': 232.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 3.389537811279297,\n",
            " 'total_loss': 6478.97216796875}\n",
            "INFO:root:Steps 17236480 ( 97251840 of 10000000000 ) @ 34435.0 SPS. Loss 18392.808594. Return per episode: 368.3. Stats:\n",
            "{'action_counts': [('E', 702),\n",
            "                   ('N', 427),\n",
            "                   ('S', 424),\n",
            "                   ('W', 389),\n",
            "                   ('W_', 125),\n",
            "                   ('SEARCH', 106),\n",
            "                   ('E_', 91),\n",
            "                   ('N_', 85),\n",
            "                   ('S_', 73),\n",
            "                   ('SW', 24),\n",
            "                   ('SE', 21),\n",
            "                   ('NW', 20),\n",
            "                   ('NE', 17),\n",
            "                   ('DOWN', 17),\n",
            "                   ('NW_', 12),\n",
            "                   ('KICK', 8),\n",
            "                   ('EAT', 7),\n",
            "                   ('RUSH', 5),\n",
            "                   ('NE_', 2),\n",
            "                   ('FIRE', 2),\n",
            "                   ('PAY', 2),\n",
            "                   ('SE_', 1)],\n",
            " 'baseline_loss': 17531.875,\n",
            " 'entropy_loss': -1.092581868171692,\n",
            " 'episode_returns': (432.0, 540.0, 344.0, 205.0, 303.0, 386.0),\n",
            " 'mean_episode_return': 368.3333435058594,\n",
            " 'num_episodes': 6,\n",
            " 'pg_loss': 862.0255737304688,\n",
            " 'total_loss': 18392.80859375}\n",
            "INFO:root:Steps 17285120 ( 97300480 of 10000000000 ) @ 34535.2 SPS. Loss 7510.472168. Return per episode: 348.2. Stats:\n",
            "{'action_counts': [('E', 567),\n",
            "                   ('S', 411),\n",
            "                   ('N', 356),\n",
            "                   ('W', 324),\n",
            "                   ('SEARCH', 273),\n",
            "                   ('W_', 199),\n",
            "                   ('E_', 187),\n",
            "                   ('N_', 84),\n",
            "                   ('S_', 44),\n",
            "                   ('SE', 22),\n",
            "                   ('SW', 20),\n",
            "                   ('NW', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('KICK', 15),\n",
            "                   ('DOWN', 10),\n",
            "                   ('NW_', 8),\n",
            "                   ('EAT', 7),\n",
            "                   ('RUSH', 2)],\n",
            " 'baseline_loss': 7677.7109375,\n",
            " 'entropy_loss': -1.2496614456176758,\n",
            " 'episode_returns': (177.0, 264.0, 421.0, 531.0),\n",
            " 'mean_episode_return': 348.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -165.989501953125,\n",
            " 'total_loss': 7510.47216796875}\n",
            "INFO:root:Steps 17333760 ( 97349120 of 10000000000 ) @ 34635.3 SPS. Loss 9932.987305. Return per episode: 213.5. Stats:\n",
            "{'action_counts': [('E', 605),\n",
            "                   ('W', 407),\n",
            "                   ('S', 313),\n",
            "                   ('N', 248),\n",
            "                   ('W_', 219),\n",
            "                   ('SEARCH', 194),\n",
            "                   ('E_', 168),\n",
            "                   ('N_', 136),\n",
            "                   ('S_', 94),\n",
            "                   ('SE', 33),\n",
            "                   ('SW', 28),\n",
            "                   ('NE', 27),\n",
            "                   ('NW_', 27),\n",
            "                   ('NW', 24),\n",
            "                   ('KICK', 13),\n",
            "                   ('EAT', 8),\n",
            "                   ('RUSH', 8),\n",
            "                   ('DOWN', 5),\n",
            "                   ('SE_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 9749.2880859375,\n",
            " 'entropy_loss': -1.6206653118133545,\n",
            " 'episode_returns': (157.0, 270.0),\n",
            " 'mean_episode_return': 213.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 185.3204345703125,\n",
            " 'total_loss': 9932.9873046875}\n",
            "INFO:root:Steps 17382400 ( 97397760 of 10000000000 ) @ 34735.4 SPS. Loss 6962.617188. Return per episode: 699.0. Stats:\n",
            "{'action_counts': [('E', 683),\n",
            "                   ('W', 434),\n",
            "                   ('SEARCH', 273),\n",
            "                   ('N', 267),\n",
            "                   ('S', 221),\n",
            "                   ('E_', 187),\n",
            "                   ('W_', 138),\n",
            "                   ('N_', 111),\n",
            "                   ('S_', 99),\n",
            "                   ('NE', 35),\n",
            "                   ('SE', 33),\n",
            "                   ('SW', 23),\n",
            "                   ('NW', 18),\n",
            "                   ('NW_', 15),\n",
            "                   ('KICK', 6),\n",
            "                   ('DOWN', 5),\n",
            "                   ('EAT', 5),\n",
            "                   ('RUSH', 2),\n",
            "                   ('ESC', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('MOVE', 1),\n",
            "                   ('PUTON', 1)],\n",
            " 'baseline_loss': 7126.505859375,\n",
            " 'entropy_loss': -1.2980387210845947,\n",
            " 'episode_returns': (594.0, 1085.0, 878.0, 502.0, 436.0),\n",
            " 'mean_episode_return': 699.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -162.5908660888672,\n",
            " 'total_loss': 6962.6171875}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 17431040 ( 97446400 of 10000000000 ) @ 34835.7 SPS. Loss 8058.031250. Return per episode: 225.8. Stats:\n",
            "{'action_counts': [('E', 549),\n",
            "                   ('W', 367),\n",
            "                   ('W_', 298),\n",
            "                   ('E_', 281),\n",
            "                   ('S', 247),\n",
            "                   ('N', 208),\n",
            "                   ('SEARCH', 205),\n",
            "                   ('N_', 174),\n",
            "                   ('SW', 60),\n",
            "                   ('S_', 57),\n",
            "                   ('SE', 30),\n",
            "                   ('NW', 17),\n",
            "                   ('EAT', 15),\n",
            "                   ('NW_', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('DOWN', 5),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SE_', 1),\n",
            "                   ('PUTON', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 8504.580078125,\n",
            " 'entropy_loss': -1.519695520401001,\n",
            " 'episode_returns': (32.0, 399.0, 205.0, 267.0),\n",
            " 'mean_episode_return': 225.75,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -445.0295104980469,\n",
            " 'total_loss': 8058.03125}\n",
            "INFO:root:Steps 17479680 ( 97495040 of 10000000000 ) @ 34935.8 SPS. Loss 8495.592773. Return per episode: 964.0. Stats:\n",
            "{'action_counts': [('E', 654),\n",
            "                   ('N', 349),\n",
            "                   ('W', 298),\n",
            "                   ('S', 226),\n",
            "                   ('E_', 213),\n",
            "                   ('W_', 205),\n",
            "                   ('N_', 155),\n",
            "                   ('SEARCH', 151),\n",
            "                   ('S_', 137),\n",
            "                   ('SW', 45),\n",
            "                   ('SE', 27),\n",
            "                   ('NW', 23),\n",
            "                   ('NE', 16),\n",
            "                   ('EAT', 16),\n",
            "                   ('NW_', 10),\n",
            "                   ('DOWN', 10),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('FIRE', 6),\n",
            "                   ('RUSH', 5),\n",
            "                   ('KICK', 3),\n",
            "                   ('WAIT', 1),\n",
            "                   ('PUTON', 1)],\n",
            " 'baseline_loss': 8510.697265625,\n",
            " 'entropy_loss': -1.6837117671966553,\n",
            " 'episode_returns': (1441.0, 487.0),\n",
            " 'mean_episode_return': 964.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -13.42117691040039,\n",
            " 'total_loss': 8495.5927734375}\n",
            "INFO:root:Steps 17530880 ( 97546240 of 10000000000 ) @ 35035.9 SPS. Loss 9415.634766. Return per episode: 224.0. Stats:\n",
            "{'action_counts': [('E', 565),\n",
            "                   ('W', 395),\n",
            "                   ('SEARCH', 338),\n",
            "                   ('S', 270),\n",
            "                   ('N', 226),\n",
            "                   ('E_', 169),\n",
            "                   ('N_', 149),\n",
            "                   ('W_', 134),\n",
            "                   ('S_', 84),\n",
            "                   ('SE', 50),\n",
            "                   ('SW', 48),\n",
            "                   ('EAT', 31),\n",
            "                   ('NW_', 27),\n",
            "                   ('NW', 24),\n",
            "                   ('NE', 10),\n",
            "                   ('DOWN', 10),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('RUSH', 8),\n",
            "                   ('KICK', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 9581.4736328125,\n",
            " 'entropy_loss': -1.76262366771698,\n",
            " 'episode_returns': (145.0, 303.0),\n",
            " 'mean_episode_return': 224.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -164.0763397216797,\n",
            " 'total_loss': 9415.634765625}\n",
            "INFO:root:Steps 17579520 ( 97594880 of 10000000000 ) @ 35136.0 SPS. Loss 9807.336914. Return per episode: 875.0. Stats:\n",
            "{'action_counts': [('E', 634),\n",
            "                   ('W', 494),\n",
            "                   ('S', 287),\n",
            "                   ('N', 263),\n",
            "                   ('SEARCH', 200),\n",
            "                   ('W_', 174),\n",
            "                   ('E_', 131),\n",
            "                   ('N_', 102),\n",
            "                   ('S_', 67),\n",
            "                   ('SW', 37),\n",
            "                   ('NW', 34),\n",
            "                   ('SE', 32),\n",
            "                   ('EAT', 25),\n",
            "                   ('NE', 15),\n",
            "                   ('INVENTORY', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('RUSH', 11),\n",
            "                   ('NW_', 9),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 9858.5302734375,\n",
            " 'entropy_loss': -1.5552747249603271,\n",
            " 'episode_returns': (891.0, 1086.0, 648.0),\n",
            " 'mean_episode_return': 875.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -49.63737487792969,\n",
            " 'total_loss': 9807.3369140625}\n",
            "INFO:root:Steps 17625600 ( 97640960 of 10000000000 ) @ 35236.1 SPS. Loss 11369.746094. Stats:\n",
            "{'action_counts': [('E', 678),\n",
            "                   ('SEARCH', 374),\n",
            "                   ('W', 315),\n",
            "                   ('N', 264),\n",
            "                   ('S', 241),\n",
            "                   ('E_', 177),\n",
            "                   ('N_', 163),\n",
            "                   ('W_', 156),\n",
            "                   ('S_', 73),\n",
            "                   ('SW', 30),\n",
            "                   ('EAT', 18),\n",
            "                   ('SE', 14),\n",
            "                   ('NW_', 14),\n",
            "                   ('NE', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('NW', 8),\n",
            "                   ('DOWN', 7),\n",
            "                   ('RUSH', 4),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 2)],\n",
            " 'baseline_loss': 11476.2578125,\n",
            " 'entropy_loss': -1.3789336681365967,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -105.13263702392578,\n",
            " 'total_loss': 11369.74609375}\n",
            "INFO:root:Steps 17676800 ( 97692160 of 10000000000 ) @ 35336.3 SPS. Loss 9988.809570. Return per episode: 542.5. Stats:\n",
            "{'action_counts': [('E', 642),\n",
            "                   ('W', 377),\n",
            "                   ('N', 344),\n",
            "                   ('S', 317),\n",
            "                   ('SEARCH', 288),\n",
            "                   ('W_', 213),\n",
            "                   ('E_', 108),\n",
            "                   ('N_', 98),\n",
            "                   ('S_', 67),\n",
            "                   ('SW', 41),\n",
            "                   ('SE', 14),\n",
            "                   ('KICK', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('NW', 9),\n",
            "                   ('DOWN', 9),\n",
            "                   ('RUSH', 5),\n",
            "                   ('NW_', 4),\n",
            "                   ('FIRE', 3)],\n",
            " 'baseline_loss': 10877.107421875,\n",
            " 'entropy_loss': -1.0589871406555176,\n",
            " 'episode_returns': (75.0, 1010.0),\n",
            " 'mean_episode_return': 542.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -887.239501953125,\n",
            " 'total_loss': 9988.8095703125}\n",
            "INFO:root:Steps 17725440 ( 97740800 of 10000000000 ) @ 35436.4 SPS. Loss 8382.536133. Stats:\n",
            "{'action_counts': [('E', 466),\n",
            "                   ('SEARCH', 388),\n",
            "                   ('W', 345),\n",
            "                   ('N', 284),\n",
            "                   ('W_', 281),\n",
            "                   ('S', 269),\n",
            "                   ('E_', 192),\n",
            "                   ('N_', 128),\n",
            "                   ('S_', 80),\n",
            "                   ('NW', 27),\n",
            "                   ('SW', 19),\n",
            "                   ('NE', 17),\n",
            "                   ('SE', 17),\n",
            "                   ('NW_', 12),\n",
            "                   ('KICK', 12),\n",
            "                   ('DOWN', 10),\n",
            "                   ('EAT', 8),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 8851.87890625,\n",
            " 'entropy_loss': -1.6145917177200317,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -467.7285461425781,\n",
            " 'total_loss': 8382.5361328125}\n",
            "INFO:root:Steps 17774080 ( 97789440 of 10000000000 ) @ 35536.5 SPS. Loss 6592.800293. Return per episode: 581.0. Stats:\n",
            "{'action_counts': [('E', 633),\n",
            "                   ('W', 506),\n",
            "                   ('S', 482),\n",
            "                   ('N', 396),\n",
            "                   ('W_', 141),\n",
            "                   ('SEARCH', 114),\n",
            "                   ('E_', 80),\n",
            "                   ('N_', 75),\n",
            "                   ('S_', 37),\n",
            "                   ('NE', 20),\n",
            "                   ('NW', 18),\n",
            "                   ('SW', 16),\n",
            "                   ('SE', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('KICK', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('EAT', 3),\n",
            "                   ('NW_', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 6548.1416015625,\n",
            " 'entropy_loss': -1.162994384765625,\n",
            " 'episode_returns': (378.0, 530.0, 428.0, 435.0, 1134.0),\n",
            " 'mean_episode_return': 581.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': 45.82163619995117,\n",
            " 'total_loss': 6592.80029296875}\n",
            "INFO:root:Steps 17822720 ( 97838080 of 10000000000 ) @ 35636.6 SPS. Loss 9394.507812. Return per episode: 362.0. Stats:\n",
            "{'action_counts': [('E', 609),\n",
            "                   ('W', 484),\n",
            "                   ('S', 292),\n",
            "                   ('N', 275),\n",
            "                   ('SEARCH', 263),\n",
            "                   ('E_', 183),\n",
            "                   ('W_', 147),\n",
            "                   ('N_', 103),\n",
            "                   ('S_', 72),\n",
            "                   ('NW', 29),\n",
            "                   ('SW', 19),\n",
            "                   ('EAT', 17),\n",
            "                   ('DOWN', 14),\n",
            "                   ('SE', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('FIRE', 5),\n",
            "                   ('NW_', 1)],\n",
            " 'baseline_loss': 9469.802734375,\n",
            " 'entropy_loss': -1.3418943881988525,\n",
            " 'episode_returns': (362.0,),\n",
            " 'mean_episode_return': 362.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -73.95330810546875,\n",
            " 'total_loss': 9394.5078125}\n",
            "INFO:root:Steps 17871360 ( 97886720 of 10000000000 ) @ 35736.8 SPS. Loss 10592.921875. Return per episode: 427.2. Stats:\n",
            "{'action_counts': [('E', 681),\n",
            "                   ('W', 525),\n",
            "                   ('N', 347),\n",
            "                   ('S', 304),\n",
            "                   ('SEARCH', 204),\n",
            "                   ('W_', 135),\n",
            "                   ('E_', 126),\n",
            "                   ('N_', 115),\n",
            "                   ('S_', 41),\n",
            "                   ('NE', 16),\n",
            "                   ('NW', 15),\n",
            "                   ('SW', 14),\n",
            "                   ('DOWN', 12),\n",
            "                   ('SE', 7),\n",
            "                   ('KICK', 7),\n",
            "                   ('NW_', 4),\n",
            "                   ('EAT', 2),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('RUSH', 1)],\n",
            " 'baseline_loss': 10819.966796875,\n",
            " 'entropy_loss': -1.0602748394012451,\n",
            " 'episode_returns': (996.0, 54.0, 382.0, 277.0),\n",
            " 'mean_episode_return': 427.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -225.98422241210938,\n",
            " 'total_loss': 10592.921875}\n",
            "INFO:root:Steps 17920000 ( 97935360 of 10000000000 ) @ 35836.9 SPS. Loss 7921.559570. Stats:\n",
            "{'action_counts': [('E', 785),\n",
            "                   ('W', 368),\n",
            "                   ('N', 278),\n",
            "                   ('S', 275),\n",
            "                   ('SEARCH', 222),\n",
            "                   ('E_', 149),\n",
            "                   ('N_', 136),\n",
            "                   ('W_', 135),\n",
            "                   ('S_', 61),\n",
            "                   ('INVENTORY', 34),\n",
            "                   ('EAT', 22),\n",
            "                   ('DOWN', 18),\n",
            "                   ('RUSH', 16),\n",
            "                   ('SW', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 8),\n",
            "                   ('FIRE', 8),\n",
            "                   ('NE', 7),\n",
            "                   ('NW_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 8166.4296875,\n",
            " 'entropy_loss': -1.297127366065979,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': -243.57260131835938,\n",
            " 'total_loss': 7921.5595703125}\n",
            "INFO:root:Steps 17968640 ( 97984000 of 10000000000 ) @ 35937.0 SPS. Loss 9701.851562. Return per episode: 188.3. Stats:\n",
            "{'action_counts': [('E', 599),\n",
            "                   ('W', 411),\n",
            "                   ('S', 307),\n",
            "                   ('N', 274),\n",
            "                   ('SEARCH', 271),\n",
            "                   ('E_', 203),\n",
            "                   ('W_', 144),\n",
            "                   ('N_', 110),\n",
            "                   ('S_', 82),\n",
            "                   ('INVENTORY', 30),\n",
            "                   ('SE', 29),\n",
            "                   ('SW', 24),\n",
            "                   ('NE', 16),\n",
            "                   ('RUSH', 13),\n",
            "                   ('EAT', 11),\n",
            "                   ('DOWN', 8),\n",
            "                   ('NW', 7),\n",
            "                   ('NW_', 7),\n",
            "                   ('KICK', 7),\n",
            "                   ('FIRE', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('APPLY', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 9453.0625,\n",
            " 'entropy_loss': -1.3206346035003662,\n",
            " 'episode_returns': (58.0, 130.0, 377.0),\n",
            " 'mean_episode_return': 188.33334350585938,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 250.10935974121094,\n",
            " 'total_loss': 9701.8515625}\n",
            "INFO:root:Steps 18017280 ( 98032640 of 10000000000 ) @ 36037.1 SPS. Loss 8811.969727. Return per episode: 355.5. Stats:\n",
            "{'action_counts': [('E', 602),\n",
            "                   ('W', 535),\n",
            "                   ('S', 367),\n",
            "                   ('N', 286),\n",
            "                   ('SEARCH', 284),\n",
            "                   ('E_', 99),\n",
            "                   ('W_', 96),\n",
            "                   ('N_', 72),\n",
            "                   ('S_', 65),\n",
            "                   ('NW', 37),\n",
            "                   ('SW', 32),\n",
            "                   ('NE', 26),\n",
            "                   ('SE', 23),\n",
            "                   ('DOWN', 18),\n",
            "                   ('NW_', 7),\n",
            "                   ('EAT', 5),\n",
            "                   ('KICK', 3),\n",
            "                   ('SW_', 1),\n",
            "                   ('FIRE', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 8754.01953125,\n",
            " 'entropy_loss': -1.1296336650848389,\n",
            " 'episode_returns': (247.0, 464.0),\n",
            " 'mean_episode_return': 355.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 59.08015060424805,\n",
            " 'total_loss': 8811.9697265625}\n",
            "INFO:root:Steps 18065920 ( 98081280 of 10000000000 ) @ 36137.3 SPS. Loss 9196.692383. Stats:\n",
            "{'action_counts': [('E', 689),\n",
            "                   ('W', 404),\n",
            "                   ('S', 321),\n",
            "                   ('SEARCH', 271),\n",
            "                   ('N', 258),\n",
            "                   ('E_', 143),\n",
            "                   ('N_', 116),\n",
            "                   ('W_', 115),\n",
            "                   ('S_', 70),\n",
            "                   ('SW', 28),\n",
            "                   ('NW', 20),\n",
            "                   ('EAT', 20),\n",
            "                   ('NE', 18),\n",
            "                   ('DOWN', 15),\n",
            "                   ('KICK', 15),\n",
            "                   ('RUSH', 15),\n",
            "                   ('SE', 12),\n",
            "                   ('INVENTORY', 11),\n",
            "                   ('NW_', 8),\n",
            "                   ('FIRE', 7),\n",
            "                   ('APPLY', 2),\n",
            "                   ('DROP', 1),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 8822.9453125,\n",
            " 'entropy_loss': -1.407486915588379,\n",
            " 'episode_returns': (),\n",
            " 'mean_episode_return': nan,\n",
            " 'num_episodes': 0,\n",
            " 'pg_loss': 375.15380859375,\n",
            " 'total_loss': 9196.6923828125}\n",
            "INFO:root:Steps 18114560 ( 98129920 of 10000000000 ) @ 36237.4 SPS. Loss 14326.106445. Return per episode: 510.8. Stats:\n",
            "{'action_counts': [('W', 658),\n",
            "                   ('E', 541),\n",
            "                   ('N', 351),\n",
            "                   ('S', 321),\n",
            "                   ('W_', 162),\n",
            "                   ('SEARCH', 130),\n",
            "                   ('E_', 93),\n",
            "                   ('N_', 88),\n",
            "                   ('S_', 85),\n",
            "                   ('EAT', 29),\n",
            "                   ('KICK', 15),\n",
            "                   ('SW', 13),\n",
            "                   ('NW', 13),\n",
            "                   ('DOWN', 13),\n",
            "                   ('NE', 12),\n",
            "                   ('RUSH', 11),\n",
            "                   ('SE', 7),\n",
            "                   ('FIRE', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('NW_', 4),\n",
            "                   ('NE_', 1)],\n",
            " 'baseline_loss': 14465.4814453125,\n",
            " 'entropy_loss': -1.151122808456421,\n",
            " 'episode_returns': (228.0, 838.0, 544.0, 868.0, 76.0),\n",
            " 'mean_episode_return': 510.8000183105469,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -138.2240447998047,\n",
            " 'total_loss': 14326.1064453125}\n",
            "INFO:root:Steps 18163200 ( 98178560 of 10000000000 ) @ 36337.5 SPS. Loss 8489.436523. Return per episode: 547.5. Stats:\n",
            "{'action_counts': [('E', 796),\n",
            "                   ('W', 409),\n",
            "                   ('N', 361),\n",
            "                   ('S', 334),\n",
            "                   ('SEARCH', 178),\n",
            "                   ('W_', 121),\n",
            "                   ('E_', 110),\n",
            "                   ('N_', 102),\n",
            "                   ('S_', 78),\n",
            "                   ('NW', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('SE', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('NE', 8),\n",
            "                   ('SW', 6),\n",
            "                   ('EAT', 5),\n",
            "                   ('FIRE', 3),\n",
            "                   ('NW_', 2),\n",
            "                   ('RUSH', 2),\n",
            "                   ('MORE', 1)],\n",
            " 'baseline_loss': 8432.640625,\n",
            " 'entropy_loss': -0.954639196395874,\n",
            " 'episode_returns': (914.0, 181.0),\n",
            " 'mean_episode_return': 547.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 57.75128936767578,\n",
            " 'total_loss': 8489.4365234375}\n",
            "INFO:root:Steps 18211840 ( 98227200 of 10000000000 ) @ 36437.6 SPS. Loss 8672.912109. Return per episode: 326.5. Stats:\n",
            "{'action_counts': [('E', 597),\n",
            "                   ('W', 526),\n",
            "                   ('S', 348),\n",
            "                   ('N', 310),\n",
            "                   ('W_', 175),\n",
            "                   ('E_', 146),\n",
            "                   ('SEARCH', 145),\n",
            "                   ('S_', 109),\n",
            "                   ('N_', 103),\n",
            "                   ('SW', 14),\n",
            "                   ('NE', 13),\n",
            "                   ('DOWN', 13),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 9),\n",
            "                   ('FIRE', 9),\n",
            "                   ('EAT', 8),\n",
            "                   ('NW_', 7),\n",
            "                   ('KICK', 7),\n",
            "                   ('RUSH', 7),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('PICKUP', 1)],\n",
            " 'baseline_loss': 8668.41015625,\n",
            " 'entropy_loss': -1.2319610118865967,\n",
            " 'episode_returns': (396.0, 257.0),\n",
            " 'mean_episode_return': 326.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 5.7342634201049805,\n",
            " 'total_loss': 8672.912109375}\n",
            "INFO:root:Steps 18260480 ( 98275840 of 10000000000 ) @ 36537.7 SPS. Loss 10738.545898. Return per episode: 51.5. Stats:\n",
            "{'action_counts': [('W', 638),\n",
            "                   ('E', 403),\n",
            "                   ('N', 307),\n",
            "                   ('S', 274),\n",
            "                   ('SEARCH', 257),\n",
            "                   ('W_', 184),\n",
            "                   ('E_', 120),\n",
            "                   ('N_', 74),\n",
            "                   ('S_', 62),\n",
            "                   ('SW', 49),\n",
            "                   ('NW', 34),\n",
            "                   ('EAT', 26),\n",
            "                   ('INVENTORY', 23),\n",
            "                   ('SE', 19),\n",
            "                   ('NE', 17),\n",
            "                   ('DOWN', 16),\n",
            "                   ('KICK', 15),\n",
            "                   ('FIRE', 12),\n",
            "                   ('RUSH', 12),\n",
            "                   ('NW_', 11),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('MORE', 1),\n",
            "                   ('CLOSE', 1),\n",
            "                   ('PUTON', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 10735.42578125,\n",
            " 'entropy_loss': -1.5475926399230957,\n",
            " 'episode_returns': (20.0, 83.0),\n",
            " 'mean_episode_return': 51.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 4.668243408203125,\n",
            " 'total_loss': 10738.5458984375}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 18309120 ( 98324480 of 10000000000 ) @ 36638.2 SPS. Loss 6483.865723. Return per episode: 332.0. Stats:\n",
            "{'action_counts': [('E', 490),\n",
            "                   ('W', 412),\n",
            "                   ('SEARCH', 309),\n",
            "                   ('S', 307),\n",
            "                   ('N', 234),\n",
            "                   ('E_', 211),\n",
            "                   ('W_', 187),\n",
            "                   ('N_', 159),\n",
            "                   ('S_', 125),\n",
            "                   ('SE', 31),\n",
            "                   ('SW', 21),\n",
            "                   ('NE', 17),\n",
            "                   ('NW', 16),\n",
            "                   ('NW_', 9),\n",
            "                   ('EAT', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 5),\n",
            "                   ('DOWN', 4),\n",
            "                   ('FIRE', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SW_', 1),\n",
            "                   ('WAIT', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 6569.48779296875,\n",
            " 'entropy_loss': -1.5557395219802856,\n",
            " 'episode_returns': (378.0, 286.0),\n",
            " 'mean_episode_return': 332.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -84.06646728515625,\n",
            " 'total_loss': 6483.86572265625}\n",
            "INFO:root:Steps 18357760 ( 98373120 of 10000000000 ) @ 36738.3 SPS. Loss 13445.576172. Return per episode: 399.0. Stats:\n",
            "{'action_counts': [('W', 563),\n",
            "                   ('E', 548),\n",
            "                   ('N', 289),\n",
            "                   ('S', 288),\n",
            "                   ('SEARCH', 284),\n",
            "                   ('E_', 159),\n",
            "                   ('W_', 96),\n",
            "                   ('S_', 94),\n",
            "                   ('N_', 91),\n",
            "                   ('SE', 28),\n",
            "                   ('SW', 26),\n",
            "                   ('NE', 24),\n",
            "                   ('EAT', 17),\n",
            "                   ('INVENTORY', 12),\n",
            "                   ('DOWN', 9),\n",
            "                   ('NW', 8),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NW_', 2)],\n",
            " 'baseline_loss': 13534.564453125,\n",
            " 'entropy_loss': -1.2348730564117432,\n",
            " 'episode_returns': (399.0,),\n",
            " 'mean_episode_return': 399.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -87.75336456298828,\n",
            " 'total_loss': 13445.576171875}\n",
            "INFO:root:Steps 18408960 ( 98424320 of 10000000000 ) @ 36838.4 SPS. Loss 14317.794922. Return per episode: 500.2. Stats:\n",
            "{'action_counts': [('E', 552),\n",
            "                   ('W', 455),\n",
            "                   ('S', 314),\n",
            "                   ('N', 267),\n",
            "                   ('SEARCH', 229),\n",
            "                   ('N_', 164),\n",
            "                   ('E_', 154),\n",
            "                   ('W_', 150),\n",
            "                   ('S_', 114),\n",
            "                   ('SW', 32),\n",
            "                   ('SE', 26),\n",
            "                   ('EAT', 24),\n",
            "                   ('NW', 12),\n",
            "                   ('DOWN', 12),\n",
            "                   ('FIRE', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('KICK', 10),\n",
            "                   ('NW_', 9),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('RUSH', 4),\n",
            "                   ('SW_', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 14340.068359375,\n",
            " 'entropy_loss': -1.3472254276275635,\n",
            " 'episode_returns': (413.0, 831.0, 300.0, 457.0),\n",
            " 'mean_episode_return': 500.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -20.925636291503906,\n",
            " 'total_loss': 14317.794921875}\n",
            "INFO:root:Steps 18452480 ( 98467840 of 10000000000 ) @ 36938.5 SPS. Loss 5445.332520. Return per episode: 427.0. Stats:\n",
            "{'action_counts': [('E', 628),\n",
            "                   ('W', 411),\n",
            "                   ('S', 287),\n",
            "                   ('SEARCH', 239),\n",
            "                   ('W_', 211),\n",
            "                   ('N', 196),\n",
            "                   ('N_', 165),\n",
            "                   ('E_', 159),\n",
            "                   ('S_', 153),\n",
            "                   ('SW', 30),\n",
            "                   ('NE', 16),\n",
            "                   ('NW_', 15),\n",
            "                   ('DOWN', 15),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 10),\n",
            "                   ('KICK', 8),\n",
            "                   ('EAT', 3),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('PICKUP', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 5696.96875,\n",
            " 'entropy_loss': -1.5793206691741943,\n",
            " 'episode_returns': (781.0, 244.0, 256.0),\n",
            " 'mean_episode_return': 427.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -250.05711364746094,\n",
            " 'total_loss': 5445.33251953125}\n",
            "INFO:root:Steps 18503680 ( 98519040 of 10000000000 ) @ 37038.6 SPS. Loss 13818.729492. Return per episode: 633.6. Stats:\n",
            "{'action_counts': [('E', 624),\n",
            "                   ('W', 455),\n",
            "                   ('S', 313),\n",
            "                   ('N', 242),\n",
            "                   ('SEARCH', 190),\n",
            "                   ('N_', 189),\n",
            "                   ('E_', 128),\n",
            "                   ('W_', 128),\n",
            "                   ('S_', 91),\n",
            "                   ('SW', 36),\n",
            "                   ('SE', 28),\n",
            "                   ('NW', 28),\n",
            "                   ('INVENTORY', 28),\n",
            "                   ('NE', 22),\n",
            "                   ('EAT', 17),\n",
            "                   ('NW_', 15),\n",
            "                   ('DOWN', 9),\n",
            "                   ('KICK', 8),\n",
            "                   ('FIRE', 5),\n",
            "                   ('RUSH', 3),\n",
            "                   ('NE_', 1)],\n",
            " 'baseline_loss': 13977.6728515625,\n",
            " 'entropy_loss': -1.4901024103164673,\n",
            " 'episode_returns': (241.0, 574.0, 317.0, 678.0, 1358.0),\n",
            " 'mean_episode_return': 633.6000366210938,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -157.45330810546875,\n",
            " 'total_loss': 13818.7294921875}\n",
            "INFO:root:Steps 18554880 ( 98570240 of 10000000000 ) @ 37138.8 SPS. Loss 8670.281250. Return per episode: 310.0. Stats:\n",
            "{'action_counts': [('W', 606),\n",
            "                   ('E', 531),\n",
            "                   ('S', 313),\n",
            "                   ('N', 298),\n",
            "                   ('W_', 201),\n",
            "                   ('E_', 151),\n",
            "                   ('SEARCH', 148),\n",
            "                   ('N_', 116),\n",
            "                   ('S_', 46),\n",
            "                   ('SW', 41),\n",
            "                   ('SE', 18),\n",
            "                   ('NW', 18),\n",
            "                   ('EAT', 15),\n",
            "                   ('DOWN', 14),\n",
            "                   ('NE', 11),\n",
            "                   ('NW_', 8),\n",
            "                   ('KICK', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('RUSH', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('ESC', 1),\n",
            "                   ('OPEN', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 8939.958984375,\n",
            " 'entropy_loss': -1.268809199333191,\n",
            " 'episode_returns': (257.0, 98.0, 266.0, 382.0, 547.0),\n",
            " 'mean_episode_return': 310.0,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -268.4091796875,\n",
            " 'total_loss': 8670.28125}\n",
            "INFO:root:Steps 18600960 ( 98616320 of 10000000000 ) @ 37238.9 SPS. Loss 8428.394531. Return per episode: 1476.0. Stats:\n",
            "{'action_counts': [('E', 607),\n",
            "                   ('W', 568),\n",
            "                   ('S', 323),\n",
            "                   ('N', 276),\n",
            "                   ('E_', 177),\n",
            "                   ('SEARCH', 148),\n",
            "                   ('W_', 129),\n",
            "                   ('N_', 96),\n",
            "                   ('S_', 81),\n",
            "                   ('SW', 26),\n",
            "                   ('INVENTORY', 21),\n",
            "                   ('SE', 20),\n",
            "                   ('KICK', 19),\n",
            "                   ('NW', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('EAT', 11),\n",
            "                   ('FIRE', 10),\n",
            "                   ('RUSH', 10),\n",
            "                   ('NE', 6),\n",
            "                   ('NW_', 5)],\n",
            " 'baseline_loss': 8500.47265625,\n",
            " 'entropy_loss': -1.1823792457580566,\n",
            " 'episode_returns': (1638.0, 1314.0),\n",
            " 'mean_episode_return': 1476.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -70.8956527709961,\n",
            " 'total_loss': 8428.39453125}\n",
            "INFO:root:Steps 18647040 ( 98662400 of 10000000000 ) @ 37339.0 SPS. Loss 15101.810547. Return per episode: 602.0. Stats:\n",
            "{'action_counts': [('W', 582),\n",
            "                   ('E', 532),\n",
            "                   ('S', 305),\n",
            "                   ('E_', 238),\n",
            "                   ('SEARCH', 216),\n",
            "                   ('W_', 196),\n",
            "                   ('N', 181),\n",
            "                   ('N_', 91),\n",
            "                   ('S_', 79),\n",
            "                   ('EAT', 36),\n",
            "                   ('RUSH', 27),\n",
            "                   ('DOWN', 13),\n",
            "                   ('SE', 12),\n",
            "                   ('FIRE', 10),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('SW', 8),\n",
            "                   ('NW_', 8),\n",
            "                   ('NW', 6),\n",
            "                   ('KICK', 6),\n",
            "                   ('NE', 4),\n",
            "                   ('PAY', 1)],\n",
            " 'baseline_loss': 15088.4794921875,\n",
            " 'entropy_loss': -1.2533648014068604,\n",
            " 'episode_returns': (873.0, 533.0, 400.0),\n",
            " 'mean_episode_return': 602.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 14.583580017089844,\n",
            " 'total_loss': 15101.810546875}\n",
            "INFO:root:Steps 18698240 ( 98713600 of 10000000000 ) @ 37439.1 SPS. Loss 11281.799805. Return per episode: 394.3. Stats:\n",
            "{'action_counts': [('E', 693),\n",
            "                   ('W', 356),\n",
            "                   ('S', 267),\n",
            "                   ('SEARCH', 264),\n",
            "                   ('N_', 210),\n",
            "                   ('N', 201),\n",
            "                   ('E_', 197),\n",
            "                   ('W_', 154),\n",
            "                   ('S_', 129),\n",
            "                   ('KICK', 17),\n",
            "                   ('SE', 16),\n",
            "                   ('EAT', 13),\n",
            "                   ('SW', 9),\n",
            "                   ('NW_', 7),\n",
            "                   ('DOWN', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NE', 5),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('NW', 4),\n",
            "                   ('FIRE', 1)],\n",
            " 'baseline_loss': 11417.603515625,\n",
            " 'entropy_loss': -1.3867803812026978,\n",
            " 'episode_returns': (620.0, 328.0, 235.0),\n",
            " 'mean_episode_return': 394.3333435058594,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -134.41661071777344,\n",
            " 'total_loss': 11281.7998046875}\n",
            "INFO:root:Steps 18746880 ( 98762240 of 10000000000 ) @ 37539.2 SPS. Loss 13336.159180. Return per episode: 1548.0. Stats:\n",
            "{'action_counts': [('E', 737),\n",
            "                   ('S', 461),\n",
            "                   ('W', 367),\n",
            "                   ('N', 241),\n",
            "                   ('N_', 170),\n",
            "                   ('SEARCH', 159),\n",
            "                   ('E_', 88),\n",
            "                   ('W_', 87),\n",
            "                   ('S_', 80),\n",
            "                   ('SE', 32),\n",
            "                   ('SW', 31),\n",
            "                   ('NE', 19),\n",
            "                   ('EAT', 18),\n",
            "                   ('KICK', 15),\n",
            "                   ('NW', 13),\n",
            "                   ('DOWN', 10),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW_', 8),\n",
            "                   ('INVENTORY', 8),\n",
            "                   ('FIRE', 4),\n",
            "                   ('QUAFF', 2),\n",
            "                   ('MORE', 1)],\n",
            " 'baseline_loss': 12719.8935546875,\n",
            " 'entropy_loss': -1.3185768127441406,\n",
            " 'episode_returns': (1548.0,),\n",
            " 'mean_episode_return': 1548.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 617.5841674804688,\n",
            " 'total_loss': 13336.1591796875}\n",
            "INFO:root:Steps 18792960 ( 98808320 of 10000000000 ) @ 37639.3 SPS. Loss 6724.205078. Return per episode: 461.5. Stats:\n",
            "{'action_counts': [('W', 491),\n",
            "                   ('E', 405),\n",
            "                   ('SEARCH', 370),\n",
            "                   ('S', 325),\n",
            "                   ('N', 258),\n",
            "                   ('S_', 166),\n",
            "                   ('E_', 136),\n",
            "                   ('N_', 116),\n",
            "                   ('W_', 100),\n",
            "                   ('EAT', 53),\n",
            "                   ('SE', 30),\n",
            "                   ('RUSH', 23),\n",
            "                   ('NE', 18),\n",
            "                   ('KICK', 16),\n",
            "                   ('SW', 13),\n",
            "                   ('NW_', 12),\n",
            "                   ('NW', 10),\n",
            "                   ('FIRE', 6),\n",
            "                   ('DOWN', 5),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('NE_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 7088.58203125,\n",
            " 'entropy_loss': -1.5253360271453857,\n",
            " 'episode_returns': (717.0, 187.0, 779.0, 163.0),\n",
            " 'mean_episode_return': 461.5,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -362.8515930175781,\n",
            " 'total_loss': 6724.205078125}\n",
            "INFO:root:Steps 18841600 ( 98856960 of 10000000000 ) @ 37739.4 SPS. Loss 6948.736328. Return per episode: 446.0. Stats:\n",
            "{'action_counts': [('E', 644),\n",
            "                   ('W', 425),\n",
            "                   ('S', 354),\n",
            "                   ('N', 288),\n",
            "                   ('SEARCH', 198),\n",
            "                   ('N_', 145),\n",
            "                   ('W_', 133),\n",
            "                   ('E_', 129),\n",
            "                   ('S_', 125),\n",
            "                   ('NE', 19),\n",
            "                   ('SE', 17),\n",
            "                   ('SW', 16),\n",
            "                   ('DOWN', 15),\n",
            "                   ('EAT', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('NW', 11),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('FIRE', 2),\n",
            "                   ('UP', 1),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 6559.6455078125,\n",
            " 'entropy_loss': -1.2025963068008423,\n",
            " 'episode_returns': (446.0,),\n",
            " 'mean_episode_return': 446.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 390.2935791015625,\n",
            " 'total_loss': 6948.736328125}\n",
            "INFO:root:Steps 18892800 ( 98908160 of 10000000000 ) @ 37839.5 SPS. Loss 12298.259766. Return per episode: 502.0. Stats:\n",
            "{'action_counts': [('W', 584),\n",
            "                   ('E', 509),\n",
            "                   ('S', 437),\n",
            "                   ('N', 235),\n",
            "                   ('E_', 179),\n",
            "                   ('SEARCH', 159),\n",
            "                   ('N_', 121),\n",
            "                   ('W_', 118),\n",
            "                   ('S_', 59),\n",
            "                   ('EAT', 29),\n",
            "                   ('NE', 24),\n",
            "                   ('SE', 16),\n",
            "                   ('DOWN', 16),\n",
            "                   ('RUSH', 15),\n",
            "                   ('SW', 13),\n",
            "                   ('KICK', 13),\n",
            "                   ('INVENTORY', 11),\n",
            "                   ('FIRE', 9),\n",
            "                   ('NW', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('UP', 1),\n",
            "                   ('CLOSE', 1)],\n",
            " 'baseline_loss': 12110.0,\n",
            " 'entropy_loss': -1.2026175260543823,\n",
            " 'episode_returns': (411.0, 248.0, 995.0, 354.0),\n",
            " 'mean_episode_return': 502.0,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 189.46156311035156,\n",
            " 'total_loss': 12298.259765625}\n",
            "INFO:root:Steps 18938880 ( 98954240 of 10000000000 ) @ 37939.6 SPS. Loss 9356.475586. Return per episode: 326.0. Stats:\n",
            "{'action_counts': [('W', 539),\n",
            "                   ('E', 472),\n",
            "                   ('S', 313),\n",
            "                   ('N', 280),\n",
            "                   ('SEARCH', 205),\n",
            "                   ('E_', 178),\n",
            "                   ('W_', 173),\n",
            "                   ('N_', 139),\n",
            "                   ('S_', 65),\n",
            "                   ('EAT', 35),\n",
            "                   ('SE', 27),\n",
            "                   ('NE', 24),\n",
            "                   ('SW', 22),\n",
            "                   ('INVENTORY', 19),\n",
            "                   ('KICK', 16),\n",
            "                   ('NW', 14),\n",
            "                   ('RUSH', 13),\n",
            "                   ('NW_', 12),\n",
            "                   ('DOWN', 9),\n",
            "                   ('FIRE', 5)],\n",
            " 'baseline_loss': 9684.736328125,\n",
            " 'entropy_loss': -1.5280131101608276,\n",
            " 'episode_returns': (326.0,),\n",
            " 'mean_episode_return': 326.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -326.73193359375,\n",
            " 'total_loss': 9356.4755859375}\n",
            "INFO:root:Steps 18987520 ( 99002880 of 10000000000 ) @ 38039.7 SPS. Loss 9418.853516. Return per episode: 209.0. Stats:\n",
            "{'action_counts': [('S', 468),\n",
            "                   ('E', 465),\n",
            "                   ('W', 416),\n",
            "                   ('E_', 230),\n",
            "                   ('W_', 220),\n",
            "                   ('SEARCH', 186),\n",
            "                   ('N', 169),\n",
            "                   ('N_', 154),\n",
            "                   ('S_', 87),\n",
            "                   ('NE', 32),\n",
            "                   ('EAT', 30),\n",
            "                   ('NW', 19),\n",
            "                   ('NW_', 17),\n",
            "                   ('SE', 16),\n",
            "                   ('SW', 12),\n",
            "                   ('KICK', 11),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('DOWN', 8),\n",
            "                   ('RUSH', 7),\n",
            "                   ('FIRE', 2),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 9346.802734375,\n",
            " 'entropy_loss': -1.6102744340896606,\n",
            " 'episode_returns': (209.0,),\n",
            " 'mean_episode_return': 209.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 73.66155242919922,\n",
            " 'total_loss': 9418.853515625}\n",
            "INFO:root:Steps 19038720 ( 99054080 of 10000000000 ) @ 38139.8 SPS. Loss 10449.318359. Return per episode: 1160.0. Stats:\n",
            "{'action_counts': [('E', 754),\n",
            "                   ('S', 465),\n",
            "                   ('W', 439),\n",
            "                   ('N', 295),\n",
            "                   ('SEARCH', 159),\n",
            "                   ('W_', 107),\n",
            "                   ('N_', 80),\n",
            "                   ('E_', 75),\n",
            "                   ('SE', 33),\n",
            "                   ('S_', 30),\n",
            "                   ('EAT', 20),\n",
            "                   ('KICK', 20),\n",
            "                   ('DOWN', 19),\n",
            "                   ('SW', 15),\n",
            "                   ('NE', 14),\n",
            "                   ('RUSH', 14),\n",
            "                   ('NW', 9),\n",
            "                   ('FIRE', 7),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('NW_', 1)],\n",
            " 'baseline_loss': 9806.2509765625,\n",
            " 'entropy_loss': -0.9650405645370483,\n",
            " 'episode_returns': (1160.0,),\n",
            " 'mean_episode_return': 1160.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 644.0318603515625,\n",
            " 'total_loss': 10449.318359375}\n",
            "INFO:root:Steps 19084800 ( 99100160 of 10000000000 ) @ 38239.9 SPS. Loss 7688.534668. Return per episode: 267.0. Stats:\n",
            "{'action_counts': [('W', 611),\n",
            "                   ('S', 430),\n",
            "                   ('E', 391),\n",
            "                   ('N', 274),\n",
            "                   ('W_', 197),\n",
            "                   ('N_', 170),\n",
            "                   ('E_', 156),\n",
            "                   ('SEARCH', 149),\n",
            "                   ('S_', 54),\n",
            "                   ('SE', 16),\n",
            "                   ('EAT', 16),\n",
            "                   ('INVENTORY', 16),\n",
            "                   ('NE', 15),\n",
            "                   ('SW', 12),\n",
            "                   ('DOWN', 12),\n",
            "                   ('FIRE', 10),\n",
            "                   ('KICK', 9),\n",
            "                   ('RUSH', 9),\n",
            "                   ('NW_', 7),\n",
            "                   ('NW', 3),\n",
            "                   ('SW_', 2),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 7716.2177734375,\n",
            " 'entropy_loss': -1.2246901988983154,\n",
            " 'episode_returns': (231.0, 303.0),\n",
            " 'mean_episode_return': 267.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -26.45854377746582,\n",
            " 'total_loss': 7688.53466796875}\n",
            "INFO:root:Steps 19130880 ( 99146240 of 10000000000 ) @ 38340.0 SPS. Loss 8027.219727. Return per episode: 930.0. Stats:\n",
            "{'action_counts': [('E', 534),\n",
            "                   ('W', 434),\n",
            "                   ('S', 428),\n",
            "                   ('N', 230),\n",
            "                   ('W_', 181),\n",
            "                   ('SEARCH', 178),\n",
            "                   ('E_', 170),\n",
            "                   ('N_', 129),\n",
            "                   ('S_', 69),\n",
            "                   ('SE', 49),\n",
            "                   ('EAT', 45),\n",
            "                   ('SW', 31),\n",
            "                   ('NW', 19),\n",
            "                   ('NE', 18),\n",
            "                   ('RUSH', 14),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('FIRE', 7),\n",
            "                   ('KICK', 6),\n",
            "                   ('NW_', 4),\n",
            "                   ('DOWN', 3),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 8304.4599609375,\n",
            " 'entropy_loss': -1.6024022102355957,\n",
            " 'episode_returns': (930.0,),\n",
            " 'mean_episode_return': 930.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -275.6376953125,\n",
            " 'total_loss': 8027.2197265625}\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n",
            "INFO:root:Steps 19179520 ( 99194880 of 10000000000 ) @ 38440.7 SPS. Loss 11388.500977. Return per episode: 448.5. Stats:\n",
            "{'action_counts': [('W', 601),\n",
            "                   ('E', 468),\n",
            "                   ('S', 388),\n",
            "                   ('N', 259),\n",
            "                   ('W_', 170),\n",
            "                   ('SEARCH', 161),\n",
            "                   ('E_', 155),\n",
            "                   ('N_', 104),\n",
            "                   ('S_', 85),\n",
            "                   ('EAT', 37),\n",
            "                   ('SW', 24),\n",
            "                   ('SE', 18),\n",
            "                   ('FIRE', 16),\n",
            "                   ('NW', 15),\n",
            "                   ('NE', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('RUSH', 12),\n",
            "                   ('KICK', 9),\n",
            "                   ('NW_', 6),\n",
            "                   ('INVENTORY', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 11303.978515625,\n",
            " 'entropy_loss': -1.3608171939849854,\n",
            " 'episode_returns': (268.0, 629.0),\n",
            " 'mean_episode_return': 448.5,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': 85.88323974609375,\n",
            " 'total_loss': 11388.5009765625}\n",
            "INFO:root:Steps 19228160 ( 99243520 of 10000000000 ) @ 38540.8 SPS. Loss 16497.996094. Return per episode: 393.3. Stats:\n",
            "{'action_counts': [('E', 611),\n",
            "                   ('W', 527),\n",
            "                   ('S', 376),\n",
            "                   ('E_', 227),\n",
            "                   ('N', 221),\n",
            "                   ('N_', 164),\n",
            "                   ('SEARCH', 163),\n",
            "                   ('W_', 127),\n",
            "                   ('S_', 74),\n",
            "                   ('NE', 14),\n",
            "                   ('SW', 10),\n",
            "                   ('EAT', 10),\n",
            "                   ('KICK', 8),\n",
            "                   ('NW', 7),\n",
            "                   ('DOWN', 7),\n",
            "                   ('FIRE', 5),\n",
            "                   ('NW_', 3),\n",
            "                   ('RUSH', 3),\n",
            "                   ('SE', 2),\n",
            "                   ('INVENTORY', 1)],\n",
            " 'baseline_loss': 16100.41796875,\n",
            " 'entropy_loss': -1.0571925640106201,\n",
            " 'episode_returns': (243.0, 579.0, 358.0),\n",
            " 'mean_episode_return': 393.3333435058594,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 398.6356201171875,\n",
            " 'total_loss': 16497.99609375}\n",
            "INFO:root:Steps 19274240 ( 99289600 of 10000000000 ) @ 38640.9 SPS. Loss 8703.097656. Return per episode: 447.0. Stats:\n",
            "{'action_counts': [('E', 479),\n",
            "                   ('W', 456),\n",
            "                   ('S', 297),\n",
            "                   ('N', 281),\n",
            "                   ('E_', 235),\n",
            "                   ('W_', 213),\n",
            "                   ('SEARCH', 171),\n",
            "                   ('N_', 149),\n",
            "                   ('S_', 41),\n",
            "                   ('NW', 36),\n",
            "                   ('NE', 35),\n",
            "                   ('INVENTORY', 34),\n",
            "                   ('SE', 26),\n",
            "                   ('SW', 24),\n",
            "                   ('DOWN', 16),\n",
            "                   ('RUSH', 16),\n",
            "                   ('FIRE', 13),\n",
            "                   ('NW_', 10),\n",
            "                   ('EAT', 10),\n",
            "                   ('KICK', 10),\n",
            "                   ('CLOSE', 2),\n",
            "                   ('DROP', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('UP', 1),\n",
            "                   ('REMOVE', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 8762.34375,\n",
            " 'entropy_loss': -1.6696585416793823,\n",
            " 'episode_returns': (411.0, 483.0),\n",
            " 'mean_episode_return': 447.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -57.57634353637695,\n",
            " 'total_loss': 8703.09765625}\n",
            "INFO:root:Steps 19325440 ( 99340800 of 10000000000 ) @ 38741.0 SPS. Loss 8966.367188. Return per episode: 463.3. Stats:\n",
            "{'action_counts': [('W', 543),\n",
            "                   ('E', 396),\n",
            "                   ('W_', 322),\n",
            "                   ('E_', 318),\n",
            "                   ('S', 290),\n",
            "                   ('N', 237),\n",
            "                   ('SEARCH', 179),\n",
            "                   ('N_', 130),\n",
            "                   ('S_', 49),\n",
            "                   ('EAT', 19),\n",
            "                   ('NW', 15),\n",
            "                   ('DOWN', 15),\n",
            "                   ('SW', 14),\n",
            "                   ('NE', 8),\n",
            "                   ('SE', 8),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('KICK', 6),\n",
            "                   ('FIRE', 3),\n",
            "                   ('NW_', 1),\n",
            "                   ('PUTON', 1)],\n",
            " 'baseline_loss': 9228.14453125,\n",
            " 'entropy_loss': -1.3664624691009521,\n",
            " 'episode_returns': (560.0, 448.0, 382.0),\n",
            " 'mean_episode_return': 463.3333435058594,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -260.41082763671875,\n",
            " 'total_loss': 8966.3671875}\n",
            "INFO:root:Steps 19376640 ( 99392000 of 10000000000 ) @ 38841.1 SPS. Loss 15757.496094. Return per episode: 496.7. Stats:\n",
            "{'action_counts': [('W', 509),\n",
            "                   ('E', 505),\n",
            "                   ('S', 349),\n",
            "                   ('N', 297),\n",
            "                   ('E_', 213),\n",
            "                   ('SEARCH', 186),\n",
            "                   ('W_', 182),\n",
            "                   ('N_', 104),\n",
            "                   ('S_', 86),\n",
            "                   ('SE', 22),\n",
            "                   ('DOWN', 18),\n",
            "                   ('RUSH', 17),\n",
            "                   ('NE', 16),\n",
            "                   ('EAT', 13),\n",
            "                   ('KICK', 12),\n",
            "                   ('NW', 9),\n",
            "                   ('NW_', 8),\n",
            "                   ('SW', 7),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('APPLY', 1),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 15517.9296875,\n",
            " 'entropy_loss': -1.1432483196258545,\n",
            " 'episode_returns': (550.0, 275.0, 665.0),\n",
            " 'mean_episode_return': 496.66668701171875,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 240.70989990234375,\n",
            " 'total_loss': 15757.49609375}\n",
            "INFO:root:Steps 19422720 ( 99438080 of 10000000000 ) @ 38941.3 SPS. Loss 10684.477539. Return per episode: 624.8. Stats:\n",
            "{'action_counts': [('W', 524),\n",
            "                   ('E', 461),\n",
            "                   ('N', 311),\n",
            "                   ('S', 283),\n",
            "                   ('E_', 266),\n",
            "                   ('W_', 230),\n",
            "                   ('N_', 136),\n",
            "                   ('SEARCH', 124),\n",
            "                   ('S_', 78),\n",
            "                   ('EAT', 27),\n",
            "                   ('RUSH', 25),\n",
            "                   ('FIRE', 20),\n",
            "                   ('SE', 13),\n",
            "                   ('SW', 13),\n",
            "                   ('NE', 11),\n",
            "                   ('NW', 11),\n",
            "                   ('KICK', 11),\n",
            "                   ('DOWN', 7),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('NW_', 2),\n",
            "                   ('NE_', 1)],\n",
            " 'baseline_loss': 10738.646484375,\n",
            " 'entropy_loss': -1.3659440279006958,\n",
            " 'episode_returns': (509.0, 323.0, 597.0, 1285.0, 410.0),\n",
            " 'mean_episode_return': 624.7999877929688,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -52.802345275878906,\n",
            " 'total_loss': 10684.4775390625}\n",
            "INFO:root:Steps 19471360 ( 99486720 of 10000000000 ) @ 39041.4 SPS. Loss 9315.214844. Return per episode: 340.0. Stats:\n",
            "{'action_counts': [('E', 570),\n",
            "                   ('W', 365),\n",
            "                   ('N', 346),\n",
            "                   ('S', 315),\n",
            "                   ('SEARCH', 241),\n",
            "                   ('W_', 228),\n",
            "                   ('E_', 172),\n",
            "                   ('N_', 132),\n",
            "                   ('S_', 63),\n",
            "                   ('SW', 33),\n",
            "                   ('SE', 16),\n",
            "                   ('NW', 15),\n",
            "                   ('NE', 13),\n",
            "                   ('DOWN', 12),\n",
            "                   ('KICK', 11),\n",
            "                   ('NW_', 9),\n",
            "                   ('EAT', 9),\n",
            "                   ('FIRE', 3),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('SE_', 1),\n",
            "                   ('LOOK', 1),\n",
            "                   ('PICKUP', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 9555.5419921875,\n",
            " 'entropy_loss': -1.2372616529464722,\n",
            " 'episode_returns': (469.0, 211.0),\n",
            " 'mean_episode_return': 340.0,\n",
            " 'num_episodes': 2,\n",
            " 'pg_loss': -239.0897216796875,\n",
            " 'total_loss': 9315.21484375}\n",
            "INFO:root:Steps 19522560 ( 99537920 of 10000000000 ) @ 39141.5 SPS. Loss 6621.626465. Return per episode: 216.8. Stats:\n",
            "{'action_counts': [('E', 622),\n",
            "                   ('S', 399),\n",
            "                   ('W', 386),\n",
            "                   ('N', 286),\n",
            "                   ('SEARCH', 244),\n",
            "                   ('W_', 195),\n",
            "                   ('E_', 183),\n",
            "                   ('N_', 112),\n",
            "                   ('S_', 42),\n",
            "                   ('DOWN', 16),\n",
            "                   ('EAT', 11),\n",
            "                   ('NE', 10),\n",
            "                   ('SE', 10),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 10),\n",
            "                   ('SW', 9),\n",
            "                   ('KICK', 7),\n",
            "                   ('FIRE', 3),\n",
            "                   ('RUSH', 3),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('REMOVE', 1)],\n",
            " 'baseline_loss': 7010.9501953125,\n",
            " 'entropy_loss': -1.26233971118927,\n",
            " 'episode_returns': (341.0, 8.0, 427.0, 20.0, 288.0),\n",
            " 'mean_episode_return': 216.8000030517578,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -388.061279296875,\n",
            " 'total_loss': 6621.62646484375}\n",
            "INFO:root:Steps 19568640 ( 99584000 of 10000000000 ) @ 39241.6 SPS. Loss 10499.975586. Return per episode: 232.8. Stats:\n",
            "{'action_counts': [('E', 552),\n",
            "                   ('W', 551),\n",
            "                   ('S', 371),\n",
            "                   ('N', 303),\n",
            "                   ('E_', 164),\n",
            "                   ('SEARCH', 158),\n",
            "                   ('W_', 152),\n",
            "                   ('N_', 143),\n",
            "                   ('S_', 65),\n",
            "                   ('SE', 22),\n",
            "                   ('SW', 19),\n",
            "                   ('EAT', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('DOWN', 9),\n",
            "                   ('KICK', 8),\n",
            "                   ('NE', 6),\n",
            "                   ('RUSH', 6),\n",
            "                   ('NW_', 5),\n",
            "                   ('SE_', 1),\n",
            "                   ('INVENTORY', 1),\n",
            "                   ('OPEN', 1)],\n",
            " 'baseline_loss': 10537.28515625,\n",
            " 'entropy_loss': -1.1683531999588013,\n",
            " 'episode_returns': (355.0, 370.0, 84.0, 122.0),\n",
            " 'mean_episode_return': 232.75,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -36.14191818237305,\n",
            " 'total_loss': 10499.9755859375}\n",
            "INFO:root:Steps 19614720 ( 99630080 of 10000000000 ) @ 39341.7 SPS. Loss 12375.177734. Return per episode: 424.0. Stats:\n",
            "{'action_counts': [('W', 548),\n",
            "                   ('E', 534),\n",
            "                   ('S', 408),\n",
            "                   ('N', 362),\n",
            "                   ('SEARCH', 175),\n",
            "                   ('W_', 136),\n",
            "                   ('E_', 102),\n",
            "                   ('N_', 95),\n",
            "                   ('S_', 56),\n",
            "                   ('SW', 25),\n",
            "                   ('NE', 21),\n",
            "                   ('SE', 21),\n",
            "                   ('EAT', 17),\n",
            "                   ('KICK', 16),\n",
            "                   ('DOWN', 12),\n",
            "                   ('NW', 10),\n",
            "                   ('NW_', 10),\n",
            "                   ('INVENTORY', 5),\n",
            "                   ('FIRE', 3),\n",
            "                   ('RUSH', 3),\n",
            "                   ('WEAR', 1)],\n",
            " 'baseline_loss': 11950.60546875,\n",
            " 'entropy_loss': -1.1092092990875244,\n",
            " 'episode_returns': (370.0, 546.0, 356.0),\n",
            " 'mean_episode_return': 424.0,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': 425.6817626953125,\n",
            " 'total_loss': 12375.177734375}\n",
            "INFO:root:Steps 19665920 ( 99681280 of 10000000000 ) @ 39441.8 SPS. Loss 9578.123047. Return per episode: 133.0. Stats:\n",
            "{'action_counts': [('E', 535),\n",
            "                   ('S', 383),\n",
            "                   ('N', 331),\n",
            "                   ('W', 331),\n",
            "                   ('W_', 226),\n",
            "                   ('N_', 184),\n",
            "                   ('E_', 172),\n",
            "                   ('SEARCH', 154),\n",
            "                   ('S_', 99),\n",
            "                   ('NE', 31),\n",
            "                   ('SE', 19),\n",
            "                   ('SW', 15),\n",
            "                   ('NW', 15),\n",
            "                   ('NW_', 14),\n",
            "                   ('EAT', 14),\n",
            "                   ('KICK', 14),\n",
            "                   ('DOWN', 11),\n",
            "                   ('RUSH', 5),\n",
            "                   ('FIRE', 2),\n",
            "                   ('INVENTORY', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('REMOVE', 1),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 9141.31640625,\n",
            " 'entropy_loss': -1.4252926111221313,\n",
            " 'episode_returns': (133.0,),\n",
            " 'mean_episode_return': 133.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': 438.23138427734375,\n",
            " 'total_loss': 9578.123046875}\n",
            "INFO:root:Steps 19714560 ( 99729920 of 10000000000 ) @ 39541.9 SPS. Loss 9780.779297. Return per episode: 756.5. Stats:\n",
            "{'action_counts': [('W', 531),\n",
            "                   ('E', 436),\n",
            "                   ('S', 435),\n",
            "                   ('N', 253),\n",
            "                   ('W_', 246),\n",
            "                   ('E_', 173),\n",
            "                   ('N_', 142),\n",
            "                   ('SEARCH', 113),\n",
            "                   ('S_', 82),\n",
            "                   ('EAT', 36),\n",
            "                   ('NE', 20),\n",
            "                   ('SE', 20),\n",
            "                   ('SW', 17),\n",
            "                   ('FIRE', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('RUSH', 10),\n",
            "                   ('INVENTORY', 9),\n",
            "                   ('DOWN', 4),\n",
            "                   ('KICK', 4),\n",
            "                   ('NW_', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('SE_', 1),\n",
            "                   ('CLOSE', 1)],\n",
            " 'baseline_loss': 9488.0859375,\n",
            " 'entropy_loss': -1.3270325660705566,\n",
            " 'episode_returns': (189.0, 77.0, 1324.0, 1436.0),\n",
            " 'mean_episode_return': 756.5,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 294.02069091796875,\n",
            " 'total_loss': 9780.779296875}\n",
            "INFO:root:Steps 19760640 ( 99776000 of 10000000000 ) @ 39642.0 SPS. Loss 7288.112793. Return per episode: 285.0. Stats:\n",
            "{'action_counts': [('W', 526),\n",
            "                   ('E', 459),\n",
            "                   ('S', 387),\n",
            "                   ('N', 270),\n",
            "                   ('W_', 196),\n",
            "                   ('N_', 167),\n",
            "                   ('E_', 161),\n",
            "                   ('SEARCH', 154),\n",
            "                   ('S_', 69),\n",
            "                   ('NE', 40),\n",
            "                   ('SE', 26),\n",
            "                   ('SW', 21),\n",
            "                   ('NW', 20),\n",
            "                   ('KICK', 19),\n",
            "                   ('EAT', 15),\n",
            "                   ('DOWN', 12),\n",
            "                   ('INVENTORY', 6),\n",
            "                   ('RUSH', 5),\n",
            "                   ('NW_', 3),\n",
            "                   ('FIRE', 3),\n",
            "                   ('DROP', 1)],\n",
            " 'baseline_loss': 7515.0205078125,\n",
            " 'entropy_loss': -1.4198813438415527,\n",
            " 'episode_returns': (285.0,),\n",
            " 'mean_episode_return': 285.0,\n",
            " 'num_episodes': 1,\n",
            " 'pg_loss': -225.48785400390625,\n",
            " 'total_loss': 7288.11279296875}\n",
            "INFO:root:Steps 19809280 ( 99824640 of 10000000000 ) @ 39742.1 SPS. Loss 12957.886719. Return per episode: 613.8. Stats:\n",
            "{'action_counts': [('E', 563),\n",
            "                   ('W', 441),\n",
            "                   ('S', 313),\n",
            "                   ('E_', 228),\n",
            "                   ('W_', 223),\n",
            "                   ('N', 215),\n",
            "                   ('SEARCH', 194),\n",
            "                   ('N_', 104),\n",
            "                   ('S_', 100),\n",
            "                   ('EAT', 33),\n",
            "                   ('SE', 25),\n",
            "                   ('RUSH', 19),\n",
            "                   ('NE', 15),\n",
            "                   ('SW', 15),\n",
            "                   ('DOWN', 15),\n",
            "                   ('INVENTORY', 15),\n",
            "                   ('NW', 13),\n",
            "                   ('KICK', 11),\n",
            "                   ('NW_', 8),\n",
            "                   ('FIRE', 7),\n",
            "                   ('NE_', 1),\n",
            "                   ('SE_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 13084.5390625,\n",
            " 'entropy_loss': -1.5047328472137451,\n",
            " 'episode_returns': (700.0, 456.0, 380.0, 1100.0, 433.0),\n",
            " 'mean_episode_return': 613.7999877929688,\n",
            " 'num_episodes': 5,\n",
            " 'pg_loss': -125.1473388671875,\n",
            " 'total_loss': 12957.88671875}\n",
            "INFO:root:Steps 19857920 ( 99873280 of 10000000000 ) @ 39842.2 SPS. Loss 9434.084961. Return per episode: 599.5. Stats:\n",
            "{'action_counts': [('E', 533),\n",
            "                   ('W', 507),\n",
            "                   ('S', 381),\n",
            "                   ('N', 241),\n",
            "                   ('E_', 183),\n",
            "                   ('W_', 177),\n",
            "                   ('SEARCH', 161),\n",
            "                   ('N_', 102),\n",
            "                   ('S_', 86),\n",
            "                   ('SW', 32),\n",
            "                   ('EAT', 26),\n",
            "                   ('SE', 25),\n",
            "                   ('NW', 19),\n",
            "                   ('NE', 18),\n",
            "                   ('RUSH', 15),\n",
            "                   ('INVENTORY', 14),\n",
            "                   ('DOWN', 13),\n",
            "                   ('NW_', 10),\n",
            "                   ('FIRE', 8),\n",
            "                   ('KICK', 8),\n",
            "                   ('TAKEOFF', 1)],\n",
            " 'baseline_loss': 9865.5986328125,\n",
            " 'entropy_loss': -1.4261157512664795,\n",
            " 'episode_returns': (876.0, 250.0, 323.0, 949.0),\n",
            " 'mean_episode_return': 599.5,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -430.0874328613281,\n",
            " 'total_loss': 9434.0849609375}\n",
            "INFO:root:Steps 19906560 ( 99921920 of 10000000000 ) @ 39942.3 SPS. Loss 15443.812500. Return per episode: 428.2. Stats:\n",
            "{'action_counts': [('E', 676),\n",
            "                   ('W', 448),\n",
            "                   ('N', 339),\n",
            "                   ('S', 263),\n",
            "                   ('W_', 204),\n",
            "                   ('E_', 159),\n",
            "                   ('N_', 126),\n",
            "                   ('SEARCH', 120),\n",
            "                   ('S_', 78),\n",
            "                   ('EAT', 31),\n",
            "                   ('NE', 27),\n",
            "                   ('SE', 14),\n",
            "                   ('RUSH', 13),\n",
            "                   ('SW', 12),\n",
            "                   ('NW', 11),\n",
            "                   ('DOWN', 10),\n",
            "                   ('INVENTORY', 10),\n",
            "                   ('NW_', 7),\n",
            "                   ('KICK', 6),\n",
            "                   ('FIRE', 4),\n",
            "                   ('NE_', 1),\n",
            "                   ('MOVE', 1)],\n",
            " 'baseline_loss': 15195.466796875,\n",
            " 'entropy_loss': -1.4561429023742676,\n",
            " 'episode_returns': (366.0, 672.0, 270.0, 405.0),\n",
            " 'mean_episode_return': 428.25,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': 249.80145263671875,\n",
            " 'total_loss': 15443.8125}\n",
            "INFO:root:Steps 19955200 ( 99970560 of 10000000000 ) @ 40042.4 SPS. Loss 11917.035156. Return per episode: 416.8. Stats:\n",
            "{'action_counts': [('E', 749),\n",
            "                   ('S', 387),\n",
            "                   ('W', 343),\n",
            "                   ('N', 316),\n",
            "                   ('SEARCH', 157),\n",
            "                   ('W_', 155),\n",
            "                   ('E_', 141),\n",
            "                   ('N_', 108),\n",
            "                   ('S_', 63),\n",
            "                   ('NE', 35),\n",
            "                   ('SE', 23),\n",
            "                   ('SW', 18),\n",
            "                   ('NW', 14),\n",
            "                   ('DOWN', 12),\n",
            "                   ('EAT', 11),\n",
            "                   ('NW_', 8),\n",
            "                   ('KICK', 7),\n",
            "                   ('FIRE', 4),\n",
            "                   ('RUSH', 4),\n",
            "                   ('INVENTORY', 3),\n",
            "                   ('NE_', 1),\n",
            "                   ('QUAFF', 1)],\n",
            " 'baseline_loss': 12114.6572265625,\n",
            " 'entropy_loss': -1.2397146224975586,\n",
            " 'episode_returns': (235.0, 354.0, 866.0, 212.0),\n",
            " 'mean_episode_return': 416.75,\n",
            " 'num_episodes': 4,\n",
            " 'pg_loss': -196.38267517089844,\n",
            " 'total_loss': 11917.03515625}\n",
            "INFO:root:Steps 20003840 ( 100019200 of 10000000000 ) @ 40142.5 SPS. Loss 12616.166016. Return per episode: 249.3. Stats:\n",
            "{'action_counts': [('E', 487),\n",
            "                   ('S', 319),\n",
            "                   ('E_', 308),\n",
            "                   ('W', 294),\n",
            "                   ('SEARCH', 271),\n",
            "                   ('N', 250),\n",
            "                   ('W_', 207),\n",
            "                   ('N_', 128),\n",
            "                   ('S_', 110),\n",
            "                   ('EAT', 38),\n",
            "                   ('NE', 32),\n",
            "                   ('SW', 30),\n",
            "                   ('SE', 23),\n",
            "                   ('NW_', 15),\n",
            "                   ('NW', 13),\n",
            "                   ('DOWN', 11),\n",
            "                   ('FIRE', 7),\n",
            "                   ('INVENTORY', 7),\n",
            "                   ('KICK', 6),\n",
            "                   ('RUSH', 2),\n",
            "                   ('NE_', 1),\n",
            "                   ('WAIT', 1)],\n",
            " 'baseline_loss': 12897.97265625,\n",
            " 'entropy_loss': -1.6589609384536743,\n",
            " 'episode_returns': (184.0, 208.0, 356.0),\n",
            " 'mean_episode_return': 249.33334350585938,\n",
            " 'num_episodes': 3,\n",
            " 'pg_loss': -280.1471252441406,\n",
            " 'total_loss': 12616.166015625}\n",
            "INFO:root:Learning finished after 100019200 steps.\n",
            "INFO:root:Saving checkpoint to torchbeast/torchbeast-20241210-053738/model.tar\n"
          ]
        }
      ],
      "source": [
        "#Net = NetHackNet\n",
        "Net = NetHackNet_GAT\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "simulated_args = [\n",
        "    '--env', \"NetHack-v0\",     # NetHackChallenge-v0 NetHackScore-v0\n",
        "    '--savedir', \"torchbeast/\",\n",
        "    '--num_actors', '80',\n",
        "    '--batch_size', '32',\n",
        "    '--unroll_length', '80',\n",
        "    '--learning_rate', '0.0005',\n",
        "    '--entropy_cost', '0.001',\n",
        "    '--use_lstm',\n",
        "    '--total_steps', '10000000000',\n",
        "    '--total_steps_', '20000000'\n",
        "]\n",
        "flags = parser.parse_args(simulated_args)\n",
        "main(flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGzFIRamEp2S"
      },
      "source": [
        "## info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXeVe-DDdZsY"
      },
      "source": [
        "0 MORE 13 C-m read the next message\n",
        "\n",
        "1 North 107 k   75 K\n",
        "\n",
        "2 East 108 l    76 L\n",
        "\n",
        "3 South 106 j   74 J\n",
        "\n",
        "4 West 104 h    72 H\n",
        "\n",
        "5 North East 117 u  85 U\n",
        "\n",
        "6 South East 110 n  78 N\n",
        "\n",
        "7 South West 98 b   66 B\n",
        "\n",
        "8 North West 121 y  89 Y\n",
        "\n",
        "17 UP 60 < go up (e.g., a staircase)\n",
        "\n",
        "18 DOWN 62 > go down (e.g., a staircase)\n",
        "\n",
        "19 WAIT / SELF 46 . rest one move while doing nothing / apply to self\n",
        "\n",
        "20 KICK 4 C-d kick something\n",
        "\n",
        "21 EAT 101 e eat something\n",
        "\n",
        "22 SEARCH 115 s search for traps and secret doors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFQYQwA0liw2"
      },
      "source": [
        "0 MiscAction.MORE\n",
        "1 CompassDirection.N\n",
        "2 CompassDirection.E\n",
        "3 CompassDirection.S\n",
        "4 CompassDirection.W\n",
        "5 CompassDirection.NE\n",
        "6 CompassDirection.SE\n",
        "7 CompassDirection.SW\n",
        "8 CompassDirection.NW\n",
        "9 CompassDirectionLonger.N\n",
        "10 CompassDirectionLonger.E\n",
        "11 CompassDirectionLonger.S\n",
        "12 CompassDirectionLonger.W\n",
        "13 CompassDirectionLonger.NE\n",
        "14 CompassDirectionLonger.SE\n",
        "15 CompassDirectionLonger.SW\n",
        "16 CompassDirectionLonger.NW\n",
        "17 MiscDirection.UP\n",
        "18 MiscDirection.DOWN\n",
        "19 MiscDirection.WAIT\n",
        "20 Command.KICK\n",
        "21 Command.EAT\n",
        "22 Command.SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZkLseWTVFQV"
      },
      "source": [
        "<details>\n",
        "<summary>Explanation of each key difference in the action space of nethack:</summary>\n",
        "\n",
        "Discrete(86)\n",
        "0 CompassDirection.N\n",
        "1 CompassDirection.E\n",
        "2 CompassDirection.S\n",
        "3 CompassDirection.W\n",
        "4 CompassDirection.NE\n",
        "5 CompassDirection.SE\n",
        "6 CompassDirection.SW\n",
        "7 CompassDirection.NW\n",
        "8 CompassDirectionLonger.N\n",
        "9 CompassDirectionLonger.E\n",
        "10 CompassDirectionLonger.S\n",
        "11 CompassDirectionLonger.W\n",
        "12 CompassDirectionLonger.NE\n",
        "13 CompassDirectionLonger.SE\n",
        "14 CompassDirectionLonger.SW\n",
        "15 CompassDirectionLonger.NW\n",
        "16 MiscDirection.UP\n",
        "17 MiscDirection.DOWN\n",
        "18 MiscDirection.WAIT\n",
        "19 MiscAction.MORE\n",
        "20 Command.ADJUST\n",
        "21 Command.APPLY\n",
        "22 Command.ATTRIBUTES\n",
        "23 Command.CALL\n",
        "24 Command.CAST\n",
        "25 Command.CHAT\n",
        "26 Command.CLOSE\n",
        "27 Command.DIP\n",
        "28 Command.DROP\n",
        "29 Command.DROPTYPE\n",
        "30 Command.EAT\n",
        "31 Command.ENGRAVE\n",
        "32 Command.ENHANCE\n",
        "33 Command.ESC\n",
        "34 Command.FIGHT\n",
        "35 Command.FIRE\n",
        "36 Command.FORCE\n",
        "37 Command.INVENTORY\n",
        "38 Command.INVENTTYPE\n",
        "39 Command.INVOKE\n",
        "40 Command.JUMP\n",
        "41 Command.KICK\n",
        "42 Command.LOOK\n",
        "43 Command.LOOT\n",
        "44 Command.MONSTER\n",
        "45 Command.MOVE\n",
        "46 Command.MOVEFAR\n",
        "47 Command.OFFER\n",
        "48 Command.OPEN\n",
        "49 Command.PAY\n",
        "50 Command.PICKUP\n",
        "51 Command.PRAY\n",
        "52 Command.PUTON\n",
        "53 Command.QUAFF\n",
        "54 Command.QUIVER\n",
        "55 Command.READ\n",
        "56 Command.REMOVE\n",
        "57 Command.RIDE\n",
        "58 Command.RUB\n",
        "59 Command.RUSH\n",
        "60 Command.RUSH2\n",
        "61 Command.SEARCH\n",
        "62 Command.SEEARMOR\n",
        "63 Command.SEERINGS\n",
        "64 Command.SEETOOLS\n",
        "65 Command.SEETRAP\n",
        "66 Command.SEEWEAPON\n",
        "67 Command.SHELL\n",
        "68 Command.SIT\n",
        "69 Command.SWAP\n",
        "70 Command.TAKEOFF\n",
        "71 Command.TAKEOFFALL\n",
        "72 Command.THROW\n",
        "73 Command.TIP\n",
        "74 Command.TURN\n",
        "75 Command.TWOWEAPON\n",
        "76 Command.UNTRAP\n",
        "77 Command.VERSIONSHORT\n",
        "78 Command.WEAR\n",
        "79 Command.WIELD\n",
        "80 Command.WIPE\n",
        "81 Command.ZAP\n",
        "82 TextCharacters.PLUS\n",
        "83 TextCharacters.QUOTE\n",
        "84 TextCharacters.DOLLAR\n",
        "85 TextCharacters.SPACE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywqo6l91oOSV"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUrccgtwjZpg"
      },
      "outputs": [],
      "source": [
        "def stack_ob(obs_list, max_len=80):\n",
        "    # Trim the list to the last max_len elements if it's too long\n",
        "    if len(obs_list) > max_len:\n",
        "        obs_list = obs_list[-max_len:]\n",
        "\n",
        "    # Stack each key's values\n",
        "    stacked_obs = {}\n",
        "    for key in obs_list[0].keys():\n",
        "        try:\n",
        "            stacked_obs[key] = torch.cat([obs[key] for obs in obs_list], dim=0)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return stacked_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTnumOyzoNuU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "#checkpointpath = 'torchbeast/plots/Relational_full_v1.tar'\n",
        "checkpointpath = 'torchbeast/plots/model4.tar'\n",
        "gym_env = gym.make(\"NetHackScore-v0\")\n",
        "# NetHack-v0 NetHackScore-v0 NetHackChallenge-v0\n",
        "# NetHackStaircase-v0 NetHackStaircasePet-v0 NetHackOracle-v0 NetHackGold-v0 NetHackEat-v0 NetHackScout-v0\n",
        "env = ResettingEnvironment(gym_env)\n",
        "model = NetHackNet_GAT(gym_env.observation_space, gym_env.action_space.n, True)\n",
        "model.eval()\n",
        "checkpoint = torch.load(checkpointpath, map_location=\"cpu\")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "agent_state = model.initial_state(batch_size=1)\n",
        "\n",
        "observation = env.initial()\n",
        "returns = []\n",
        "\n",
        "agent_state = model.initial_state(batch_size=1)\n",
        "\n",
        "a_list = [9,10,9,10,9,10]\n",
        "\n",
        "num_episodes = 200\n",
        "reward = 0\n",
        "action_list = []\n",
        "while len(returns) < num_episodes:\n",
        "    policy_outputs, agent_state = model(observation, agent_state)\n",
        "    #raction = torch.tensor([[gym_env.action_space.sample()]])\n",
        "    #raction_ = torch.tensor([[sample([9,10,11,12],1)]])\n",
        "    action = policy_outputs[\"action\"]\n",
        "    #action_list.append(action)\n",
        "    observation = env.step(action)\n",
        "    reward += observation[\"reward\"].item()\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    env.gym_env.render()\n",
        "    print(observation[\"blstats\"])\n",
        "    print(policy_outputs)\n",
        "    print(reward)\n",
        "    if observation[\"done\"].item():\n",
        "        actions = torch.tensor(action_list)\n",
        "        elements, counts = torch.unique(actions, return_counts=True)\n",
        "        print('elements',elements)\n",
        "        print('counts',counts)\n",
        "        print('reward',reward)\n",
        "\n",
        "        # action_list = []\n",
        "        reward = 0\n",
        "        returns.append(observation[\"episode_return\"].item())\n",
        "        logging.info(\n",
        "            \"Episode ended after %d steps. Return: %.1f\",\n",
        "            observation[\"episode_step\"].item(),\n",
        "            observation[\"episode_return\"].item(),\n",
        "            observation[\"last_action\"].item()\n",
        "        )\n",
        "        time.sleep(3)\n",
        "\n",
        "    time.sleep(0.3)\n",
        "env.close()\n",
        "logging.info(\n",
        "    \"Average returns over %i steps: %.1f\", num_episodes, sum(returns) / len(returns)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIvVY2eiA2_q"
      },
      "source": [
        "0 MiscAction.MORE\n",
        "1 CompassDirection.N\n",
        "2 CompassDirection.E\n",
        "3 CompassDirection.S\n",
        "4 CompassDirection.W\n",
        "5 CompassDirection.NE\n",
        "6 CompassDirection.SE\n",
        "7 CompassDirection.SW\n",
        "8 CompassDirection.NW\n",
        "9 CompassDirectionLonger.N\n",
        "10 CompassDirectionLonger.E\n",
        "11 CompassDirectionLonger.S\n",
        "12 CompassDirectionLonger.W\n",
        "13 CompassDirectionLonger.NE\n",
        "14 CompassDirectionLonger.SE\n",
        "15 CompassDirectionLonger.SW\n",
        "16 CompassDirectionLonger.NW\n",
        "17 MiscDirection.UP\n",
        "18 MiscDirection.DOWN\n",
        "19 MiscDirection.WAIT\n",
        "20 Command.KICK\n",
        "21 Command.EAT\n",
        "22 Command.SEARCH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0LyFoHBFCRi"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y24iZpktRHR5",
        "outputId": "2dfc690d-6ca4-4394-a9ad-b8c99963cd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "plotting torchbeast/Relational_v1/logs.tsv\n",
            "\f\u001b[0;39m                                                                                \n",
            "\u001b[0;39m                                                                                \n",
            "\u001b[0;39m                           \u001b[0;39maveraged mean_episode_return                         \n",
            "\u001b[0;39m \u001b[0;39m 120 +---------------------------------------------------------------------+   \n",
            "\u001b[0;39m      \u001b[0;39m|             +             +             +             +             |   \n",
            "\u001b[0;39m      \u001b[0;39m|             \u001b[0;39m:             :       \u001b[1;35m++-+ ++-++-+ ++-+-+ \u001b[0;39m:             \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 100 |-+\u001b[0;39m...........:.............:..\u001b[1;35m++-++++++++++++++++++++++-+\u001b[0;39m..........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m-+           \u001b[0;39m:             \u001b[1;35m++++++++|+++++|++|++++|+||++-+            \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m-+           \u001b[0;39m:     \u001b[1;35m+-++-+++++++|++|||++|||||||||||||||||             \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m 80 |\u001b[1;35m-+\u001b[0;39m....\u001b[1;35m+++-+-+\u001b[0;39m:\u001b[1;35m++++++++++++++||||||||||||||||||||||||||||\u001b[0;39m...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-+++++++++++++++++++||||||||||||||AA|||AAAAA|AAAAAAAA|             \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-+++++++++++++||||||||||||AAAAAAAAAAAAAAAAAAAAAAAAAAAA             \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m 60 |\u001b[1;35m+|+++++++|||+|||||||||AAAAA|AAAAAAA|||AAA|||||||||||||||\u001b[0;39m...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35mA+++++++|||||||AAAAAAAAAAAAAA|||||||||||||||||||||||||||             \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m 40 |\u001b[1;35mA||||||AAAAAAAAAAAAA|||||||||||||||+||||++|++||||+|++|||\u001b[0;39m...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35mAA||||AAAAAAAAAA|||||||||||||||+|++++++++++++++++++++++-+            \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35mAAAAAAAAA||||||||||||++++|+++++++++++++++-+-++++++-+  +-+            \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m 20 |\u001b[1;35mAAAAAAA|||||++++|++++++++++-++++-+\u001b[0;39m.....\u001b[1;35m+-+\u001b[0;39m.............:...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m||||||||+++++++++++++-+    \u001b[0;39m:             :             :             \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m||||||+++++-+\u001b[0;39m:             :             :             :             \u001b[0;39m|   \n",
            "\u001b[0;39m   \u001b[0;39m 0 |\u001b[1;35m++++++++-+\u001b[0;39m...:.............:.............:.............:...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++++++-+     \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-++-+      \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m-20 |\u001b[1;35m+\u001b[0;39m+\u001b[0;39m...........:.............:.............:.............:...........\u001b[0;39m+-|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m+            \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n",
            "\u001b[0;39m      \u001b[0;39m|\u001b[1;35m+            \u001b[0;39m+             +             +             +             |   \n",
            "\u001b[0;39m  \u001b[0;39m-40 +---------------------------------------------------------------------+   \n",
            "\u001b[0;39m     \u001b[0;39m 0           5e+06         1e+07        1.5e+07        2e+07        2.5e+07\n",
            "\u001b[0;39m                                       \u001b[0;39msteps                                    \n",
            "\u001b[0;39m                                                                                \n",
            "\u001b[0;39;49mplotting torchbeast/Relational_v2/logs.tsv\n",
            "\f\u001b[0;39m                                                                                \n",
            "\u001b[0;39m                                                                                \n",
            "\u001b[0;39m                          \u001b[0;39maveraged mean_episode_return                          \n",
            "\u001b[0;39m \u001b[0;39m 90 +----------------------------------------------------------------------+   \n",
            "\u001b[0;39m     \u001b[0;39m|         +         +         \u001b[1;35m+|+        \u001b[0;39m+       \u001b[1;35m+++-+     ++-+        \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 80 |-+\u001b[0;39m.......:.........:.........\u001b[1;35m+|+\u001b[0;39m..\u001b[1;35m+-+\u001b[0;39m...:.\u001b[1;35m+-++++++++++++++++|+\u001b[0;39m......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|         \u001b[0;39m:         :         \u001b[1;35m++-+++-+-+++++++++++++++++++++||         \u001b[0;39m|   \n",
            "\u001b[0;39m     \u001b[0;39m|   \u001b[1;35m++-+  \u001b[0;39m:      \u001b[1;35m+-+\u001b[0;39m:     \u001b[1;35m++++-++++++++++++++++|||||||||+|||||         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 70 |-+\u001b[0;39m.\u001b[1;35m++-+\u001b[0;39m..:......\u001b[1;35m+-+\u001b[0;39m:.\u001b[1;35m++++++++++++||+|++||||+|||||||||||||||||\u001b[0;39m.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m+-+++-+  \u001b[0;39m:    \u001b[1;35m++-++-++++++++||+|||||||||||||||A||||A||||||A|A         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 60 |\u001b[1;35m+-++||\u001b[0;39m...\u001b[1;35m++++++++++++++++||||||||||||||||||A|AAAAAAAAAAAAAAAA\u001b[0;39m.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m++-+|| +-++++++++|++||++||||||A||AAAAAAAAAAAAAAAA|AAAAAAAA|||         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 50 |\u001b[1;35m++-++++++++|||||||||||+|||AAAAAAAAAAA|AAAAA|A||||||||||||||||\u001b[0;39m.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m|++++++++|||||||||A|||AAAAAAAAAAA||||||||||||||||||||||||+|||         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 40 |\u001b[1;35m||++||++|||||AAAAAAAAAAAAA|||||||||||||||||||+|++||+|+++++++-+\u001b[0;39m......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m||+|||||||AAAAAAAA|AA||A|||||||||+||++|||++++++++++++++++-++-+        \u001b[0;39m|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35mAA||AA|||AAA||||||||||||||||+++++++++++++++++-++++-++-+     \u001b[0;39m:         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 30 |\u001b[1;35mAAAAAAAAAA|||||||||||+|++++++-++++++-+++-+\u001b[0;39m........:.........:.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA|AAAAAA||||++++|+++++++++++-+|+-+      \u001b[0;39m:         :         :         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 20 |\u001b[1;35mA||A|||||+++++++++++-++-+\u001b[0;39m....\u001b[1;35m++-+\u001b[0;39m.......:.........:.........:.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA||||||+++++-+  +-+\u001b[0;39m:         \u001b[1;35m+-+        \u001b[0;39m:         :         :         \u001b[0;39m|   \n",
            "\u001b[0;39m \u001b[0;39m 10 |\u001b[1;35mA||||++++++-+\u001b[0;39m...\u001b[1;35m+-+\u001b[0;39m:.........:..........:.........:.........:.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA+++++++-+         \u001b[0;39m:         :          :         :         :         \u001b[0;39m|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m+++++-++-+         \u001b[0;39m:         :          :         :         :         \u001b[0;39m|   \n",
            "\u001b[0;39m  \u001b[0;39m 0 |\u001b[1;35m+-++||\u001b[0;39m...:.........:.........:..........:.........:.........:.......\u001b[0;39m+-|   \n",
            "\u001b[0;39m     \u001b[0;39m|\u001b[1;35m-+ ++-+  \u001b[0;39m+         +         +          +         +         +         |   \n",
            "\u001b[0;39m \u001b[0;39m-10 +----------------------------------------------------------------------+   \n",
            "\u001b[0;39m    \u001b[0;39m 0       5e+06     1e+07    1.5e+07     2e+07    2.5e+07    3e+07    3.5e+07\n",
            "\u001b[0;39m                                      \u001b[0;39msteps                                     \n",
            "\u001b[0;39m                                                                                \n",
            "\u001b[0;39;49m"
          ]
        }
      ],
      "source": [
        "! python -m nle.scripts.plot --file \"torchbeast/Relational_v1/logs.tsv\"\n",
        "! python -m nle.scripts.plot --file \"torchbeast/Relational_v2/logs.tsv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JamJ_ow3E0W7"
      },
      "source": [
        "# Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzA69tWuA2Si"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTjWiJdy7ICt"
      },
      "outputs": [],
      "source": [
        "from math import nan, isnan\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "import csv\n",
        "\n",
        "# Change to the desired working directory\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "# List of TSV log file paths\n",
        "tsv_files = [\n",
        "    'torchbeast/plots/Baseline_v1.tsv',\n",
        "    'torchbeast/plots/Relational_v1.tsv',\n",
        "    # Add more log files here\n",
        "]\n",
        "\n",
        "# Directory to save the TensorBoard logs\n",
        "log_dir = 'runs/log_example'\n",
        "\n",
        "# Create the log directory if it doesn't exist\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "# Function to process a TSV file and log it to TensorBoard\n",
        "def process_tsv_file(tsv_file_path, run_name):\n",
        "    # Create a SummaryWriter for each run (log file)\n",
        "    writer = SummaryWriter(os.path.join(log_dir, run_name))\n",
        "\n",
        "    with open(tsv_file_path, 'r') as tsv_file:\n",
        "        reader = csv.DictReader(tsv_file, delimiter='\\t')\n",
        "\n",
        "        for row in reader:\n",
        "            try:\n",
        "                # Extract the step, loss, and return values from the TSV file\n",
        "                step = int(row['# Step'])              # Step or iteration number\n",
        "                loss = float(row['total_loss'])        # Loss value\n",
        "                ret = float(row['mean_episode_return'])# Return value\n",
        "\n",
        "                # Log the scalar values to TensorBoard\n",
        "                writer.add_scalar('Loss', loss, step)\n",
        "                if not isnan(ret):\n",
        "                    writer.add_scalar('Return', ret, step)\n",
        "\n",
        "            except KeyError as e:\n",
        "                print(f\"Missing expected column: {e}\")\n",
        "            except ValueError as e:\n",
        "                print(f\"Data conversion error: {e}\")\n",
        "\n",
        "    # Close the writer after processing the file\n",
        "    writer.close()\n",
        "\n",
        "# Process all the log files\n",
        "for tsv_file_path in tsv_files:\n",
        "    # Use the file name (without extension) as the run name\n",
        "    run_name = os.path.splitext(os.path.basename(tsv_file_path))[0]\n",
        "    process_tsv_file(tsv_file_path, run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdYQAZrKDoFp"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Launch TensorBoard\n",
        "%tensorboard --logdir runs/log_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o87lJVnyNAFW"
      },
      "source": [
        "# Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLecPM6uJjtV"
      },
      "source": [
        "wall 2361:- 2360:| 2362-2365:↖↗↘↙corner 2378:floor 2359:black\n",
        "< 2382 > 2383"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIufZ5n9c-_v"
      },
      "source": [
        "<details>\n",
        "<summary>Explanation of each key difference in the action space of challenge:</summary>\n",
        "\n",
        "Discrete(121)\n",
        "0 CompassDirection.N\n",
        "1 CompassDirection.E\n",
        "2 CompassDirection.S\n",
        "3 CompassDirection.W\n",
        "4 CompassDirection.NE\n",
        "5 CompassDirection.SE\n",
        "6 CompassDirection.SW\n",
        "7 CompassDirection.NW\n",
        "8 CompassDirectionLonger.N\n",
        "9 CompassDirectionLonger.E\n",
        "10 CompassDirectionLonger.S\n",
        "11 CompassDirectionLonger.W\n",
        "12 CompassDirectionLonger.NE\n",
        "13 CompassDirectionLonger.SE\n",
        "14 CompassDirectionLonger.SW\n",
        "15 CompassDirectionLonger.NW\n",
        "16 MiscDirection.UP\n",
        "17 MiscDirection.DOWN\n",
        "18 MiscDirection.WAIT\n",
        "19 MiscAction.MORE\n",
        "\t20 Command.EXTCMD\n",
        "\t21 Command.EXTLIST\n",
        "22 Command.ADJUST 20\n",
        "\t23 Command.ANNOTATE\n",
        "24 Command.APPLY 21\n",
        "25 Command.ATTRIBUTES 22\n",
        "\t26 Command.AUTOPICKUP\n",
        "27 Command.CALL 23\n",
        "28 Command.CAST\n",
        "29 Command.CHAT\n",
        "30 Command.CLOSE 26\n",
        "\t31 Command.CONDUCT\n",
        "32 Command.DIP 27\n",
        "33 Command.DROP\n",
        "34 Command.DROPTYPE\n",
        "35 Command.EAT\n",
        "36 Command.ENGRAVE\n",
        "37 Command.ENHANCE\n",
        "38 Command.ESC\n",
        "39 Command.FIGHT\n",
        "40 Command.FIRE\n",
        "41 Command.FORCE 36\n",
        "\t42 Command.GLANCE\n",
        "\t43 Command.HISTORY\n",
        "44 Command.INVENTORY 37\n",
        "45 Command.INVENTTYPE\n",
        "46 Command.INVOKE\n",
        "47 Command.JUMP\n",
        "48 Command.KICK 41\n",
        "\t49 Command.KNOWN\n",
        "\t50 Command.KNOWNCLASS\n",
        "51 Command.LOOK 42\n",
        "52 Command.LOOT\n",
        "53 Command.MONSTER\n",
        "54 Command.MOVE\n",
        "55 Command.MOVEFAR\n",
        "56 Command.OFFER\n",
        "57 Command.OPEN 48\n",
        "\t58 Command.OPTIONS\n",
        "\t59 Command.OVERVIEW\n",
        "60 Command.PAY 49\n",
        "61 Command.PICKUP\n",
        "62 Command.PRAY\n",
        "63 Command.PUTON\n",
        "64 Command.QUAFF 53\n",
        "\t65 Command.QUIT\n",
        "66 Command.QUIVER 54\n",
        "67 Command.READ 55\n",
        "\t68 Command.REDRAW\n",
        "69 Command.REMOVE 56\n",
        "70 Command.RIDE\n",
        "71 Command.RUB\n",
        "72 Command.RUSH\n",
        "73 Command.RUSH2 60\n",
        "\t74 Command.SAVE\n",
        "75 Command.SEARCH 61\n",
        "\t76 Command.SEEALL\n",
        "\t77 Command.SEEAMULET\n",
        "78 Command.SEEARMOR 62\n",
        "\t79 Command.SEEGOLD\n",
        "80 Command.SEERINGS 63\n",
        "\t81 Command.SEESPELLS\n",
        "82 Command.SEETOOLS 64\n",
        "83 Command.SEETRAP\n",
        "84 Command.SEEWEAPON\n",
        "85 Command.SHELL\n",
        "86 Command.SIT\n",
        "87 Command.SWAP\n",
        "88 Command.TAKEOFF\n",
        "89 Command.TAKEOFFALL 71\n",
        "\t90 Command.TELEPORT\n",
        "91 Command.THROW 72\n",
        "92 Command.TIP 73\n",
        "\t93 Command.TRAVEL\n",
        "94 Command.TURN 74\n",
        "95 Command.TWOWEAPON\n",
        "96 Command.UNTRAP 76\n",
        "\t97 Command.VERSION\n",
        "98 Command.VERSIONSHORT 77\n",
        "99 Command.WEAR 78\n",
        "\t100 Command.WHATDOES\n",
        "\t101 Command.WHATIS\n",
        "102 Command.WIELD 79\n",
        "103 Command.WIPE\n",
        "104 Command.ZAP\n",
        "105 TextCharacters.PLUS 82\n",
        "\t106 TextCharacters.MINUS\n",
        "107 TextCharacters.SPACE 83\n",
        "\t108 TextCharacters.APOS\n",
        "109 TextCharacters.QUOTE 84\n",
        "\t110 TextCharacters.NUM_0\n",
        "\t111 TextCharacters.NUM_1\n",
        "\t112 TextCharacters.NUM_2\n",
        "\t113 TextCharacters.NUM_3\n",
        "\t114 TextCharacters.NUM_4\n",
        "\t115 TextCharacters.NUM_5\n",
        "\t116 TextCharacters.NUM_6\n",
        "\t117 TextCharacters.NUM_7\n",
        "\t118 TextCharacters.NUM_8\n",
        "\t119 TextCharacters.NUM_9\n",
        "120 TextCharacters.DOLLAR 85"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3S2HBGmR6kt"
      },
      "source": [
        "<details>\n",
        "<summary>Explanation of each key in the action space:</summary>\n",
        "\n",
        "TASK_ACTIONS = tuple(\n",
        "    [nethack.MiscAction.MORE]\n",
        "    + list(nethack.CompassDirection)\n",
        "    + list(nethack.CompassDirectionLonger)\n",
        "    + list(nethack.MiscDirection)\n",
        "    + [nethack.Command.KICK, nethack.Command.EAT, nethack.Command.SEARCH]\n",
        ")\n",
        "class MiscAction(enum.IntEnum):\n",
        "    MORE = ord(\"\\r\")  # read the next message\n",
        "\n",
        "CompassDirection = enum.IntEnum(\n",
        "    \"CompassDirection\",\n",
        "    {\n",
        "        **CompassCardinalDirection.__members__,\n",
        "        **CompassIntercardinalDirection.__members__,\n",
        "    },\n",
        ")\n",
        "class CompassCardinalDirection(enum.IntEnum):\n",
        "    N = ord(\"k\")\n",
        "    E = ord(\"l\")\n",
        "    S = ord(\"j\")\n",
        "    W = ord(\"h\")\n",
        "\n",
        "\n",
        "class CompassIntercardinalDirection(enum.IntEnum):\n",
        "    NE = ord(\"u\")\n",
        "    SE = ord(\"n\")\n",
        "    SW = ord(\"b\")\n",
        "    NW = ord(\"y\")\n",
        "\n",
        "\n",
        "CompassDirectionLonger = enum.IntEnum(\n",
        "    \"CompassDirectionLonger\",\n",
        "    {\n",
        "        **CompassCardinalDirectionLonger.__members__,\n",
        "        **CompassIntercardinalDirectionLonger.__members__,\n",
        "    },\n",
        ")\n",
        "class CompassCardinalDirectionLonger(enum.IntEnum):\n",
        "    N = ord(\"K\")\n",
        "    E = ord(\"L\")\n",
        "    S = ord(\"J\")\n",
        "    W = ord(\"H\")\n",
        "\n",
        "class CompassIntercardinalDirectionLonger(enum.IntEnum):\n",
        "    NE = ord(\"U\")\n",
        "    SE = ord(\"N\")\n",
        "    SW = ord(\"B\")\n",
        "    NW = ord(\"Y\")\n",
        "\n",
        "class MiscDirection(enum.IntEnum):\n",
        "    UP = ord(\"<\")  # go up a staircase\n",
        "    DOWN = ord(\">\")  # go down a staircase\n",
        "    WAIT = ord(\".\")  # rest one move while doing nothing / apply to self\n",
        "\n",
        "KICK = C(\"d\")  # kick something\n",
        "EAT = ord(\"e\")  # eat something\n",
        "SEARCH = ord(\"s\")  # search for traps and secret doors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGbjFka4cqfh"
      },
      "source": [
        "<details>\n",
        "<summary>Explanation of each key in the observation dictionary:</summary>\n",
        "\n",
        "1. glyphs: A 2D array representing the visual state of the game.\n",
        "   Each value corresponds to a specific visual element or \"glyph\" in NetHack.\n",
        "   Example shape: (21, 79) - corresponding to a 21x79 grid.\n",
        "\n",
        "2. chars: A 2D array containing the character representation of the glyphs.\n",
        "   Each value is a character code that visually represents the game's state.\n",
        "   Example shape: (21, 79).\n",
        "\n",
        "3. colors: A 2D array providing color information for the glyphs.\n",
        "   Each value corresponds to a color code, giving more context to the visual elements.\n",
        "   Example shape: (21, 79).\n",
        "\n",
        "4. specials: A 2D array that indicates special attributes or states for each cell.\n",
        "   These might include things like \"lit\" or \"dark\" areas, traps, etc.\n",
        "   Example shape: (21, 79).\n",
        "\n",
        "5. blstats: A 1D array containing various statistics and information about the player's status.\n",
        "   This includes health, experience, gold, etc.\n",
        "   Example shape: (25,) - representing various player statistics.\n",
        "\n",
        "6. message: A 1D array (or string) that contains the latest message displayed to the player.\n",
        "   This is typically the last line of text describing what happened in the game.\n",
        "   Example shape: (256,) - representing the characters in the message.\n",
        "\n",
        "7. inv_glyphs: A 1D array showing the glyphs for items in the player's inventory.\n",
        "   Each glyph corresponds to an item.\n",
        "   Example shape: (55,) - representing the inventory slots.\n",
        "\n",
        "8. inv_strs: A 1D array containing the string representations of the inventory items.\n",
        "   Each string describes an item in the player's inventory.\n",
        "   Example shape: (55,) - one string per inventory slot.\n",
        "\n",
        "9. inv_letters: A 1D array giving the letters corresponding to each item in the inventory.\n",
        "   In NetHack, each item in the inventory is usually assigned a letter for quick access.\n",
        "   Example shape: (55,).\n",
        "\n",
        "10. inv_oclasses: A 1D array showing the object classes for items in the inventory.\n",
        "    This indicates the type of each item, such as weapon, armor, potion, etc.\n",
        "    Example shape: (55,).\n",
        "\n",
        "11. tty_chars: A 2D array representing the state of the game as displayed in a traditional terminal (TTY) view.\n",
        "    This is a character-based view of the game, similar to the \"chars\" key.\n",
        "    Example shape: (24, 80) - a standard terminal size.\n",
        "\n",
        "12. tty_colors: A 2D array providing color information for the TTY view.\n",
        "    Each value corresponds to a color code, similar to the \"colors\" key.\n",
        "    Example shape: (24, 80).\n",
        "\n",
        "13. tty_cursor: A 1D array indicating the position of the cursor in the TTY view.\n",
        "    This shows where the cursor is currently located on the screen.\n",
        "    Example shape: (2,) - representing the row and column of the cursor.>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw5aaMkb3Ht3"
      },
      "source": [
        "<details>\n",
        "<summary>Explanation of each key in the blstats:</summary>\n",
        "\n",
        "        1.0 / 79.0, # hero col\n",
        "        1.0 / 21, # hero row\n",
        "        0.0, # strength pct\n",
        "        1.0 / 10, # strength\n",
        "        1.0 / 10, # dexterity\n",
        "        1.0 / 10, # constitution\n",
        "        1.0 / 10, # intelligence\n",
        "        1.0 / 10, # wisdom\n",
        "        1.0 / 10, # charisma\n",
        "        0.0,      # score\n",
        "        1.0 / 10, # hitpoints\n",
        "        1.0 / 10, # max hitpoints\n",
        "        0.0, # depth\n",
        "        1.0 / 1000, # gold\n",
        "        1.0 / 10, # energy\n",
        "        1.0 / 10, # max energy\n",
        "        1.0 / 10, # armor class\n",
        "        0.0, # monster level\n",
        "        1.0 / 10, # experience level\n",
        "        1.0 / 100, # experience points\n",
        "        1.0 / 1000, # time\n",
        "        1.0, # hunger_state\n",
        "        1.0 / 10, # carrying capacity\n",
        "        0.0, # carrying capacity\n",
        "        0.0, # level number\n",
        "        0.0, # condition bits\n",
        "\n",
        "        /* blstats indices, see also botl.c and statusfields in botl.h. */\n",
        "        #define NLE_BL_X 0\n",
        "        #define NLE_BL_Y 1\n",
        "        #define NLE_BL_STR25 2  /* strength 3..25 */\n",
        "        #define NLE_BL_STR125 3 /* strength 3..125   */\n",
        "        #define NLE_BL_DEX 4\n",
        "        #define NLE_BL_CON 5\n",
        "        #define NLE_BL_INT 6\n",
        "        #define NLE_BL_WIS 7\n",
        "        #define NLE_BL_CHA 8\n",
        "        #define NLE_BL_SCORE 9\n",
        "        #define NLE_BL_HP 10\n",
        "        #define NLE_BL_HPMAX 11\n",
        "        #define NLE_BL_DEPTH 12\n",
        "        #define NLE_BL_GOLD 13\n",
        "        #define NLE_BL_ENE 14\n",
        "        #define NLE_BL_ENEMAX 15\n",
        "        #define NLE_BL_AC 16\n",
        "        #define NLE_BL_HD 17  /* monster level, \"hit-dice\" */\n",
        "        #define NLE_BL_XP 18  /* experience level */\n",
        "        #define NLE_BL_EXP 19 /* experience points */\n",
        "        #define NLE_BL_TIME 20\n",
        "        #define NLE_BL_HUNGER 21 /* hunger state */\n",
        "        #define NLE_BL_CAP 22    /* carrying capacity */\n",
        "        #define NLE_BL_DNUM 23\n",
        "        #define NLE_BL_DLEVEL 24\n",
        "        #define NLE_BL_CONDITION 25 /* condition bit mask */\n",
        "        #define NLE_BL_ALIGN 26"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N31kKPYpMco0"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZU0IROIoZMP"
      },
      "outputs": [],
      "source": [
        "# def generate_mask(done, input_tensor):\n",
        "\n",
        "#     cum_sum_mark = torch.cumsum(done.float(), dim=0)  # Shape [T*B, 1]\n",
        "#     print(cum_sum_mark)\n",
        "#     mask = (cum_sum_mark > 0).float()  # Valid elements where the cumulative sum is 0\n",
        "\n",
        "#     return mask\n",
        "\n",
        "# end_mark = torch.tensor([[False,False], [True,False], [False,False], [True,False]])\n",
        "\n",
        "# # Define an input tensor of shape [T*B, seq_len]\n",
        "# input_tensor = torch.randn(4, 2, 8)  # [T*B, seq_len], 5 sequences of length 8\n",
        "\n",
        "# # Generate the mask\n",
        "# mask = generate_mask(end_mark, input_tensor)\n",
        "# print(mask)\n",
        "# glyphs = torch.randint(0, 2500, (2, 9, 9))\n",
        "\n",
        "# # Step 2: Instantiate the get_graph_batch class\n",
        "# graph_generator = get_graph_batch()\n",
        "\n",
        "# # Step 3: Use the forward method to process the glyph maps and generate a batch of graphs\n",
        "# graph_batch = graph_generator(glyphs)\n",
        "\n",
        "# # Step 4: Print out key information to verify the output\n",
        "# print(\"Node features (x):\", graph_batch.x)          # Node features (glyph IDs)\n",
        "# print(\"Edge index (edges):\", graph_batch.edge_index)  # Edge indices between nodes\n",
        "\n",
        "# # If you want to verify further details:\n",
        "# print(f\"Number of nodes: {graph_batch.num_nodes}\")\n",
        "# print(f\"Number of edges: {graph_batch.num_edges}\")\n",
        "# print(f\"Batch size (graphs): {graph_batch.batch.max().item() + 1}\")\n",
        "# print(graph_batch.x.shape,graph_batch.edge_index.shape)\n",
        "\n",
        "# gym_env22 = create_env('NetHackScore-v0')\n",
        "# print(gym_env22.action_space)\n",
        "# gym_env22.unwrapped.print_action_meanings()\n",
        "# print(gym_env22.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVUcY7y4Sqsp",
        "outputId": "bec6212b-ac80-4358-9688-b35d19f454c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[13, 1], edge_index=[2, 30], batch=[13], ptr=[3])\n",
            "torch.Size([13, 8])\n",
            "torch.Size([2, 4])\n"
          ]
        }
      ],
      "source": [
        "grid1 = np.array([\n",
        "    [1, 2, 3],\n",
        "     [4, 5, 6],\n",
        "      [7, 8, 9]])\n",
        "\n",
        "grid2 = np.array([\n",
        "    [10, 11, 12],\n",
        "     [13, 14, 15],\n",
        "      [16, 17, 18]])\n",
        "blocked_ids = {2, 5, 10, 11, 14}\n",
        "\n",
        "# Blocked node IDs (walls)\n",
        "batch = get_graph_batch(torch.tensor([grid1,grid2]),blocked_ids)\n",
        "print(batch)\n",
        "\n",
        "emb = nn.Embedding(25, 8)\n",
        "embed = emb(batch.x).squeeze()\n",
        "print(embed.shape)\n",
        "md = GAT(8,4,4)\n",
        "out = md(embed,batch.edge_index,batch.batch)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tohrom_Exyzb",
        "outputId": "38f114f1-2b3a-48ff-9e34-ce92845320bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-4.5952139e+00  2.5131800e+00 -3.8313661e+00  1.9113704e+00\n",
            "  8.7944448e-01 -2.3180022e+00  6.1128769e+00  1.2038110e+00\n",
            " -1.2344328e+00  1.5616350e+00  2.4153082e+00  2.5785027e+00\n",
            " -3.6446410e-01  3.0473328e-01 -6.0826463e-01 -1.1008358e+00\n",
            "  5.7924190e-04 -2.1493638e+00 -4.9723701e+00 -4.4495182e+00]\n",
            "doorway: 0.940357506275177\n",
            "drawbridge: 0.925424337387085\n",
            "doors: 0.8861480355262756\n",
            "closet: 0.8858508467674255\n",
            "closed: 0.8701543211936951\n",
            "locked: 0.870032012462616\n",
            "wall: 0.8460908532142639\n",
            "secret: 0.8290387392044067\n",
            "square: 0.8033416271209717\n",
            "stairs: 0.7987379431724548\n",
            "downwards: 0.7928858995437622\n",
            "hole: 0.7917185425758362\n",
            "corridor: 0.7909206748008728\n",
            "ladder: 0.7736232876777649\n",
            "floor: 0.7626956105232239\n",
            "portcullis: 0.7608734965324402\n",
            "vibrating: 0.7536466717720032\n",
            "downstairs: 0.7511041760444641\n",
            "chest: 0.7509174346923828\n",
            "doorways: 0.750511646270752\n",
            "magically: 0.7503724694252014\n",
            "couldnt: 0.7420575022697449\n",
            "grave: 0.741557776927948\n",
            "untrapped: 0.7406994104385376\n",
            "unseen: 0.7403778433799744\n",
            "drawbridges: 0.7375343441963196\n",
            "closing: 0.7368388175964355\n",
            "lock: 0.7343628406524658\n",
            "shop: 0.7342779636383057\n",
            "underneath: 0.7328135967254639\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "w2v = Word2Vec.load(\"wiki.model\")\n",
        "v1 = w2v.wv['dog']\n",
        "print(v1) # Return a 300 Dim vector\n",
        "\n",
        "similar_words = w2v.wv.most_similar('door', topn=30)  # topn determines how many similar words you want\n",
        "\n",
        "# Print the similar words\n",
        "for similar_word, similarity_score in similar_words:\n",
        "    print(f\"{similar_word}: {similarity_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lnZInqnVxJ6",
        "outputId": "0a4994d6-1801-41f1-efb3-63921239b162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'CartPole-v0': EnvSpec(id='CartPole-v0', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv'), 'CartPole-v1': EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv'), 'MountainCar-v0': EnvSpec(id='MountainCar-v0', entry_point='gymnasium.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0, additional_wrappers=(), vector_entry_point=None), 'MountainCarContinuous-v0': EnvSpec(id='MountainCarContinuous-v0', entry_point='gymnasium.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0, additional_wrappers=(), vector_entry_point=None), 'Pendulum-v1': EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None), 'Acrobot-v1': EnvSpec(id='Acrobot-v1', entry_point='gymnasium.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1, additional_wrappers=(), vector_entry_point=None), 'phys2d/CartPole-v0': EnvSpec(id='phys2d/CartPole-v0', entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='CartPole', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxVectorEnv'), 'phys2d/CartPole-v1': EnvSpec(id='phys2d/CartPole-v1', entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxVectorEnv'), 'phys2d/Pendulum-v0': EnvSpec(id='phys2d/Pendulum-v0', entry_point='gymnasium.envs.phys2d.pendulum:PendulumJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='Pendulum', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.pendulum:PendulumJaxVectorEnv'), 'LunarLander-v2': EnvSpec(id='LunarLander-v2', entry_point='gymnasium.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2, additional_wrappers=(), vector_entry_point=None), 'LunarLanderContinuous-v2': EnvSpec(id='LunarLanderContinuous-v2', entry_point='gymnasium.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2, additional_wrappers=(), vector_entry_point=None), 'BipedalWalker-v3': EnvSpec(id='BipedalWalker-v3', entry_point='gymnasium.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3, additional_wrappers=(), vector_entry_point=None), 'BipedalWalkerHardcore-v3': EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gymnasium.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3, additional_wrappers=(), vector_entry_point=None), 'CarRacing-v2': EnvSpec(id='CarRacing-v2', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2, additional_wrappers=(), vector_entry_point=None), 'Blackjack-v1': EnvSpec(id='Blackjack-v1', entry_point='gymnasium.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1, additional_wrappers=(), vector_entry_point=None), 'FrozenLake-v1': EnvSpec(id='FrozenLake-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1, additional_wrappers=(), vector_entry_point=None), 'FrozenLake8x8-v1': EnvSpec(id='FrozenLake8x8-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1, additional_wrappers=(), vector_entry_point=None), 'CliffWalking-v0': EnvSpec(id='CliffWalking-v0', entry_point='gymnasium.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0, additional_wrappers=(), vector_entry_point=None), 'Taxi-v3': EnvSpec(id='Taxi-v3', entry_point='gymnasium.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3, additional_wrappers=(), vector_entry_point=None), 'tabular/Blackjack-v0': EnvSpec(id='tabular/Blackjack-v0', entry_point='gymnasium.envs.tabular.blackjack:BlackJackJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sutton_and_barto': True, 'natural': False}, namespace='tabular', name='Blackjack', version=0, additional_wrappers=(), vector_entry_point=None), 'tabular/CliffWalking-v0': EnvSpec(id='tabular/CliffWalking-v0', entry_point='gymnasium.envs.tabular.cliffwalking:CliffWalkingJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='tabular', name='CliffWalking', version=0, additional_wrappers=(), vector_entry_point=None), 'Reacher-v2': EnvSpec(id='Reacher-v2', entry_point='gymnasium.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2, additional_wrappers=(), vector_entry_point=None), 'Reacher-v4': EnvSpec(id='Reacher-v4', entry_point='gymnasium.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4, additional_wrappers=(), vector_entry_point=None), 'Pusher-v2': EnvSpec(id='Pusher-v2', entry_point='gymnasium.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2, additional_wrappers=(), vector_entry_point=None), 'Pusher-v4': EnvSpec(id='Pusher-v4', entry_point='gymnasium.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4, additional_wrappers=(), vector_entry_point=None), 'InvertedPendulum-v2': EnvSpec(id='InvertedPendulum-v2', entry_point='gymnasium.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2, additional_wrappers=(), vector_entry_point=None), 'InvertedPendulum-v4': EnvSpec(id='InvertedPendulum-v4', entry_point='gymnasium.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4, additional_wrappers=(), vector_entry_point=None), 'InvertedDoublePendulum-v2': EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gymnasium.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2, additional_wrappers=(), vector_entry_point=None), 'InvertedDoublePendulum-v4': EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gymnasium.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v2': EnvSpec(id='HalfCheetah-v2', entry_point='gymnasium.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v3': EnvSpec(id='HalfCheetah-v3', entry_point='gymnasium.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v4': EnvSpec(id='HalfCheetah-v4', entry_point='gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4, additional_wrappers=(), vector_entry_point=None), 'Hopper-v2': EnvSpec(id='Hopper-v2', entry_point='gymnasium.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2, additional_wrappers=(), vector_entry_point=None), 'Hopper-v3': EnvSpec(id='Hopper-v3', entry_point='gymnasium.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3, additional_wrappers=(), vector_entry_point=None), 'Hopper-v4': EnvSpec(id='Hopper-v4', entry_point='gymnasium.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v2': EnvSpec(id='Swimmer-v2', entry_point='gymnasium.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v3': EnvSpec(id='Swimmer-v3', entry_point='gymnasium.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v4': EnvSpec(id='Swimmer-v4', entry_point='gymnasium.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v2': EnvSpec(id='Walker2d-v2', entry_point='gymnasium.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v3': EnvSpec(id='Walker2d-v3', entry_point='gymnasium.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v4': EnvSpec(id='Walker2d-v4', entry_point='gymnasium.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4, additional_wrappers=(), vector_entry_point=None), 'Ant-v2': EnvSpec(id='Ant-v2', entry_point='gymnasium.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2, additional_wrappers=(), vector_entry_point=None), 'Ant-v3': EnvSpec(id='Ant-v3', entry_point='gymnasium.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3, additional_wrappers=(), vector_entry_point=None), 'Ant-v4': EnvSpec(id='Ant-v4', entry_point='gymnasium.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v2': EnvSpec(id='Humanoid-v2', entry_point='gymnasium.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v3': EnvSpec(id='Humanoid-v3', entry_point='gymnasium.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v4': EnvSpec(id='Humanoid-v4', entry_point='gymnasium.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4, additional_wrappers=(), vector_entry_point=None), 'HumanoidStandup-v2': EnvSpec(id='HumanoidStandup-v2', entry_point='gymnasium.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2, additional_wrappers=(), vector_entry_point=None), 'HumanoidStandup-v4': EnvSpec(id='HumanoidStandup-v4', entry_point='gymnasium.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4, additional_wrappers=(), vector_entry_point=None), 'GymV21Environment-v0': EnvSpec(id='GymV21Environment-v0', entry_point=<function _raise_shimmy_error at 0x78a372580040>, reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='GymV21Environment', version=0, additional_wrappers=(), vector_entry_point=None), 'GymV26Environment-v0': EnvSpec(id='GymV26Environment-v0', entry_point=<function _raise_shimmy_error at 0x78a372580040>, reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='GymV26Environment', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHack-v0': EnvSpec(id='NetHack-v0', entry_point='nle.env.base:NLE', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHack', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackScore-v0': EnvSpec(id='NetHackScore-v0', entry_point='nle.env.tasks:NetHackScore', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackScore', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackStaircase-v0': EnvSpec(id='NetHackStaircase-v0', entry_point='nle.env.tasks:NetHackStaircase', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackStaircase', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackStaircasePet-v0': EnvSpec(id='NetHackStaircasePet-v0', entry_point='nle.env.tasks:NetHackStaircasePet', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackStaircasePet', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackOracle-v0': EnvSpec(id='NetHackOracle-v0', entry_point='nle.env.tasks:NetHackOracle', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackOracle', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackGold-v0': EnvSpec(id='NetHackGold-v0', entry_point='nle.env.tasks:NetHackGold', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackGold', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackEat-v0': EnvSpec(id='NetHackEat-v0', entry_point='nle.env.tasks:NetHackEat', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackEat', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackScout-v0': EnvSpec(id='NetHackScout-v0', entry_point='nle.env.tasks:NetHackScout', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackScout', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackChallenge-v0': EnvSpec(id='NetHackChallenge-v0', entry_point='nle.env.tasks:NetHackChallenge', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackChallenge', version=0, additional_wrappers=(), vector_entry_point=None)}\n",
            "Discrete(23)\n",
            "0 MiscAction.MORE\n",
            "1 CompassDirection.N\n",
            "2 CompassDirection.E\n",
            "3 CompassDirection.S\n",
            "4 CompassDirection.W\n",
            "5 CompassDirection.NE\n",
            "6 CompassDirection.SE\n",
            "7 CompassDirection.SW\n",
            "8 CompassDirection.NW\n",
            "9 CompassDirectionLonger.N\n",
            "10 CompassDirectionLonger.E\n",
            "11 CompassDirectionLonger.S\n",
            "12 CompassDirectionLonger.W\n",
            "13 CompassDirectionLonger.NE\n",
            "14 CompassDirectionLonger.SE\n",
            "15 CompassDirectionLonger.SW\n",
            "16 CompassDirectionLonger.NW\n",
            "17 MiscDirection.UP\n",
            "18 MiscDirection.DOWN\n",
            "19 MiscDirection.WAIT\n",
            "20 Command.KICK\n",
            "21 Command.EAT\n",
            "22 Command.SEARCH\n",
            "Dict('blstats': Box(-2147483648, 2147483647, (27,), int64), 'chars': Box(0, 255, (21, 79), uint8), 'colors': Box(0, 15, (21, 79), uint8), 'glyphs': Box(0, 5976, (21, 79), int16), 'inv_glyphs': Box(0, 5976, (55,), int16), 'inv_letters': Box(0, 127, (55,), uint8), 'inv_oclasses': Box(0, 18, (55,), uint8), 'inv_strs': Box(0, 255, (55, 80), uint8), 'message': Box(0, 255, (256,), uint8), 'screen_descriptions': Box(0, 127, (21, 79, 80), uint8), 'specials': Box(0, 255, (21, 79), uint8), 'tty_chars': Box(0, 255, (24, 80), uint8), 'tty_colors': Box(0, 31, (24, 80), int8), 'tty_cursor': Box(0, 255, (2,), uint8))\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import nle\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "from nle.env.tasks import NetHackChallenge\n",
        "from nle.env.tasks import NetHackScore\n",
        "import numpy as np\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "print(gym.envs.registry )\n",
        "env = gym.make(\"NetHackScore-v0\",character=\"mon-hum-neu-mal\") # NetHack-v0 NetHackScore-v0 NetHackChallenge-v0\n",
        "print(env.action_space)\n",
        "env.unwrapped.print_action_meanings()\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fydstESP6fBq"
      },
      "outputs": [],
      "source": [
        "env.reset()  # each reset generates a new dungeon\n",
        "#ob = env.step(1)  # move agent '@' north\n",
        "#env.render()\n",
        "\n",
        "def to_char(l):\n",
        "    try:\n",
        "        char_message = [chr(n) for n in l if 0 <= n <= 127]\n",
        "        char_message = ''.join(char_message)\n",
        "        return char_message\n",
        "    except TypeError:\n",
        "        print(f\"Error processing key: {i}, value type: {type(obs[i])}, value: {obs[i]}\") # Print more info for debugging\n",
        "\n",
        "a_list = [9,10,9,10,9,10]#[0,9,3,8,8,21]\n",
        "\n",
        "for i in range(len(a_list)):\n",
        "    #a = env.action_space.sample()\n",
        "    a = a_list[i]\n",
        "    print('act:',a)\n",
        "    #clear_output(wait=True) ###\n",
        "    obs, reward, done, truncation, info = env.step(a)\n",
        "    cord = [obs['blstats'][0],obs['blstats'][1]]\n",
        "    print(cord)\n",
        "    for i in ['glyphs','chars','colors','specials']:\n",
        "        print(i,crop(torch.tensor([obs[i]]),torch.tensor([cord])))\n",
        "    print(get_edge_index(torch.tensor([obs['glyphs']])))\n",
        "    # char_message = to_char(obs['message'])\n",
        "    # print(char_message)\n",
        "    # char_inv = to_char(obs['inv_letters'])\n",
        "    # print(char_inv)\n",
        "    # print(obs['inv_oclasses'])\n",
        "    # print(obs['inv_strs'])\n",
        "\n",
        "    #print(obs)\n",
        "    env.render()\n",
        "    time.sleep(1.0) ###\n",
        "\n",
        "#env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VPUFfjku6PYh",
        "DRO_HN7y7ibG",
        "BswoL49WBxye",
        "hTuLRQpqPLri",
        "OGzFIRamEp2S",
        "ywqo6l91oOSV",
        "Y0LyFoHBFCRi",
        "JamJ_ow3E0W7",
        "o87lJVnyNAFW",
        "N31kKPYpMco0"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}