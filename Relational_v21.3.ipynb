{"cells":[{"cell_type":"markdown","metadata":{"id":"VPUFfjku6PYh"},"source":["# Install NLE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KiytataVWnY4","outputId":"4241f90b-818e-4c17-b7aa-16302d29f333","executionInfo":{"status":"ok","timestamp":1733809055496,"user_tz":-480,"elapsed":17427,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["55 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","\u001b[1;31mE: \u001b[0mUnable to locate package libglib2.0\u001b[0m\n","\u001b[1;31mE: \u001b[0mCouldn't find any package by glob 'libglib2.0'\u001b[0m\n","Requirement already satisfied: gnuplotlib in /usr/local/lib/python3.10/dist-packages (0.42)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gnuplotlib) (1.26.4)\n","Requirement already satisfied: numpysane>=0.3 in /usr/local/lib/python3.10/dist-packages (from gnuplotlib) (0.40)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","gnuplot is already the newest version (5.4.2+dfsg2-2).\n","0 upgraded, 0 newly installed, 0 to remove and 55 not upgraded.\n","Requirement already satisfied: nle in /usr/local/lib/python3.10/dist-packages (1.1.0)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle) (2.13.6)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from nle) (1.26.4)\n","Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.10/dist-packages (from nle) (1.0.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle) (0.0.4)\n","Requirement already satisfied: nle[agent] in /usr/local/lib/python3.10/dist-packages (1.1.0)\n","Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (2.13.6)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (1.26.4)\n","Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (1.0.0)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from nle[agent]) (2.5.1+cu121)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (3.1.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==1.0.0->nle[agent]) (0.0.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.1->nle[agent]) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.1->nle[agent]) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.1->nle[agent]) (3.0.2)\n","Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"]}],"source":["! apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0 libsm6 libxext6 cmake\n","! pip install gnuplotlib\n","! apt-get install -y gnuplot\n","! pip install nle\n","! pip install \"nle[agent]\"\n","! pip install torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sHtjr9ULoaW"},"outputs":[],"source":["# From: https://github.com/facebookresearch/nle/issues/359#issue-1782082844\n","# !sudo apt-get install -y build-essential autoconf libtool pkg-config \\\n","#     python3-dev python3-pip python3-numpy git flex bison libbz2-dev\n","# ! pip install --no-use-pep517 nle\n","# ! pip install \"nle[agent]\"\n","\n","# Faile\n","# !apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0 libsm6 libxext6 cmake\n","# !pip install nle==0.9.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ai1ceOJXJ1tR"},"outputs":[],"source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"]},{"cell_type":"markdown","metadata":{"id":"DRO_HN7y7ibG"},"source":["# Base Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-FbjyrY5c_xf"},"outputs":[],"source":["import argparse\n","import logging\n","import pprint\n","import threading\n","import time\n","import timeit\n","import traceback\n","import shutil\n","import numpy as np\n","\n","# Necessary for multithreading.\n","os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n","load_model = True\n","f_dir = 'torchbeast/plots/Relational_v21.3_3.tar'\n","log_time = 100\n","\n","act_list = [\n","    \"N\", \"E\", \"S\", \"W\", \"NE\", \"SE\", \"SW\", \"NW\",  # CompassDirection\n","    \"N_\", \"E_\", \"S_\", \"W_\", \"NE_\", \"SE_\", \"SW_\", \"NW_\",  # CompassDirectionLonger\n","    \"UP\", \"DOWN\", \"WAIT\", \"MORE\",           # MiscDirection and MiscAction\n","    \"ADJUST\", \"APPLY\", \"ATTRIBUTES\", \"CALL\", \"CAST\", \"CHAT\", \"CLOSE\", \"DIP\", \"DROP\", \"DROPTYPE\",\n","    \"EAT\", \"ENGRAVE\", \"ENHANCE\", \"ESC\", \"FIGHT\", \"FIRE\", \"FORCE\", \"INVENTORY\", \"INVENTTYPE\", \"INVOKE\",\n","    \"JUMP\", \"KICK\", \"LOOK\", \"LOOT\", \"MONSTER\", \"MOVE\", \"MOVEFAR\", \"OFFER\", \"OPEN\", \"PAY\", \"PICKUP\",\n","    \"PRAY\", \"PUTON\", \"QUAFF\", \"QUIVER\", \"READ\", \"REMOVE\", \"RIDE\", \"RUB\", \"RUSH\", \"RUSH2\", \"SEARCH\",\n","    \"SEEARMOR\", \"SEERINGS\", \"SEETOOLS\", \"SEETRAP\", \"SEEWEAPON\", \"SHELL\", \"SIT\", \"SWAP\", \"TAKEOFF\",\n","    \"TAKEOFFALL\", \"THROW\", \"TIP\", \"TURN\", \"TWOWEAPON\", \"UNTRAP\", \"VERSIONSHORT\", \"WEAR\", \"WIELD\",\n","    \"WIPE\", \"ZAP\",\n","    \"PLUS\", \"QUOTE\", \"DOLLAR\", \"SPACE\"         # TextCharacters\n","]\n","\n","act_useful = [\n","    \"MORE\",\n","    # C-m\n","    \"N\", \"E\", \"S\", \"W\", \"NE\", \"SE\", \"SW\", \"NW\",\n","    # k  l  j   h  u   n   b   y\n","    \"N_\", \"E_\", \"S_\", \"W_\", \"NE_\", \"SE_\", \"SW_\", \"NW_\",\n","    \"UP\", \"DOWN\", \"WAIT\", \"KICK\", \"EAT\", \"SEARCH\",\n","    # <   >    .   C-d    e    s\n","     \"PICKUP\", \"ESC\", \"DROP\", \"LOOK\",\n","    #  ,         d    :\n","     \"WIELD\" ,\"PUTON\", \"REMOVE\", \"WEAR\", \"TAKEOFF\",\n","    #  w    P     R     W     T\n","     \"APPLY\", \"CLOSE\", \"FIRE\", \"RUSH\", \"INVENTORY\", \"MOVE\",\n","    #  a    c     f    g     i     m\n","     \"OPEN\", \"PAY\", \"QUAFF\",# \"READ\", \"TAKEOFFALL\", \"UNTRAP\", \"ZAP\", \"CAST\",\n","    #  o    p    q\n","]\n","\n","# act_index = [act_list.index(i) for i in act_useful]\n","act_mask = [1 if action in act_useful else 0 for action in act_list]\n","\n","\n","try:\n","    import torch\n","    from torch import multiprocessing as mp\n","    from torch import nn\n","    from torch.nn import functional as F\n","except ImportError:\n","    logging.exception(\n","        \"PyTorch not found. Please install the agent dependencies with \"\n","        '`pip install \"nle[agent]\"`'\n","    )\n","\n","import gymnasium as gym  # noqa: E402\n","\n","import nle  # noqa: F401, E402\n","from nle import nethack  # noqa: E402\n","from nle.agent import vtrace  # noqa: E402\n","\n","# yapf: disable\n","parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n","\n","parser.add_argument(\"--env\", type=str, default=\"NetHackScore-v0\",\n","                    help=\"Gym environment.\")\n","parser.add_argument(\"--mode\", default=\"train\",\n","                    choices=[\"train\", \"test\", \"test_render\"],\n","                    help=\"Training or test mode.\")\n","\n","# Training settings.\n","parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n","                    help=\"Disable saving checkpoint.\")\n","parser.add_argument(\"--savedir\", default=\"~/torchbeast/\",\n","                    help=\"Root dir where experiment data will be saved.\")\n","parser.add_argument(\"--num_actors\", default=4, type=int, metavar=\"N\",\n","                    help=\"Number of actors (default: 4).\")\n","parser.add_argument(\"--total_steps\", default=100000, type=int, metavar=\"T\",\n","                    help=\"Total environment steps to train for.\")\n","parser.add_argument(\"--total_steps_\", default=100000, type=int, metavar=\"T\",\n","                    help=\"Total environment steps each time to train for.\")\n","parser.add_argument(\"--batch_size\", default=8, type=int, metavar=\"B\",\n","                    help=\"Learner batch size.\")\n","parser.add_argument(\"--unroll_length\", default=80, type=int, metavar=\"T\",\n","                    help=\"The unroll length (time dimension).\")\n","parser.add_argument(\"--num_buffers\", default=None, type=int,\n","                    metavar=\"N\", help=\"Number of shared-memory buffers.\")\n","parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=2, type=int,\n","                    metavar=\"N\", help=\"Number learner threads.\")\n","parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n","                    help=\"Disable CUDA.\")\n","parser.add_argument(\"--use_lstm\", action=\"store_true\",\n","                    help=\"Use LSTM in agent model.\")\n","parser.add_argument(\"--save_ttyrec_every\", default=1000, type=int,\n","                    metavar=\"N\", help=\"Save ttyrec every N episodes.\")\n","\n","\n","# Loss settings.\n","parser.add_argument(\"--entropy_cost\", default=0.0006,\n","                    type=float, help=\"Entropy cost/multiplier.\")\n","parser.add_argument(\"--baseline_cost\", default=0.5,\n","                    type=float, help=\"Baseline cost/multiplier.\")\n","parser.add_argument(\"--discounting\", default=0.99,\n","                    type=float, help=\"Discounting factor.\")\n","parser.add_argument(\"--reward_clipping\", default=\"abs_one\",\n","                    choices=[\"abs_one\", \"none\"],\n","                    help=\"Reward clipping.\")\n","\n","# Optimizer settings.\n","parser.add_argument(\"--learning_rate\", default=0.00048,\n","                    type=float, metavar=\"LR\", help=\"Learning rate.\")\n","parser.add_argument(\"--alpha\", default=0.99, type=float,\n","                    help=\"RMSProp smoothing constant.\")\n","parser.add_argument(\"--momentum\", default=0, type=float,\n","                    help=\"RMSProp momentum.\")\n","parser.add_argument(\"--epsilon\", default=0.01, type=float,\n","                    help=\"RMSProp epsilon.\")\n","parser.add_argument(\"--grad_norm_clipping\", default=40.0, type=float,\n","                    help=\"Global gradient norm clip.\")\n","# yapf: enable\n","\n","\n","logging.basicConfig(\n","    format=(\n","        \"[%(levelname)s:%(process)d %(module)s:%(lineno)d %(asctime)s] \" \"%(message)s\"\n","    ),\n","    level=logging.INFO,\n",")\n","\n","\n","def nested_map(f, n):\n","    if isinstance(n, tuple) or isinstance(n, list):\n","        return n.__class__(nested_map(f, sn) for sn in n)\n","    if isinstance(n, dict):\n","        return {k: nested_map(f, v) for k, v in n.items()}\n","    return f(n)\n","\n","\n","def compute_baseline_loss(advantages):\n","    return 0.5 * torch.sum(advantages**2)\n","\n","\n","def compute_entropy_loss(logits):\n","    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n","    policy = F.softmax(logits, dim=-1)\n","    log_policy = F.log_softmax(logits, dim=-1)\n","    return torch.sum(policy * log_policy)\n","\n","\n","def compute_policy_gradient_loss(logits, actions, advantages):\n","    cross_entropy = F.nll_loss(\n","        F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n","        target=torch.flatten(actions, 0, 1),\n","        reduction=\"none\",\n","    )\n","    cross_entropy = cross_entropy.view_as(advantages)\n","    return torch.sum(cross_entropy * advantages.detach())\n","\n","\n","def create_env(name, *args, **kwargs):\n","    return gym.make(name, observation_keys=(\"glyphs\", \"blstats\", \"message\", \"inv_glyphs\", \"inv_letters\", \"inv_oclasses\", \"inv_strs\"), *args, **kwargs)  # noqa: B026\n","    # \"inv_glyphs\",\"inv_letters\", \"inv_oclasses\", \"inv_strs\"\n","\n","def act(\n","    flags,\n","    actor_index: int,\n","    free_queue: mp.SimpleQueue,\n","    full_queue: mp.SimpleQueue,\n","    model: torch.nn.Module,\n","    buffers,\n","    initial_agent_state_buffers,\n","):\n","    try:\n","        logging.info(\"Actor %i started.\", actor_index)\n","\n","        gym_env = create_env(\n","            flags.env, savedir=flags.rundir, save_ttyrec_every=flags.save_ttyrec_every\n","        )\n","        env = ResettingEnvironment(gym_env)\n","        env_output = env.initial()\n","        agent_state = model.initial_state(batch_size=1)\n","        agent_output, unused_state = model(env_output, agent_state)\n","        while True:\n","            index = free_queue.get()\n","            if index is None:\n","                break\n","\n","            # Write old rollout end.\n","            for key in env_output:\n","                buffers[key][index][0, ...] = env_output[key]\n","            for key in agent_output:\n","                buffers[key][index][0, ...] = agent_output[key]\n","            for i, tensor in enumerate(agent_state):\n","                initial_agent_state_buffers[index][i][...] = tensor\n","\n","            # Do new rollout.\n","            for t in range(flags.unroll_length):\n","                with torch.no_grad():\n","                    agent_output, agent_state = model(env_output, agent_state)\n","\n","                env_output = env.step(agent_output[\"action\"])\n","\n","                for key in env_output:\n","                    buffers[key][index][t + 1, ...] = env_output[key]\n","                for key in agent_output:\n","                    buffers[key][index][t + 1, ...] = agent_output[key]\n","\n","            full_queue.put(index)\n","\n","    except KeyboardInterrupt:\n","        pass  # Return silently.\n","    except Exception:\n","        logging.error(\"Exception in worker process %i\", actor_index)\n","        traceback.print_exc()\n","        print()\n","        raise\n","\n","\n","def get_batch(\n","    flags,\n","    free_queue: mp.SimpleQueue,\n","    full_queue: mp.SimpleQueue,\n","    buffers,\n","    initial_agent_state_buffers,\n","    lock=threading.Lock(),\n","):\n","    with lock:\n","        indices = [full_queue.get() for _ in range(flags.batch_size)]\n","    batch = {\n","        key: torch.stack([buffers[key][m] for m in indices], dim=1) for key in buffers\n","    }\n","    initial_agent_state = (\n","        torch.cat(ts, dim=1)\n","        for ts in zip(*[initial_agent_state_buffers[m] for m in indices])\n","    )\n","    for m in indices:\n","        free_queue.put(m)\n","    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n","    initial_agent_state = tuple(\n","        t.to(device=flags.device, non_blocking=True) for t in initial_agent_state\n","    )\n","    return batch, initial_agent_state\n","\n","\n","def learn(\n","    flags,\n","    actor_model,\n","    model,\n","    batch,\n","    initial_agent_state,\n","    optimizer,\n","    scheduler,\n","    lock=threading.Lock(),  # noqa: B008\n","):\n","    \"\"\"Performs a learning (optimization) step.\"\"\"\n","    with lock:\n","        learner_outputs, unused_state = model(batch, initial_agent_state)\n","\n","        # Take final value function slice for bootstrapping.\n","        bootstrap_value = learner_outputs[\"baseline\"][-1]\n","\n","        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n","        batch = {key: tensor[1:] for key, tensor in batch.items()}\n","        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n","\n","        rewards = batch[\"reward\"]\n","\n","\n","        if flags.reward_clipping == \"abs_one\":\n","            clipped_rewards = torch.clamp(rewards, -1, 1)\n","        elif flags.reward_clipping == \"none\":\n","            clipped_rewards = rewards\n","\n","        discounts = (~batch[\"done\"]).float() * flags.discounting\n","\n","        vtrace_returns = vtrace.from_logits(\n","            behavior_policy_logits=batch[\"policy_logits\"],\n","            target_policy_logits=learner_outputs[\"policy_logits\"],\n","            actions=batch[\"action\"],\n","            discounts=discounts,\n","            rewards=clipped_rewards,\n","            values=learner_outputs[\"baseline\"],\n","            bootstrap_value=bootstrap_value,\n","        )\n","\n","        pg_loss = compute_policy_gradient_loss(\n","            learner_outputs[\"policy_logits\"],\n","            batch[\"action\"],\n","            vtrace_returns.pg_advantages,\n","        )\n","        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n","            vtrace_returns.vs - learner_outputs[\"baseline\"]\n","        )\n","        entropy_loss = flags.entropy_cost * compute_entropy_loss(\n","            learner_outputs[\"policy_logits\"]\n","        )\n","\n","        total_loss = pg_loss + baseline_loss + entropy_loss\n","\n","        # TODO\n","        acts = learner_outputs[\"action\"]\n","        elements, counts = torch.unique(acts, return_counts=True)\n","        combined = list(zip([act_list[i] for i in elements.tolist()], counts.tolist()))\n","        combined_ = sorted(combined, key=lambda x: x[1], reverse=True)\n","        episode_returns = batch[\"episode_return\"][batch[\"done\"]]\n","\n","        stats = {\n","            \"episode_returns\": tuple(episode_returns.cpu().numpy()[:10]),\n","            \"mean_episode_return\": torch.mean(episode_returns).item(),\n","            \"total_loss\": total_loss.item(),\n","            \"pg_loss\": pg_loss.item(),\n","            \"baseline_loss\": baseline_loss.item(),\n","            \"entropy_loss\": entropy_loss.item(),\n","            #\"actions\": elements,\n","            \"action_counts\": combined_[:30],\n","            \"num_episodes\": episode_returns.shape[0]\n","        }\n","\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), flags.grad_norm_clipping)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        actor_model.load_state_dict(model.state_dict())\n","        return stats\n","\n","\n","def create_buffers(flags, observation_space, num_actions, num_overlapping_steps=1):\n","    size = (flags.unroll_length + num_overlapping_steps,)\n","\n","    # Get specimens to infer shapes and dtypes.\n","    samples = {k: torch.from_numpy(v) for k, v in observation_space.sample().items()}\n","\n","    specs = {\n","        key: dict(size=size + sample.shape, dtype=sample.dtype)\n","        for key, sample in samples.items()\n","    }\n","    specs.update(\n","        reward=dict(size=size, dtype=torch.float32),\n","        done=dict(size=size, dtype=torch.bool),\n","        episode_return=dict(size=size, dtype=torch.float32),\n","        episode_step=dict(size=size, dtype=torch.int32),\n","        policy_logits=dict(size=size + (num_actions,), dtype=torch.float32),\n","        baseline=dict(size=size, dtype=torch.float32),\n","        last_action=dict(size=size, dtype=torch.int64),\n","        action=dict(size=size, dtype=torch.int64),\n","    )\n","    buffers = {key: [] for key in specs}\n","    for _ in range(flags.num_buffers):\n","        for key in buffers:\n","            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n","    return buffers\n","\n","\n","def _format_observations(observation, keys=(\"glyphs\", \"blstats\", \"message\", \"inv_glyphs\", \"inv_letters\", \"inv_oclasses\", \"inv_strs\")):\n","    observations = {}\n","    for key in keys:\n","        entry = observation[key]\n","        entry = torch.from_numpy(entry)\n","        entry = entry.view((1, 1) + entry.shape)  # (...) -> (T,B,...).\n","        observations[key] = entry\n","    return observations\n","\n","\n","class ResettingEnvironment:\n","    \"\"\"Turns a Gym environment into something that can be step()ed indefinitely.\"\"\"\n","\n","    def __init__(self, gym_env):\n","        self.gym_env = gym_env\n","        self.episode_return = None\n","        self.episode_step = None\n","        self.depth = 1\n","        self.hungry = 1\n","        self.glyphs = None\n","        self.glyphs_sum = None\n","        self.attr_index = [3, 4, 5, 6, 7, 8, 11, 15, 18]\n","        self.attributes = None\n","        self.AC = None\n","        self.T = 1\n","        self.frozen_init = -3\n","        self.loop_init = -20\n","        self.frozen_step = self.frozen_init\n","        self.loop_step = self.loop_init\n","\n","\n","\n","    def initial(self):\n","        initial_reward = torch.zeros(1, 1)\n","        # This supports only single-tensor actions ATM.\n","        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n","        self.episode_return = torch.zeros(1, 1)\n","        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n","        initial_done = torch.ones(1, 1, dtype=torch.uint8)\n","        obs, reset_info = self.gym_env.reset()\n","        self.depth = obs['blstats'][12]\n","        self.hungry = obs['blstats'][21]\n","        self.glyphs = np.count_nonzero(obs['glyphs'] == 2359)\n","        self.glyphs_sum = np.sum(obs['glyphs'])\n","        self.attributes = sum(obs['blstats'][pos] for pos in self.attr_index)\n","        self.AC = obs['blstats'][16]\n","\n","        self.T = obs['blstats'][20]\n","        self.frozen_step = self.frozen_init\n","        self.loop_step = self.loop_init\n","        result = _format_observations(obs)\n","        result.update(\n","            reward=initial_reward,\n","            done=initial_done,\n","            episode_return=self.episode_return,\n","            episode_step=self.episode_step,\n","            last_action=initial_last_action,\n","        )\n","        return result\n","\n","    def step(self, action):\n","        observation, reward, done, truncated, unused_info = self.gym_env.step(\n","            action.item()\n","        )\n","        self.episode_step += 1\n","\n","        self.episode_return += reward\n","        episode_step = self.episode_step\n","        episode_return = self.episode_return\n","\n","        ### reward shaping\n","        # + gain item\n","        # + gain non-crop food\n","        # - CC diff\n","        # + lv diff\n","        # - in trap\n","        #reward_ = torch.tensor(reward).view(1, 1)\n","\n","        # origin 4*exp 10\n","        reward = np.tanh(reward)*2\n","\n","        # attributes 3 4 5 6 7 8 / 11 13 15 16 18 HP Gold Mana AC(negative) lvl -----------------------------------------\n","        attributes = sum(observation['blstats'][pos] for pos in self.attr_index)\n","        reward += max(0,attributes-self.attributes)*2\n","        self.attributes = max(attributes,self.attributes)\n","        # AC\n","        AC = observation['blstats'][16]\n","        reward += max(0,self.AC-AC)*2\n","        self.AC = min(AC,self.AC)\n","\n","        # hungry 21\n","        hungry = observation['blstats'][21]\n","        reward += max(0,self.hungry-hungry)*3\n","        self.hungry = hungry\n","\n","        # discover 2359\n","        glyphs = np.count_nonzero(observation['glyphs'] == 2359)\n","        glyphs_diff = np.tanh(self.glyphs-glyphs)\n","        reward += max(0,glyphs_diff)\n","        self.glyphs = glyphs\n","\n","        # depth 12\n","        depth = observation['blstats'][12]\n","        reward += max(0,depth-self.depth-glyphs_diff)*5\n","        self.depth = max(depth,self.depth)\n","\n","        # time 20\n","        T = observation['blstats'][20]\n","        if T == self.T:\n","            self.frozen_step += 1\n","        else:\n","            self.frozen_step = self.frozen_init\n","        reward -= max(0,self.frozen_step)*0.001\n","        self.T = T\n","\n","        # loop\n","        glyphs_sum = np.sum(observation['glyphs'])\n","        if glyphs_sum == self.glyphs_sum:\n","            self.loop_step += 1\n","        else:\n","            self.loop_step = self.loop_init\n","        reward -= max(0,self.frozen_step)*0.001\n","        self.glyphs_sum = glyphs_sum\n","\n","        reward = np.tanh(reward)\n","        # print(f\"reward is {reward}\"\n","        ###\n","\n","        if done:\n","            observation, reset_info = self.gym_env.reset()\n","            self.episode_return = torch.zeros(1, 1)\n","            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n","            self.depth = 1\n","            self.hungry = 1\n","            self.glyphs = np.count_nonzero(observation['glyphs'] == 2359)\n","            self.glyphs_sum = np.sum(observation['glyphs'])\n","            self.attributes = sum(observation['blstats'][pos] for pos in self.attr_index)\n","            self.AC = observation['blstats'][16]\n","            self.T = observation['blstats'][20]\n","            self.frozen_step = self.frozen_init\n","            self.loop_step = self.loop_init\n","\n","        result = _format_observations(observation)\n","\n","        reward = torch.tensor(reward).view(1, 1)\n","        done = torch.tensor(done).view(1, 1)\n","\n","        result.update(\n","            reward=reward,\n","            #reward_=reward_,\n","            done=done,\n","            episode_return=episode_return,\n","            episode_step=episode_step,\n","            last_action=action,\n","        )\n","        return result\n","\n","    def close(self):\n","        self.gym_env.close()\n","\n","\n","def train(flags):  # pylint: disable=too-many-branches, too-many-statements\n","    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n","\n","    rundir = os.path.join(\n","        flags.savedir, \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n","    )\n","\n","    if not os.path.exists(rundir):\n","        os.makedirs(rundir)\n","    logging.info(\"Logging results to %s\", rundir)\n","\n","    symlink = os.path.join(flags.savedir, \"latest\")\n","    try:\n","        if os.path.islink(symlink):\n","            os.remove(symlink)\n","        if not os.path.exists(symlink):\n","            os.symlink(rundir, symlink)\n","        logging.info(\"Symlinked log directory: %s\", symlink)\n","    except OSError:\n","        raise\n","\n","    logfile = open(os.path.join(rundir, \"logs.tsv\"), \"a\", buffering=1)\n","\n","    checkpointpath = os.path.join(rundir, \"model.tar\")\n","\n","    flags.rundir = rundir\n","\n","    if flags.num_buffers is None:  # Set sensible default for num_buffers.\n","        flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n","    if flags.num_actors >= flags.num_buffers:\n","        raise ValueError(\"num_buffers should be larger than num_actors\")\n","    if flags.num_buffers < flags.batch_size:\n","        raise ValueError(\"num_buffers should be larger than batch_size\")\n","\n","    T = flags.unroll_length\n","    B = flags.batch_size\n","\n","    flags.device = None\n","    if not flags.disable_cuda and torch.cuda.is_available():\n","        logging.info(\"Using CUDA.\")\n","        flags.device = torch.device(\"cuda\")\n","    else:\n","        logging.info(\"Not using CUDA.\")\n","        flags.device = torch.device(\"cpu\")\n","\n","    env = create_env(flags.env)\n","    observation_space = env.observation_space\n","    action_space = env.action_space\n","    del env  # End this before forking.\n","\n","    model = Net(observation_space, action_space.n, flags.use_lstm)\n","\n","    # load model #\n","    if load_model:\n","        ckpt = torch.load(f_dir, map_location=\"cpu\")\n","        state_dict = ckpt[\"model_state_dict\"]\n","\n","        # if 'act_mask' in state_dict:\n","        #     del state_dict['act_mask']\n","\n","        model.load_state_dict(state_dict,)\n","        model.reset_act_mask(act_mask)\n","\n","\n","    buffers = create_buffers(flags, observation_space, model.num_actions)\n","\n","    model.share_memory()\n","\n","    # Add initial RNN state.\n","    initial_agent_state_buffers = []\n","    for _ in range(flags.num_buffers):\n","        state = model.initial_state(batch_size=1)\n","        for t in state:\n","            t.share_memory_()\n","        initial_agent_state_buffers.append(state)\n","\n","    actor_processes = []\n","    ctx = mp.get_context(\"fork\")\n","    free_queue = ctx.SimpleQueue()\n","    full_queue = ctx.SimpleQueue()\n","\n","    for i in range(flags.num_actors):\n","        actor = ctx.Process(\n","            target=act,\n","            args=(\n","                flags,\n","                i,\n","                free_queue,\n","                full_queue,\n","                model,\n","                buffers,\n","                initial_agent_state_buffers,\n","            ),\n","            name=\"Actor-%i\" % i,\n","        )\n","        actor.start()\n","        actor_processes.append(actor)\n","\n","    learner_model = Net(observation_space, action_space.n, flags.use_lstm).to(\n","        device=flags.device\n","    )\n","    learner_model.load_state_dict(model.state_dict())\n","\n","    optimizer = torch.optim.RMSprop(\n","        learner_model.parameters(),\n","        lr=flags.learning_rate,\n","        momentum=flags.momentum,\n","        eps=flags.epsilon,\n","        alpha=flags.alpha,\n","    )\n","\n","    # optimizer = torch.optim.Adam(\n","    #     learner_model.parameters(),\n","    #     lr=flags.learning_rate,\n","    #     eps=flags.epsilon,\n","    # )\n","\n","    def lr_lambda(epoch):\n","        return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n","\n","    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    if load_model:\n","        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n","        scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n","        last_epoch = ckpt[\"scheduler_state_dict\"][\"last_epoch\"]\n","        step = last_epoch*T*B\n","    else:\n","        step = 0\n","\n","    stat_keys = [\n","        \"total_loss\",\n","        \"mean_episode_return\",\n","        \"pg_loss\",\n","        \"baseline_loss\",\n","        \"entropy_loss\",\n","        \"num_episodes\",\n","    ]\n","    logfile.write(\"# Step\\t%s\\n\" % \"\\t\".join(stat_keys))\n","\n","    step_, stats = 0, {}\n","\n","    def batch_and_learn(i, lock=threading.Lock()):\n","        \"\"\"Thread target for the learning process.\"\"\"\n","        nonlocal step, step_, stats\n","        while step_ < flags.total_steps_:\n","            batch, agent_state = get_batch(\n","                flags, free_queue, full_queue, buffers, initial_agent_state_buffers\n","            )\n","\n","            stats = learn(\n","                flags, model, learner_model, batch, agent_state, optimizer, scheduler\n","            )\n","            with lock:\n","                logfile.write(\"%i\\t\" % step)\n","                logfile.write(\"\\t\".join(str(stats[k]) for k in stat_keys))\n","                logfile.write(\"\\n\")\n","                step += T * B\n","                step_ += T * B\n","\n","    for m in range(flags.num_buffers):\n","        free_queue.put(m)\n","\n","    threads = []\n","    for i in range(flags.num_learner_threads):\n","        thread = threading.Thread(\n","            target=batch_and_learn,\n","            name=\"batch-and-learn-%d\" % i,\n","            args=(i,),\n","            daemon=True,  # To support KeyboardInterrupt below.\n","        )\n","        thread.start()\n","        threads.append(thread)\n","\n","    def checkpoint():\n","        if flags.disable_checkpoint:\n","            return\n","        logging.info(\"Saving checkpoint to %s\", checkpointpath)\n","        torch.save(\n","            {\n","                \"model_state_dict\": model.state_dict(),\n","                \"optimizer_state_dict\": optimizer.state_dict(),\n","                \"scheduler_state_dict\": scheduler.state_dict(),\n","                \"flags\": vars(flags),\n","            },\n","            checkpointpath,\n","        )\n","\n","    timer = timeit.default_timer\n","    try:\n","        last_checkpoint_time = timer()\n","        start_step = 0 ####\n","        while step_ < flags.total_steps_:\n","\n","            if start_step == step_: ####\n","                continue\n","            else:\n","                start_step = step_\n","            start_time = timer()\n","            time.sleep(log_time)\n","\n","            if timer() - last_checkpoint_time > 30 * 60:  # Save every 10 min.\n","                checkpoint()\n","                last_checkpoint_time = timer()\n","                source_path = os.path.join(rundir, \"logs.tsv\")\n","                destination_path = os.path.join(rundir, \"logs_.tsv\")\n","                shutil.copy(source_path, destination_path)\n","\n","            #sps = (step - start_step) / (timer() - start_time)\n","            sps = timer()\n","            if stats.get(\"episode_returns\", None):\n","                mean_return = (\n","                    \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n","                )\n","            else:\n","                mean_return = \"\"\n","            total_loss = stats.get(\"total_loss\", float(\"inf\"))\n","            logging.info(\n","                \"Steps %i ( %i of %i ) @ %.1f SPS. Loss %f. %sStats:\\n%s\",\n","                step_,\n","                step,\n","                flags.total_steps,\n","                sps,\n","                total_loss,\n","                mean_return,\n","                pprint.pformat(stats),\n","            )\n","    except KeyboardInterrupt:\n","        logging.warning(\"Quitting.\")\n","        return  # Try joining actors then quit.\n","    else:\n","        for thread in threads:\n","            thread.join()\n","        logging.info(\"Learning finished after %d steps.\", step)\n","    finally:\n","        for _ in range(flags.num_actors):\n","            free_queue.put(None)\n","        for actor in actor_processes:\n","            actor.join(timeout=1)\n","\n","    checkpoint()\n","    logfile.close()\n","\n","\n","def test(flags, num_episodes=10):\n","    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n","    checkpointpath = os.path.join(flags.savedir, \"GAT_TRAN_test_4\", \"model.tar\")\n","\n","    gym_env = create_env(flags.env, save_ttyrec_every=flags.save_ttyrec_every)\n","    env = ResettingEnvironment(gym_env)\n","    model = Net(gym_env.observation_space, gym_env.action_space.n, flags.use_lstm)\n","    model.eval()\n","    checkpoint = torch.load(checkpointpath, map_location=\"cpu\")\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","    observation = env.initial()\n","    returns = []\n","\n","    agent_state = model.initial_state(batch_size=1)\n","\n","    while len(returns) < num_episodes:\n","        if flags.mode == \"test_render\":\n","            env.gym_env.render()\n","        policy_outputs, agent_state = model(observation, agent_state)\n","        observation = env.step(policy_outputs[\"action\"])\n","        if observation[\"done\"].item():\n","            returns.append(observation[\"episode_return\"].item())\n","            logging.info(\n","                \"Episode ended after %d steps. Return: %.1f\",\n","                observation[\"episode_step\"].item(),\n","                observation[\"episode_return\"].item(),\n","            )\n","    env.close()\n","    logging.info(\n","        \"Average returns over %i steps: %.1f\", num_episodes, sum(returns) / len(returns)\n","    )\n","\n","\n","class RandomNet(nn.Module):\n","    def __init__(self, observation_shape, num_actions, use_lstm):\n","        super(RandomNet, self).__init__()\n","        del observation_shape, use_lstm\n","        self.num_actions = num_actions\n","        self.theta = torch.nn.Parameter(torch.zeros(self.num_actions))\n","\n","    def forward(self, inputs, core_state):\n","        # print(inputs)\n","        T, B, *_ = inputs[\"observation\"].shape\n","        zeros = self.theta * 0\n","        # set logits to 0\n","        policy_logits = zeros[None, :].expand(T * B, -1)\n","        # set baseline to 0\n","        baseline = policy_logits.sum(dim=1).view(-1, B)\n","\n","        # sample random action\n","        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1).view(\n","            T, B\n","        )\n","        policy_logits = policy_logits.view(T, B, self.num_actions)\n","        return (\n","            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n","            core_state,\n","        )\n","\n","    def initial_state(self, batch_size):\n","        return ()\n","\n","\n","def _step_to_range(delta, num_steps):\n","    \"\"\"Range of `num_steps` integers with distance `delta` centered around zero.\"\"\"\n","    return delta * torch.arange(-num_steps // 2, num_steps // 2)\n","\n","\n","class Crop(nn.Module):\n","    \"\"\"Helper class for NetHackNet below.\"\"\"\n","\n","    def __init__(self, height, width, height_target, width_target):\n","        super(Crop, self).__init__()\n","        self.width = width\n","        self.height = height\n","        self.width_target = width_target\n","        self.height_target = height_target\n","        width_grid = _step_to_range(2 / (self.width - 1), self.width_target)[\n","            None, :\n","        ].expand(self.height_target, -1)\n","        height_grid = _step_to_range(2 / (self.height - 1), height_target)[\n","            :, None\n","        ].expand(-1, self.width_target)\n","\n","        # \"clone\" necessary, https://github.com/pytorch/pytorch/issues/34880\n","        self.register_buffer(\"width_grid\", width_grid.clone())\n","        self.register_buffer(\"height_grid\", height_grid.clone())\n","\n","    def forward(self, inputs, coordinates):\n","        \"\"\"Calculates centered crop around given x,y coordinates.\n","        Args:\n","           inputs [B x H x W]\n","           coordinates [B x 2] x,y coordinates\n","        Returns:\n","           [B x H' x W'] inputs cropped and centered around x,y coordinates.\n","        \"\"\"\n","        assert inputs.shape[1] == self.height\n","        assert inputs.shape[2] == self.width\n","\n","        inputs = inputs[:, None, :, :].float()\n","\n","        x = coordinates[:, 0]+1\n","        y = coordinates[:, 1]+1\n","\n","        x_shift = 2 / (self.width - 1) * (x.float() - self.width // 2)\n","        y_shift = 2 / (self.height - 1) * (y.float() - self.height // 2)\n","\n","        grid = torch.stack(\n","            [\n","                self.width_grid[None, :, :] + x_shift[:, None, None],\n","                self.height_grid[None, :, :] + y_shift[:, None, None],\n","            ],\n","            dim=3,\n","        )\n","\n","        # TODO: only cast to int if original tensor was int\n","        return (\n","            torch.round(F.grid_sample(inputs, grid, align_corners=True))\n","            .squeeze(1)\n","            .long()\n","        )\n"]},{"cell_type":"markdown","metadata":{"id":"BswoL49WBxye"},"source":["# relational"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ki7s-mVIB2pK"},"outputs":[],"source":["# 'chars': Box(0, 255, (21, 79), uint8), 'colors': Box(0, 15, (21, 79), uint8), 'specials': Box(0, 255, (21, 79), uint8),\n","# 'inv_glyphs': Box(0, 5976, (55,), int16), 'inv_letters': Box(0, 127, (55,), uint8), 'inv_oclasses': Box(0, 18, (55,), uint8), 'inv_strs': Box(0, 255, (55, 80), uint8),\n","# 'chars','colors','specials'\n","from torch.nn import Linear, LayerNorm\n","import numpy as np\n","from torch.nn.functional import one_hot\n","\n","class NetHackNet_GAT(nn.Module):\n","    def __init__(\n","        self,\n","        observation_shape,\n","        num_actions,\n","        use_lstm,\n","        embedding_dim=32,\n","        crop_dim=13,\n","        num_layers=5,\n","    ):\n","        super(NetHackNet_GAT, self).__init__()\n","\n","        self.act_mask = nn.Parameter((torch.tensor(act_mask, dtype=torch.float32)-1)*1e9, requires_grad=False)\n","\n","\n","        BLSTAT_NORMALIZATION_STATS = [[\n","            1.0 / 79.0, # hero col\n","            1.0 / 21, # hero row\n","            0.0, # strength pct\n","            1.0 / 10, # strength\n","            1.0 / 10, # dexterity\n","            1.0 / 10, # constitution\n","            1.0 / 10, # intelligence\n","            1.0 / 10, # wisdom\n","            1.0 / 10, # charisma\n","            0.0,      # score\n","            1.0 / 10, # hitpoints\n","            1.0 / 10, # max hitpoints\n","            1.0, # depth\n","            1.0 / 1000, # gold\n","            1.0 / 10, # energy\n","            1.0 / 10, # max energy\n","            1.0 / 10, # armor class\n","            0.0, # monster level\n","            1.0 / 10, # experience level\n","            1.0 / 100, # experience points\n","            1.0 / 1000, # time\n","            1.0, # hunger_state\n","            1.0 / 10, # carrying capacity\n","            0.0, # dungeon number\n","            0.0, # level number\n","            0.0, # condition bits\n","            0.0, # character alignment\n","            ]]\n","        self.blstats_scale = nn.Parameter(torch.tensor(BLSTAT_NORMALIZATION_STATS, dtype=torch.float32))\n","        self.BLSTAT_CLIP_RANGE = (0, 5)\n","\n","        self.glyph_shape = observation_shape[\"glyphs\"].shape\n","        self.blstats_size = observation_shape[\"blstats\"].shape[0]\n","        self.message_size = observation_shape[\"message\"].shape[0]\n","        self.inv_shape = observation_shape[\"inv_strs\"].shape\n","        self.timer = timeit.default_timer\n","        self.t = self.timer()\n","\n","        self.num_actions = num_actions\n","        self.base_model = False # not(use_lstm)\n","        self.use_message = True\n","        self.use_inv = True\n","\n","        self.H = self.glyph_shape[0]\n","        self.W = self.glyph_shape[1]\n","\n","        self.k_dim = embedding_dim  # glyph_dim\n","        self.s_dim = 128 # 128    blstats_dim\n","        self.att_fc_h_dim = 256\n","        self.att_fc_out_dim = 512   # 1024\n","        self.h_dim = 512   # 1024\n","        self.cnn_out_dim = self.s_dim   # 128\n","\n","        self.crop_dim = crop_dim\n","\n","\n","        #--------------inv emb---------------#\n","\n","        if self.use_inv:\n","            self.inv_embed = nn.Embedding(nethack.MAX_GLYPH+1, self.k_dim)\n","\n","            self.inv_emb_norm = nn.LayerNorm(self.k_dim)\n","\n","            self.embed_inv = nn.Sequential(\n","                nn.Linear(self.k_dim+self.inv_shape[-1]+1+19, 128),\n","                nn.ReLU(),\n","                nn.Linear(128, self.s_dim),\n","                nn.ReLU(),\n","                )\n","            self.inv_norm = nn.LayerNorm(self.s_dim)\n","\n","        #--------------------------------------#\n","\n","\n","\n","        #--------------CNN crop---------------#\n","        K = embedding_dim  # number of input filters\n","        F = 3  # filter dimensions\n","        S = 1  # stride\n","        P = 1  # padding\n","        M = 128  # number of intermediate filters   16\n","        Y = self.cnn_out_dim  # number of output filters  8\n","        L = num_layers  # number of convnet layers\n","\n","        self.crop = Crop(self.H, self.W, self.crop_dim, self.crop_dim)\n","        in_channels = [K] + [M] * (L - 1)\n","        out_channels = [M] * (L - 1) + [Y]\n","        def interleave(xs, ys):\n","            return [val for pair in zip(xs, ys) for val in pair]\n","        self.embed = nn.Embedding(nethack.MAX_GLYPH, self.k_dim)\n","        conv_extract_crop = [\n","            nn.Conv2d(\n","                in_channels=in_channels[i],\n","                out_channels=out_channels[i],\n","                kernel_size=(F, F),\n","                stride=S,\n","                padding=P,\n","            )\n","            for i in range(L)\n","        ]\n","\n","        self.extract_crop_representation = nn.Sequential(\n","            *interleave(conv_extract_crop, [nn.ELU()] * len(conv_extract_crop))\n","        )\n","        self.CNN_residual_norm = nn.LayerNorm(Y)\n","\n","        # self.conv_pool = nn.MaxPool2d(self.crop_dim,5)\n","        # self.CNN_residual_norm2 = nn.LayerNorm(Y)\n","        #--------------------------------------#\n","\n","        #---------------baseCNN----------------#\n","        if self.base_model:\n","            in_channels_ = [self.cnn_out_dim] + [32] * 4\n","            out_channels_ = [32] * 4 + [self.cnn_out_dim]\n","            base_conv = [\n","                nn.Conv2d(\n","                    in_channels=in_channels_[i],\n","                    out_channels=out_channels_[i],\n","                    kernel_size=(3, 3),\n","                    stride=1,\n","                    padding=1,\n","                )\n","                for i in range(len(in_channels_))\n","            ]\n","\n","            self.base_conv_representation = nn.Sequential(\n","                *interleave(base_conv, [nn.ELU()] * len(base_conv))\n","            )\n","            out_dim = self.att_fc_out_dim + self.s_dim\n","        #--------------------------------------#\n","\n","        #----------------ATT-------------------#\n","        else:\n","            # encoder_layer = nn.TransformerEncoderLayer(d_model=self.cnn_out_dim, nhead=2, batch_first=True)\n","            # self.attention = nn.TransformerEncoder(encoder_layer, num_layers=2)\n","            self.attention = nn.MultiheadAttention(embed_dim=self.cnn_out_dim, num_heads=2, dropout=0.1, batch_first=True)\n","            self.attention_mlp = nn.Sequential(\n","                nn.Linear(self.cnn_out_dim, self.att_fc_h_dim),\n","                nn.ReLU(),\n","                nn.Linear(self.att_fc_h_dim, self.cnn_out_dim),\n","                nn.ReLU(),\n","            )\n","            self.dropout = nn.Dropout(0.1)\n","\n","            self.att_norm1 = nn.LayerNorm(self.cnn_out_dim)\n","            self.att_norm2 = nn.LayerNorm(self.cnn_out_dim)\n","            self.att_norm3 = nn.LayerNorm(self.cnn_out_dim)\n","            self.att_norm4 = nn.LayerNorm(self.cnn_out_dim)\n","            self.dropout1 = nn.Dropout(0.1)\n","            self.dropout2 = nn.Dropout(0.1)\n","\n","            out_dim = self.att_fc_out_dim\n","            self.att_fc = nn.Sequential(\n","                nn.Linear(self.cnn_out_dim, self.att_fc_h_dim),\n","                nn.ReLU(),\n","                nn.Linear(self.att_fc_h_dim, self.att_fc_out_dim),\n","                nn.ReLU(),\n","            )\n","\n","        #--------------------------------------#\n","\n","        if self.use_message:\n","            self.embed_message = nn.Sequential(\n","                nn.Linear(self.message_size, 128),\n","                nn.ReLU(),\n","                nn.Linear(128, self.s_dim),\n","                nn.ReLU(),\n","            )\n","            self.message_norm = nn.LayerNorm(self.s_dim)\n","\n","        self.action_embed = nn.Embedding(self.num_actions, self.s_dim)\n","        self.embed_action = nn.Sequential(\n","            nn.Linear(self.s_dim, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, self.s_dim),\n","            nn.ReLU(),\n","        )\n","        self.embed_blstats = nn.Sequential(\n","            nn.Linear(self.blstats_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, self.s_dim),\n","            nn.ReLU(),\n","        )\n","\n","        self.att_norm = nn.LayerNorm(self.att_fc_out_dim)\n","        self.blstats_norm = nn.LayerNorm(self.s_dim)\n","        self.action_norm = nn.LayerNorm(self.s_dim)\n","\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(out_dim, self.h_dim),\n","            nn.ReLU(),\n","            nn.Linear(self.h_dim, self.h_dim),\n","            nn.ReLU(),\n","        )\n","\n","        self.core = nn.LSTM(self.h_dim, self.h_dim, num_layers=1)\n","\n","        self.policy = nn.Linear(self.h_dim, self.num_actions)\n","        self.baseline = nn.Linear(self.h_dim, 1)\n","\n","    def initial_state(self, batch_size=1):\n","        return tuple(\n","            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)\n","            for _ in range(2)\n","        )\n","    def reset_act_mask(self,a_mask=None):\n","        if a_mask is None:\n","            self.act_mask = None\n","        else:\n","            self.act_mask = nn.Parameter((torch.tensor(a_mask, dtype=torch.float32)-1)*1e9, requires_grad=False)\n","\n","    def _select(self, embed, x):\n","        # Work around slow backward pass of nn.Embedding, see\n","        # https://github.com/pytorch/pytorch/issues/24912\n","        out = embed.weight.index_select(0, x.reshape(-1))\n","        return out.reshape(x.shape + (-1,))\n","\n","    def forward(self, env_outputs, core_state):\n","        # print(env_outputs)\n","        # time.sleep(100)\n","\n","        # -- [T x B x H x W]\n","        glyphs = env_outputs[\"glyphs\"]\n","        T, B, *_ = glyphs.shape\n","\n","        # -- [T x B x F]\n","        blstats = env_outputs[\"blstats\"]\n","        # -- [B' x F]\n","        blstats = blstats.view(T * B, -1).float()\n","        coordinates = blstats[:, :2]\n","\n","        # -- [T x B x 1]\n","        last_actions = env_outputs[\"last_action\"]\n","\n","        #----------blstats+action---------#\n","        # -- [B' x 1]\n","        last_actions = last_actions.view(T * B, -1)\n","        # -- [B' x emb]\n","        action_emb_ = self.action_embed(last_actions)\n","        action_emb = self.embed_action(action_emb_)\n","\n","        ## scale\n","        blstats = blstats * self.blstats_scale\n","        blstats = torch.clamp(blstats, min=self.BLSTAT_CLIP_RANGE[0], max=self.BLSTAT_CLIP_RANGE[1])\n","\n","        ##\n","\n","        # -- [B' x 27]\n","        #blstats = torch.log1p(torch.nn.functional.relu(blstats))\n","\n","\n","        # -- [B' x 1+27]\n","        #blstats_action = torch.cat([action_emb.squeeze(1),blstats],dim=1)\n","\n","        # -- [B' x K''']\n","        blstats_emb = self.embed_blstats(blstats)\n","\n","        assert blstats_emb.shape[0] == T * B\n","        #----------------------------------#\n","\n","        #----------glyphs CNN-------------#\n","        # -- [B' x H x W]\n","        glyphs = torch.flatten(glyphs, 0, 1)  # Merge time and batch.\n","        # -- [B' x H x W]\n","        glyphs = glyphs.long()\n","        # -- [B' x H' x W']\n","        crop = self.crop(glyphs, coordinates)\n","        # -- [B' x H' x W' x K]\n","        crop_emb = self._select(self.embed, crop)\n","        # CNN crop model.\n","        # -- [B' x K x W' x H']\n","        crop_emb = crop_emb.transpose(1, 3)  # -- TODO: slow?\n","        # -- [B' x K' x W' x H']\n","        crop_rep = self.extract_crop_representation(crop_emb)\n","        #--------------------------------------#\n","\n","        ## full\n","        # glyphs_emb = self._select(self.embed, glyphs)\n","        # glyphs_emb = glyphs_emb.transpose(1, 3)\n","        # glyphs_rep_  = self.extract_crop_representation(glyphs_emb)\n","        # glyphs_rep = self.conv_pool(glyphs_rep_)\n","        ##\n","\n","        # crop_rep_ += crop_emb\n","        # crop_rep_ = crop_rep_.transpose(1, 3)\n","        # crop_rep = self.CNN_residual_norm(crop_rep_)\n","        # # -- [B' x K' x W' x H']\n","        # crop_rep = crop_rep.transpose(1, 3)\n","\n","\n","        #-------------base CNN----------------#\n","        if self.base_model:\n","            # # -- [B' x K' x W' x H']\n","            observation_att = self.base_conv_representation(crop_rep)\n","            observation_att = observation_att.view(T * B, -1, self.cnn_out_dim)\n","            observation_rep_, max_ind = torch.max(observation_att,dim=1)\n","            # -- [B' x K']\n","            observation_rep = self.att_fc(observation_rep_)\n","\n","            reps =[self.att_norm(observation_rep)]\n","            reps.append(self.blstats_norm(blstats_emb))\n","            reps.append(self.message_norm(message_emb))\n","\n","            st = torch.cat(reps, dim=1)\n","            # -- [B x K]\n","            st = self.fc(st)\n","\n","        #--------------------------------------#\n","\n","\n","\n","\n","\n","        #----------------ATT-------------------#\n","        else:\n","            # -- [B' x W'H' x K']\n","            crop_rep = crop_rep.reshape(T * B, -1, self.cnn_out_dim)\n","            assert crop_rep.shape[0] == T * B\n","            # glyphs_rep = glyphs_rep.reshape(T * B, -1, self.cnn_out_dim)\n","            # assert glyphs_rep.shape[0] == T * B\n","\n","            reps = [self.CNN_residual_norm(crop_rep)]\n","            # reps.append(self.CNN_residual_norm2(glyphs_rep))\n","            reps.append(self.blstats_norm(blstats_emb).unsqueeze(1))\n","            reps.append(self.action_norm(action_emb))\n","\n","            if self.use_inv:\n","                inv_glyphs = env_outputs[\"inv_glyphs\"]\n","                inv_glyphs_emb = self.inv_embed(inv_glyphs.long())\n","                inv_glyphs_emb = self.inv_emb_norm(inv_glyphs_emb)\n","                inv_letters = env_outputs[\"inv_letters\"]/127\n","                inv_letters = inv_letters.unsqueeze(-1)\n","                inv_oclasses = torch.nn.functional.one_hot(env_outputs[\"inv_oclasses\"].long(),19)\n","                inv_strs = env_outputs[\"inv_strs\"]/255\n","\n","                inv_emb_ = torch.cat((inv_glyphs_emb, inv_letters, inv_oclasses, inv_strs), dim=-1)\n","                inv_emb = self.embed_inv(inv_emb_)\n","                inv_emb = self.inv_norm(inv_emb)\n","                inv_emb = torch.flatten(inv_emb, 0, 1)\n","                reps.append(inv_emb)\n","\n","            if self.use_message:\n","                # -- [T x B x F]\n","                message = env_outputs[\"message\"]\n","                # -- [B' x F]\n","                message = message.view(T * B, -1).float()\n","                message_emb = self.embed_message(message/255)\n","                reps.append(self.message_norm(message_emb).unsqueeze(1))\n","\n","            # -- [B' x W'H'+3 x K']\n","            crop_rep_ = torch.cat(reps, dim=1)\n","\n","            # -- [B' x W'H' x K']\n","            #observation_att = self.attention(crop_rep_)\n","\n","            observation_att_, att_w = self.attention(crop_rep_, crop_rep_, crop_rep_)\n","            observation_att = self.att_norm1(self.dropout1(observation_att_) + crop_rep_)\n","            observation_att_mlp = self.att_norm2(self.attention_mlp(observation_att)+observation_att)\n","            #observation_att_2, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n","            observation_att = self.att_norm3(self.dropout2(observation_att_mlp) + observation_att_)\n","            observation_att_out = self.att_norm4(self.attention_mlp(observation_att)+observation_att)\n","            ## back\n","            # observation_att_, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n","            # observation_att_ = self.att_norm1(self.dropout1(observation_att_) + crop_rep_)\n","            # #observation_att_2, _ = self.attention(crop_rep_, crop_rep_, crop_rep_)\n","            # observation_att = self.att_norm2(self.dropout2(observation_att_) + crop_rep_)\n","\n","\n","\n","            # -- [B' x K']\n","            observation_rep_, max_ind = torch.max(observation_att_out,dim=1)\n","            # -- [B' x K']\n","            observation_rep = self.att_fc(observation_rep_)\n","            st = self.att_norm(observation_rep)\n","        #--------------------------------------#\n","\n","\n","        #----------------LSTM-------------------#\n","        core_input = st.view(T, B, -1)\n","        core_output_list = []\n","        notdone = (~env_outputs[\"done\"]).float()\n","        for input, nd in zip(core_input.unbind(), notdone.unbind()):\n","            # Reset core state to zero whenever an episode ended.\n","            # Make `done` broadcastable with (num_layers, B, hidden_size)\n","            # states:\n","            nd = nd.view(1, -1, 1)\n","            core_state = tuple(nd * s for s in core_state)\n","            output, core_state = self.core(input.unsqueeze(0), core_state)\n","            core_output_list.append(output)\n","        # -- [T x B x K']\n","        core_output_ = torch.cat(core_output_list)\n","\n","        # -- [B' x K']\n","        core_output = torch.flatten(core_output_, 0, 1)\n","\n","        #---------------------------------------#\n","\n","        # -- [B' x A]\n","        policy_logits = self.policy(core_output)\n","        # -- [B' x A]\n","        baseline = self.baseline(core_output)\n","\n","        if self.act_mask is not None:\n","            policy_logits += self.act_mask\n","\n","        if self.training:\n","            ##\n","            #policy_logits_ = policy_logits.clone()\n","            #policy_logits_[:, 1:17] *= (1 / 16)\n","            # print(f\"logits/16 is {policy_logits_}\")\n","            ##\n","            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n","        else:\n","            # Don't sample when testing.\n","            action = torch.argmax(policy_logits, dim=1)\n","\n","        # if last_actions.shape[0] == 1:\n","        #     if last_actions[0] == 21 or last_actions[0] == 18:\n","        #         if torch.rand(1) < 0.6:\n","        #             action[0] = 8\n","\n","\n","        policy_logits = policy_logits.view(T, B, self.num_actions)\n","        baseline = baseline.view(T, B)\n","        action = action.view(T, B)\n","        return (\n","            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n","            core_state,\n","        )\n","\n","def main(flags):\n","    if flags.mode == \"train\":\n","        train(flags)\n","    else:\n","        test(flags)"]},{"cell_type":"markdown","metadata":{"id":"hTuLRQpqPLri"},"source":["# train"]},{"cell_type":"markdown","metadata":{"id":"l3s5ipuzCK-x"},"source":["TODO:\n","action mask\n","exp replay actor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcPs79dQFSbt"},"outputs":[],"source":["#Net = NetHackNet\n","Net = NetHackNet_GAT\n","logging.getLogger().setLevel(logging.INFO)\n","#logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","simulated_args = [\n","    '--env', \"NetHack-v0\",     # NetHackChallenge-v0 NetHackScore-v0\n","    '--savedir', \"torchbeast/\",\n","    '--num_actors', '80',\n","    '--batch_size', '32',\n","    '--unroll_length', '80',\n","    '--learning_rate', '0.0005',\n","    '--entropy_cost', '0.001',\n","    '--use_lstm',\n","    '--total_steps', '10000000000',\n","    '--total_steps_', '20000000'\n","]\n","flags = parser.parse_args(simulated_args)\n","main(flags)"]},{"cell_type":"markdown","metadata":{"id":"OGzFIRamEp2S"},"source":["## info"]},{"cell_type":"markdown","metadata":{"id":"HXeVe-DDdZsY"},"source":["0 MORE 13 C-m read the next message\n","\n","1 North 107 k   75 K\n","\n","2 East 108 l    76 L\n","\n","3 South 106 j   74 J\n","\n","4 West 104 h    72 H\n","\n","5 North East 117 u  85 U\n","\n","6 South East 110 n  78 N\n","\n","7 South West 98 b   66 B\n","\n","8 North West 121 y  89 Y\n","\n","17 UP 60 < go up (e.g., a staircase)\n","\n","18 DOWN 62 > go down (e.g., a staircase)\n","\n","19 WAIT / SELF 46 . rest one move while doing nothing / apply to self\n","\n","20 KICK 4 C-d kick something\n","\n","21 EAT 101 e eat something\n","\n","22 SEARCH 115 s search for traps and secret doors"]},{"cell_type":"markdown","metadata":{"id":"pFQYQwA0liw2"},"source":["0 MiscAction.MORE\n","1 CompassDirection.N\n","2 CompassDirection.E\n","3 CompassDirection.S\n","4 CompassDirection.W\n","5 CompassDirection.NE\n","6 CompassDirection.SE\n","7 CompassDirection.SW\n","8 CompassDirection.NW\n","9 CompassDirectionLonger.N\n","10 CompassDirectionLonger.E\n","11 CompassDirectionLonger.S\n","12 CompassDirectionLonger.W\n","13 CompassDirectionLonger.NE\n","14 CompassDirectionLonger.SE\n","15 CompassDirectionLonger.SW\n","16 CompassDirectionLonger.NW\n","17 MiscDirection.UP\n","18 MiscDirection.DOWN\n","19 MiscDirection.WAIT\n","20 Command.KICK\n","21 Command.EAT\n","22 Command.SEARCH"]},{"cell_type":"markdown","metadata":{"id":"bZkLseWTVFQV"},"source":["<details>\n","<summary>Explanation of each key difference in the action space of nethack:</summary>\n","\n","Discrete(86)\n","0 CompassDirection.N\n","1 CompassDirection.E\n","2 CompassDirection.S\n","3 CompassDirection.W\n","4 CompassDirection.NE\n","5 CompassDirection.SE\n","6 CompassDirection.SW\n","7 CompassDirection.NW\n","8 CompassDirectionLonger.N\n","9 CompassDirectionLonger.E\n","10 CompassDirectionLonger.S\n","11 CompassDirectionLonger.W\n","12 CompassDirectionLonger.NE\n","13 CompassDirectionLonger.SE\n","14 CompassDirectionLonger.SW\n","15 CompassDirectionLonger.NW\n","16 MiscDirection.UP\n","17 MiscDirection.DOWN\n","18 MiscDirection.WAIT\n","19 MiscAction.MORE\n","20 Command.ADJUST\n","21 Command.APPLY\n","22 Command.ATTRIBUTES\n","23 Command.CALL\n","24 Command.CAST\n","25 Command.CHAT\n","26 Command.CLOSE\n","27 Command.DIP\n","28 Command.DROP\n","29 Command.DROPTYPE\n","30 Command.EAT\n","31 Command.ENGRAVE\n","32 Command.ENHANCE\n","33 Command.ESC\n","34 Command.FIGHT\n","35 Command.FIRE\n","36 Command.FORCE\n","37 Command.INVENTORY\n","38 Command.INVENTTYPE\n","39 Command.INVOKE\n","40 Command.JUMP\n","41 Command.KICK\n","42 Command.LOOK\n","43 Command.LOOT\n","44 Command.MONSTER\n","45 Command.MOVE\n","46 Command.MOVEFAR\n","47 Command.OFFER\n","48 Command.OPEN\n","49 Command.PAY\n","50 Command.PICKUP\n","51 Command.PRAY\n","52 Command.PUTON\n","53 Command.QUAFF\n","54 Command.QUIVER\n","55 Command.READ\n","56 Command.REMOVE\n","57 Command.RIDE\n","58 Command.RUB\n","59 Command.RUSH\n","60 Command.RUSH2\n","61 Command.SEARCH\n","62 Command.SEEARMOR\n","63 Command.SEERINGS\n","64 Command.SEETOOLS\n","65 Command.SEETRAP\n","66 Command.SEEWEAPON\n","67 Command.SHELL\n","68 Command.SIT\n","69 Command.SWAP\n","70 Command.TAKEOFF\n","71 Command.TAKEOFFALL\n","72 Command.THROW\n","73 Command.TIP\n","74 Command.TURN\n","75 Command.TWOWEAPON\n","76 Command.UNTRAP\n","77 Command.VERSIONSHORT\n","78 Command.WEAR\n","79 Command.WIELD\n","80 Command.WIPE\n","81 Command.ZAP\n","82 TextCharacters.PLUS\n","83 TextCharacters.QUOTE\n","84 TextCharacters.DOLLAR\n","85 TextCharacters.SPACE"]},{"cell_type":"markdown","metadata":{"id":"ywqo6l91oOSV"},"source":["# Eval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUrccgtwjZpg"},"outputs":[],"source":["def stack_ob(obs_list, max_len=80):\n","    # Trim the list to the last max_len elements if it's too long\n","    if len(obs_list) > max_len:\n","        obs_list = obs_list[-max_len:]\n","\n","    # Stack each key's values\n","    stacked_obs = {}\n","    for key in obs_list[0].keys():\n","        try:\n","            stacked_obs[key] = torch.cat([obs[key] for obs in obs_list], dim=0)\n","        except:\n","            pass\n","\n","    return stacked_obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTnumOyzoNuU"},"outputs":[],"source":["from IPython.display import clear_output\n","from random import sample\n","#checkpointpath = 'torchbeast/plots/Relational_full_v1.tar'\n","checkpointpath = 'torchbeast/plots/model4.tar'\n","gym_env = gym.make(\"NetHackScore-v0\")\n","# NetHack-v0 NetHackScore-v0 NetHackChallenge-v0\n","# NetHackStaircase-v0 NetHackStaircasePet-v0 NetHackOracle-v0 NetHackGold-v0 NetHackEat-v0 NetHackScout-v0\n","env = ResettingEnvironment(gym_env)\n","model = NetHackNet_GAT(gym_env.observation_space, gym_env.action_space.n, True)\n","model.eval()\n","checkpoint = torch.load(checkpointpath, map_location=\"cpu\")\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","\n","agent_state = model.initial_state(batch_size=1)\n","\n","observation = env.initial()\n","returns = []\n","\n","agent_state = model.initial_state(batch_size=1)\n","\n","a_list = [9,10,9,10,9,10]\n","\n","num_episodes = 200\n","reward = 0\n","action_list = []\n","while len(returns) < num_episodes:\n","    policy_outputs, agent_state = model(observation, agent_state)\n","    #raction = torch.tensor([[gym_env.action_space.sample()]])\n","    #raction_ = torch.tensor([[sample([9,10,11,12],1)]])\n","    action = policy_outputs[\"action\"]\n","    #action_list.append(action)\n","    observation = env.step(action)\n","    reward += observation[\"reward\"].item()\n","\n","    clear_output(wait=True)\n","    env.gym_env.render()\n","    print(observation[\"blstats\"])\n","    print(policy_outputs)\n","    print(reward)\n","    if observation[\"done\"].item():\n","        actions = torch.tensor(action_list)\n","        elements, counts = torch.unique(actions, return_counts=True)\n","        print('elements',elements)\n","        print('counts',counts)\n","        print('reward',reward)\n","\n","        # action_list = []\n","        reward = 0\n","        returns.append(observation[\"episode_return\"].item())\n","        logging.info(\n","            \"Episode ended after %d steps. Return: %.1f\",\n","            observation[\"episode_step\"].item(),\n","            observation[\"episode_return\"].item(),\n","            observation[\"last_action\"].item()\n","        )\n","        time.sleep(3)\n","\n","    time.sleep(0.3)\n","env.close()\n","logging.info(\n","    \"Average returns over %i steps: %.1f\", num_episodes, sum(returns) / len(returns)\n",")"]},{"cell_type":"markdown","metadata":{"id":"dIvVY2eiA2_q"},"source":["0 MiscAction.MORE\n","1 CompassDirection.N\n","2 CompassDirection.E\n","3 CompassDirection.S\n","4 CompassDirection.W\n","5 CompassDirection.NE\n","6 CompassDirection.SE\n","7 CompassDirection.SW\n","8 CompassDirection.NW\n","9 CompassDirectionLonger.N\n","10 CompassDirectionLonger.E\n","11 CompassDirectionLonger.S\n","12 CompassDirectionLonger.W\n","13 CompassDirectionLonger.NE\n","14 CompassDirectionLonger.SE\n","15 CompassDirectionLonger.SW\n","16 CompassDirectionLonger.NW\n","17 MiscDirection.UP\n","18 MiscDirection.DOWN\n","19 MiscDirection.WAIT\n","20 Command.KICK\n","21 Command.EAT\n","22 Command.SEARCH"]},{"cell_type":"markdown","metadata":{"id":"Y0LyFoHBFCRi"},"source":["# Plot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11790,"status":"ok","timestamp":1727042095676,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"},"user_tz":-480},"id":"y24iZpktRHR5","outputId":"2dfc690d-6ca4-4394-a9ad-b8c99963cd50"},"outputs":[{"name":"stdout","output_type":"stream","text":["plotting torchbeast/Relational_v1/logs.tsv\n","\f\u001b[0;39m                                                                                \n","\u001b[0;39m                                                                                \n","\u001b[0;39m                           \u001b[0;39maveraged mean_episode_return                         \n","\u001b[0;39m \u001b[0;39m 120 +---------------------------------------------------------------------+   \n","\u001b[0;39m      \u001b[0;39m|             +             +             +             +             |   \n","\u001b[0;39m      \u001b[0;39m|             \u001b[0;39m:             :       \u001b[1;35m++-+ ++-++-+ ++-+-+ \u001b[0;39m:             \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 100 |-+\u001b[0;39m...........:.............:..\u001b[1;35m++-++++++++++++++++++++++-+\u001b[0;39m..........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m-+           \u001b[0;39m:             \u001b[1;35m++++++++|+++++|++|++++|+||++-+            \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m-+           \u001b[0;39m:     \u001b[1;35m+-++-+++++++|++|||++|||||||||||||||||             \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m 80 |\u001b[1;35m-+\u001b[0;39m....\u001b[1;35m+++-+-+\u001b[0;39m:\u001b[1;35m++++++++++++++||||||||||||||||||||||||||||\u001b[0;39m...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-+++++++++++++++++++||||||||||||||AA|||AAAAA|AAAAAAAA|             \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-+++++++++++++||||||||||||AAAAAAAAAAAAAAAAAAAAAAAAAAAA             \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m 60 |\u001b[1;35m+|+++++++|||+|||||||||AAAAA|AAAAAAA|||AAA|||||||||||||||\u001b[0;39m...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35mA+++++++|||||||AAAAAAAAAAAAAA|||||||||||||||||||||||||||             \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m 40 |\u001b[1;35mA||||||AAAAAAAAAAAAA|||||||||||||||+||||++|++||||+|++|||\u001b[0;39m...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35mAA||||AAAAAAAAAA|||||||||||||||+|++++++++++++++++++++++-+            \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35mAAAAAAAAA||||||||||||++++|+++++++++++++++-+-++++++-+  +-+            \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m 20 |\u001b[1;35mAAAAAAA|||||++++|++++++++++-++++-+\u001b[0;39m.....\u001b[1;35m+-+\u001b[0;39m.............:...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m||||||||+++++++++++++-+    \u001b[0;39m:             :             :             \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m||||||+++++-+\u001b[0;39m:             :             :             :             \u001b[0;39m|   \n","\u001b[0;39m   \u001b[0;39m 0 |\u001b[1;35m++++++++-+\u001b[0;39m...:.............:.............:.............:...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++++++-+     \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m++-++-+      \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m-20 |\u001b[1;35m+\u001b[0;39m+\u001b[0;39m...........:.............:.............:.............:...........\u001b[0;39m+-|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m+            \u001b[0;39m:             :             :             :             \u001b[0;39m|   \n","\u001b[0;39m      \u001b[0;39m|\u001b[1;35m+            \u001b[0;39m+             +             +             +             |   \n","\u001b[0;39m  \u001b[0;39m-40 +---------------------------------------------------------------------+   \n","\u001b[0;39m     \u001b[0;39m 0           5e+06         1e+07        1.5e+07        2e+07        2.5e+07\n","\u001b[0;39m                                       \u001b[0;39msteps                                    \n","\u001b[0;39m                                                                                \n","\u001b[0;39;49mplotting torchbeast/Relational_v2/logs.tsv\n","\f\u001b[0;39m                                                                                \n","\u001b[0;39m                                                                                \n","\u001b[0;39m                          \u001b[0;39maveraged mean_episode_return                          \n","\u001b[0;39m \u001b[0;39m 90 +----------------------------------------------------------------------+   \n","\u001b[0;39m     \u001b[0;39m|         +         +         \u001b[1;35m+|+        \u001b[0;39m+       \u001b[1;35m+++-+     ++-+        \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 80 |-+\u001b[0;39m.......:.........:.........\u001b[1;35m+|+\u001b[0;39m..\u001b[1;35m+-+\u001b[0;39m...:.\u001b[1;35m+-++++++++++++++++|+\u001b[0;39m......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|         \u001b[0;39m:         :         \u001b[1;35m++-+++-+-+++++++++++++++++++++||         \u001b[0;39m|   \n","\u001b[0;39m     \u001b[0;39m|   \u001b[1;35m++-+  \u001b[0;39m:      \u001b[1;35m+-+\u001b[0;39m:     \u001b[1;35m++++-++++++++++++++++|||||||||+|||||         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 70 |-+\u001b[0;39m.\u001b[1;35m++-+\u001b[0;39m..:......\u001b[1;35m+-+\u001b[0;39m:.\u001b[1;35m++++++++++++||+|++||||+|||||||||||||||||\u001b[0;39m.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m+-+++-+  \u001b[0;39m:    \u001b[1;35m++-++-++++++++||+|||||||||||||||A||||A||||||A|A         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 60 |\u001b[1;35m+-++||\u001b[0;39m...\u001b[1;35m++++++++++++++++||||||||||||||||||A|AAAAAAAAAAAAAAAA\u001b[0;39m.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m++-+|| +-++++++++|++||++||||||A||AAAAAAAAAAAAAAAA|AAAAAAAA|||         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 50 |\u001b[1;35m++-++++++++|||||||||||+|||AAAAAAAAAAA|AAAAA|A||||||||||||||||\u001b[0;39m.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m|++++++++|||||||||A|||AAAAAAAAAAA||||||||||||||||||||||||+|||         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 40 |\u001b[1;35m||++||++|||||AAAAAAAAAAAAA|||||||||||||||||||+|++||+|+++++++-+\u001b[0;39m......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m||+|||||||AAAAAAAA|AA||A|||||||||+||++|||++++++++++++++++-++-+        \u001b[0;39m|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35mAA||AA|||AAA||||||||||||||||+++++++++++++++++-++++-++-+     \u001b[0;39m:         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 30 |\u001b[1;35mAAAAAAAAAA|||||||||||+|++++++-++++++-+++-+\u001b[0;39m........:.........:.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA|AAAAAA||||++++|+++++++++++-+|+-+      \u001b[0;39m:         :         :         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 20 |\u001b[1;35mA||A|||||+++++++++++-++-+\u001b[0;39m....\u001b[1;35m++-+\u001b[0;39m.......:.........:.........:.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA||||||+++++-+  +-+\u001b[0;39m:         \u001b[1;35m+-+        \u001b[0;39m:         :         :         \u001b[0;39m|   \n","\u001b[0;39m \u001b[0;39m 10 |\u001b[1;35mA||||++++++-+\u001b[0;39m...\u001b[1;35m+-+\u001b[0;39m:.........:..........:.........:.........:.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35mA+++++++-+         \u001b[0;39m:         :          :         :         :         \u001b[0;39m|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m+++++-++-+         \u001b[0;39m:         :          :         :         :         \u001b[0;39m|   \n","\u001b[0;39m  \u001b[0;39m 0 |\u001b[1;35m+-++||\u001b[0;39m...:.........:.........:..........:.........:.........:.......\u001b[0;39m+-|   \n","\u001b[0;39m     \u001b[0;39m|\u001b[1;35m-+ ++-+  \u001b[0;39m+         +         +          +         +         +         |   \n","\u001b[0;39m \u001b[0;39m-10 +----------------------------------------------------------------------+   \n","\u001b[0;39m    \u001b[0;39m 0       5e+06     1e+07    1.5e+07     2e+07    2.5e+07    3e+07    3.5e+07\n","\u001b[0;39m                                      \u001b[0;39msteps                                     \n","\u001b[0;39m                                                                                \n","\u001b[0;39;49m"]}],"source":["! python -m nle.scripts.plot --file \"torchbeast/Relational_v1/logs.tsv\"\n","! python -m nle.scripts.plot --file \"torchbeast/Relational_v2/logs.tsv\""]},{"cell_type":"markdown","metadata":{"id":"JamJ_ow3E0W7"},"source":["# Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzA69tWuA2Si"},"outputs":[],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTjWiJdy7ICt"},"outputs":[],"source":["from math import nan, isnan\n","import os\n","from tensorboardX import SummaryWriter\n","import csv\n","\n","# Change to the desired working directory\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","# List of TSV log file paths\n","tsv_files = [\n","    'torchbeast/plots/Baseline_v1.tsv',\n","    'torchbeast/plots/Relational_v1.tsv',\n","    # Add more log files here\n","]\n","\n","# Directory to save the TensorBoard logs\n","log_dir = 'runs/log_example'\n","\n","# Create the log directory if it doesn't exist\n","# if not os.path.exists(log_dir):\n","#     os.makedirs(log_dir)\n","\n","# Function to process a TSV file and log it to TensorBoard\n","def process_tsv_file(tsv_file_path, run_name):\n","    # Create a SummaryWriter for each run (log file)\n","    writer = SummaryWriter(os.path.join(log_dir, run_name))\n","\n","    with open(tsv_file_path, 'r') as tsv_file:\n","        reader = csv.DictReader(tsv_file, delimiter='\\t')\n","\n","        for row in reader:\n","            try:\n","                # Extract the step, loss, and return values from the TSV file\n","                step = int(row['# Step'])              # Step or iteration number\n","                loss = float(row['total_loss'])        # Loss value\n","                ret = float(row['mean_episode_return'])# Return value\n","\n","                # Log the scalar values to TensorBoard\n","                writer.add_scalar('Loss', loss, step)\n","                if not isnan(ret):\n","                    writer.add_scalar('Return', ret, step)\n","\n","            except KeyError as e:\n","                print(f\"Missing expected column: {e}\")\n","            except ValueError as e:\n","                print(f\"Data conversion error: {e}\")\n","\n","    # Close the writer after processing the file\n","    writer.close()\n","\n","# Process all the log files\n","for tsv_file_path in tsv_files:\n","    # Use the file name (without extension) as the run name\n","    run_name = os.path.splitext(os.path.basename(tsv_file_path))[0]\n","    process_tsv_file(tsv_file_path, run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QdYQAZrKDoFp"},"outputs":[],"source":["# Load the TensorBoard extension\n","%load_ext tensorboard\n","\n","# Launch TensorBoard\n","%tensorboard --logdir runs/log_example"]},{"cell_type":"markdown","metadata":{"id":"o87lJVnyNAFW"},"source":["# Info"]},{"cell_type":"markdown","metadata":{"id":"RLecPM6uJjtV"},"source":["wall 2361:- 2360:| 2362-2365:↖↗↘↙corner 2378:floor 2359:black\n","< 2382 > 2383"]},{"cell_type":"markdown","metadata":{"id":"pIufZ5n9c-_v"},"source":["<details>\n","<summary>Explanation of each key difference in the action space of challenge:</summary>\n","\n","Discrete(121)\n","0 CompassDirection.N\n","1 CompassDirection.E\n","2 CompassDirection.S\n","3 CompassDirection.W\n","4 CompassDirection.NE\n","5 CompassDirection.SE\n","6 CompassDirection.SW\n","7 CompassDirection.NW\n","8 CompassDirectionLonger.N\n","9 CompassDirectionLonger.E\n","10 CompassDirectionLonger.S\n","11 CompassDirectionLonger.W\n","12 CompassDirectionLonger.NE\n","13 CompassDirectionLonger.SE\n","14 CompassDirectionLonger.SW\n","15 CompassDirectionLonger.NW\n","16 MiscDirection.UP\n","17 MiscDirection.DOWN\n","18 MiscDirection.WAIT\n","19 MiscAction.MORE\n","\t20 Command.EXTCMD\n","\t21 Command.EXTLIST\n","22 Command.ADJUST 20\n","\t23 Command.ANNOTATE\n","24 Command.APPLY 21\n","25 Command.ATTRIBUTES 22\n","\t26 Command.AUTOPICKUP\n","27 Command.CALL 23\n","28 Command.CAST\n","29 Command.CHAT\n","30 Command.CLOSE 26\n","\t31 Command.CONDUCT\n","32 Command.DIP 27\n","33 Command.DROP\n","34 Command.DROPTYPE\n","35 Command.EAT\n","36 Command.ENGRAVE\n","37 Command.ENHANCE\n","38 Command.ESC\n","39 Command.FIGHT\n","40 Command.FIRE\n","41 Command.FORCE 36\n","\t42 Command.GLANCE\n","\t43 Command.HISTORY\n","44 Command.INVENTORY 37\n","45 Command.INVENTTYPE\n","46 Command.INVOKE\n","47 Command.JUMP\n","48 Command.KICK 41\n","\t49 Command.KNOWN\n","\t50 Command.KNOWNCLASS\n","51 Command.LOOK 42\n","52 Command.LOOT\n","53 Command.MONSTER\n","54 Command.MOVE\n","55 Command.MOVEFAR\n","56 Command.OFFER\n","57 Command.OPEN 48\n","\t58 Command.OPTIONS\n","\t59 Command.OVERVIEW\n","60 Command.PAY 49\n","61 Command.PICKUP\n","62 Command.PRAY\n","63 Command.PUTON\n","64 Command.QUAFF 53\n","\t65 Command.QUIT\n","66 Command.QUIVER 54\n","67 Command.READ 55\n","\t68 Command.REDRAW\n","69 Command.REMOVE 56\n","70 Command.RIDE\n","71 Command.RUB\n","72 Command.RUSH\n","73 Command.RUSH2 60\n","\t74 Command.SAVE\n","75 Command.SEARCH 61\n","\t76 Command.SEEALL\n","\t77 Command.SEEAMULET\n","78 Command.SEEARMOR 62\n","\t79 Command.SEEGOLD\n","80 Command.SEERINGS 63\n","\t81 Command.SEESPELLS\n","82 Command.SEETOOLS 64\n","83 Command.SEETRAP\n","84 Command.SEEWEAPON\n","85 Command.SHELL\n","86 Command.SIT\n","87 Command.SWAP\n","88 Command.TAKEOFF\n","89 Command.TAKEOFFALL 71\n","\t90 Command.TELEPORT\n","91 Command.THROW 72\n","92 Command.TIP 73\n","\t93 Command.TRAVEL\n","94 Command.TURN 74\n","95 Command.TWOWEAPON\n","96 Command.UNTRAP 76\n","\t97 Command.VERSION\n","98 Command.VERSIONSHORT 77\n","99 Command.WEAR 78\n","\t100 Command.WHATDOES\n","\t101 Command.WHATIS\n","102 Command.WIELD 79\n","103 Command.WIPE\n","104 Command.ZAP\n","105 TextCharacters.PLUS 82\n","\t106 TextCharacters.MINUS\n","107 TextCharacters.SPACE 83\n","\t108 TextCharacters.APOS\n","109 TextCharacters.QUOTE 84\n","\t110 TextCharacters.NUM_0\n","\t111 TextCharacters.NUM_1\n","\t112 TextCharacters.NUM_2\n","\t113 TextCharacters.NUM_3\n","\t114 TextCharacters.NUM_4\n","\t115 TextCharacters.NUM_5\n","\t116 TextCharacters.NUM_6\n","\t117 TextCharacters.NUM_7\n","\t118 TextCharacters.NUM_8\n","\t119 TextCharacters.NUM_9\n","120 TextCharacters.DOLLAR 85"]},{"cell_type":"markdown","metadata":{"id":"K3S2HBGmR6kt"},"source":["<details>\n","<summary>Explanation of each key in the action space:</summary>\n","\n","TASK_ACTIONS = tuple(\n","    [nethack.MiscAction.MORE]\n","    + list(nethack.CompassDirection)\n","    + list(nethack.CompassDirectionLonger)\n","    + list(nethack.MiscDirection)\n","    + [nethack.Command.KICK, nethack.Command.EAT, nethack.Command.SEARCH]\n",")\n","class MiscAction(enum.IntEnum):\n","    MORE = ord(\"\\r\")  # read the next message\n","\n","CompassDirection = enum.IntEnum(\n","    \"CompassDirection\",\n","    {\n","        **CompassCardinalDirection.__members__,\n","        **CompassIntercardinalDirection.__members__,\n","    },\n",")\n","class CompassCardinalDirection(enum.IntEnum):\n","    N = ord(\"k\")\n","    E = ord(\"l\")\n","    S = ord(\"j\")\n","    W = ord(\"h\")\n","\n","\n","class CompassIntercardinalDirection(enum.IntEnum):\n","    NE = ord(\"u\")\n","    SE = ord(\"n\")\n","    SW = ord(\"b\")\n","    NW = ord(\"y\")\n","\n","\n","CompassDirectionLonger = enum.IntEnum(\n","    \"CompassDirectionLonger\",\n","    {\n","        **CompassCardinalDirectionLonger.__members__,\n","        **CompassIntercardinalDirectionLonger.__members__,\n","    },\n",")\n","class CompassCardinalDirectionLonger(enum.IntEnum):\n","    N = ord(\"K\")\n","    E = ord(\"L\")\n","    S = ord(\"J\")\n","    W = ord(\"H\")\n","\n","class CompassIntercardinalDirectionLonger(enum.IntEnum):\n","    NE = ord(\"U\")\n","    SE = ord(\"N\")\n","    SW = ord(\"B\")\n","    NW = ord(\"Y\")\n","\n","class MiscDirection(enum.IntEnum):\n","    UP = ord(\"<\")  # go up a staircase\n","    DOWN = ord(\">\")  # go down a staircase\n","    WAIT = ord(\".\")  # rest one move while doing nothing / apply to self\n","\n","KICK = C(\"d\")  # kick something\n","EAT = ord(\"e\")  # eat something\n","SEARCH = ord(\"s\")  # search for traps and secret doors"]},{"cell_type":"markdown","metadata":{"id":"yGbjFka4cqfh"},"source":["<details>\n","<summary>Explanation of each key in the observation dictionary:</summary>\n","\n","1. glyphs: A 2D array representing the visual state of the game.\n","   Each value corresponds to a specific visual element or \"glyph\" in NetHack.\n","   Example shape: (21, 79) - corresponding to a 21x79 grid.\n","\n","2. chars: A 2D array containing the character representation of the glyphs.\n","   Each value is a character code that visually represents the game's state.\n","   Example shape: (21, 79).\n","\n","3. colors: A 2D array providing color information for the glyphs.\n","   Each value corresponds to a color code, giving more context to the visual elements.\n","   Example shape: (21, 79).\n","\n","4. specials: A 2D array that indicates special attributes or states for each cell.\n","   These might include things like \"lit\" or \"dark\" areas, traps, etc.\n","   Example shape: (21, 79).\n","\n","5. blstats: A 1D array containing various statistics and information about the player's status.\n","   This includes health, experience, gold, etc.\n","   Example shape: (25,) - representing various player statistics.\n","\n","6. message: A 1D array (or string) that contains the latest message displayed to the player.\n","   This is typically the last line of text describing what happened in the game.\n","   Example shape: (256,) - representing the characters in the message.\n","\n","7. inv_glyphs: A 1D array showing the glyphs for items in the player's inventory.\n","   Each glyph corresponds to an item.\n","   Example shape: (55,) - representing the inventory slots.\n","\n","8. inv_strs: A 1D array containing the string representations of the inventory items.\n","   Each string describes an item in the player's inventory.\n","   Example shape: (55,) - one string per inventory slot.\n","\n","9. inv_letters: A 1D array giving the letters corresponding to each item in the inventory.\n","   In NetHack, each item in the inventory is usually assigned a letter for quick access.\n","   Example shape: (55,).\n","\n","10. inv_oclasses: A 1D array showing the object classes for items in the inventory.\n","    This indicates the type of each item, such as weapon, armor, potion, etc.\n","    Example shape: (55,).\n","\n","11. tty_chars: A 2D array representing the state of the game as displayed in a traditional terminal (TTY) view.\n","    This is a character-based view of the game, similar to the \"chars\" key.\n","    Example shape: (24, 80) - a standard terminal size.\n","\n","12. tty_colors: A 2D array providing color information for the TTY view.\n","    Each value corresponds to a color code, similar to the \"colors\" key.\n","    Example shape: (24, 80).\n","\n","13. tty_cursor: A 1D array indicating the position of the cursor in the TTY view.\n","    This shows where the cursor is currently located on the screen.\n","    Example shape: (2,) - representing the row and column of the cursor.>"]},{"cell_type":"markdown","metadata":{"id":"bw5aaMkb3Ht3"},"source":["<details>\n","<summary>Explanation of each key in the blstats:</summary>\n","\n","        1.0 / 79.0, # hero col\n","        1.0 / 21, # hero row\n","        0.0, # strength pct\n","        1.0 / 10, # strength\n","        1.0 / 10, # dexterity\n","        1.0 / 10, # constitution\n","        1.0 / 10, # intelligence\n","        1.0 / 10, # wisdom\n","        1.0 / 10, # charisma\n","        0.0,      # score\n","        1.0 / 10, # hitpoints\n","        1.0 / 10, # max hitpoints\n","        0.0, # depth\n","        1.0 / 1000, # gold\n","        1.0 / 10, # energy\n","        1.0 / 10, # max energy\n","        1.0 / 10, # armor class\n","        0.0, # monster level\n","        1.0 / 10, # experience level\n","        1.0 / 100, # experience points\n","        1.0 / 1000, # time\n","        1.0, # hunger_state\n","        1.0 / 10, # carrying capacity\n","        0.0, # carrying capacity\n","        0.0, # level number\n","        0.0, # condition bits\n","\n","        /* blstats indices, see also botl.c and statusfields in botl.h. */\n","        #define NLE_BL_X 0\n","        #define NLE_BL_Y 1\n","        #define NLE_BL_STR25 2  /* strength 3..25 */\n","        #define NLE_BL_STR125 3 /* strength 3..125   */\n","        #define NLE_BL_DEX 4\n","        #define NLE_BL_CON 5\n","        #define NLE_BL_INT 6\n","        #define NLE_BL_WIS 7\n","        #define NLE_BL_CHA 8\n","        #define NLE_BL_SCORE 9\n","        #define NLE_BL_HP 10\n","        #define NLE_BL_HPMAX 11\n","        #define NLE_BL_DEPTH 12\n","        #define NLE_BL_GOLD 13\n","        #define NLE_BL_ENE 14\n","        #define NLE_BL_ENEMAX 15\n","        #define NLE_BL_AC 16\n","        #define NLE_BL_HD 17  /* monster level, \"hit-dice\" */\n","        #define NLE_BL_XP 18  /* experience level */\n","        #define NLE_BL_EXP 19 /* experience points */\n","        #define NLE_BL_TIME 20\n","        #define NLE_BL_HUNGER 21 /* hunger state */\n","        #define NLE_BL_CAP 22    /* carrying capacity */\n","        #define NLE_BL_DNUM 23\n","        #define NLE_BL_DLEVEL 24\n","        #define NLE_BL_CONDITION 25 /* condition bit mask */\n","        #define NLE_BL_ALIGN 26"]},{"cell_type":"markdown","metadata":{"id":"N31kKPYpMco0"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZU0IROIoZMP"},"outputs":[],"source":["# def generate_mask(done, input_tensor):\n","\n","#     cum_sum_mark = torch.cumsum(done.float(), dim=0)  # Shape [T*B, 1]\n","#     print(cum_sum_mark)\n","#     mask = (cum_sum_mark > 0).float()  # Valid elements where the cumulative sum is 0\n","\n","#     return mask\n","\n","# end_mark = torch.tensor([[False,False], [True,False], [False,False], [True,False]])\n","\n","# # Define an input tensor of shape [T*B, seq_len]\n","# input_tensor = torch.randn(4, 2, 8)  # [T*B, seq_len], 5 sequences of length 8\n","\n","# # Generate the mask\n","# mask = generate_mask(end_mark, input_tensor)\n","# print(mask)\n","# glyphs = torch.randint(0, 2500, (2, 9, 9))\n","\n","# # Step 2: Instantiate the get_graph_batch class\n","# graph_generator = get_graph_batch()\n","\n","# # Step 3: Use the forward method to process the glyph maps and generate a batch of graphs\n","# graph_batch = graph_generator(glyphs)\n","\n","# # Step 4: Print out key information to verify the output\n","# print(\"Node features (x):\", graph_batch.x)          # Node features (glyph IDs)\n","# print(\"Edge index (edges):\", graph_batch.edge_index)  # Edge indices between nodes\n","\n","# # If you want to verify further details:\n","# print(f\"Number of nodes: {graph_batch.num_nodes}\")\n","# print(f\"Number of edges: {graph_batch.num_edges}\")\n","# print(f\"Batch size (graphs): {graph_batch.batch.max().item() + 1}\")\n","# print(graph_batch.x.shape,graph_batch.edge_index.shape)\n","\n","# gym_env22 = create_env('NetHackScore-v0')\n","# print(gym_env22.action_space)\n","# gym_env22.unwrapped.print_action_meanings()\n","# print(gym_env22.observation_space)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1725460093066,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"},"user_tz":-480},"id":"qVUcY7y4Sqsp","outputId":"bec6212b-ac80-4358-9688-b35d19f454c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["DataBatch(x=[13, 1], edge_index=[2, 30], batch=[13], ptr=[3])\n","torch.Size([13, 8])\n","torch.Size([2, 4])\n"]}],"source":["grid1 = np.array([\n","    [1, 2, 3],\n","     [4, 5, 6],\n","      [7, 8, 9]])\n","\n","grid2 = np.array([\n","    [10, 11, 12],\n","     [13, 14, 15],\n","      [16, 17, 18]])\n","blocked_ids = {2, 5, 10, 11, 14}\n","\n","# Blocked node IDs (walls)\n","batch = get_graph_batch(torch.tensor([grid1,grid2]),blocked_ids)\n","print(batch)\n","\n","emb = nn.Embedding(25, 8)\n","embed = emb(batch.x).squeeze()\n","print(embed.shape)\n","md = GAT(8,4,4)\n","out = md(embed,batch.edge_index,batch.batch)\n","print(out.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1235,"status":"ok","timestamp":1725250097068,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"},"user_tz":-480},"id":"tohrom_Exyzb","outputId":"38f114f1-2b3a-48ff-9e34-ce92845320bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-4.5952139e+00  2.5131800e+00 -3.8313661e+00  1.9113704e+00\n","  8.7944448e-01 -2.3180022e+00  6.1128769e+00  1.2038110e+00\n"," -1.2344328e+00  1.5616350e+00  2.4153082e+00  2.5785027e+00\n"," -3.6446410e-01  3.0473328e-01 -6.0826463e-01 -1.1008358e+00\n","  5.7924190e-04 -2.1493638e+00 -4.9723701e+00 -4.4495182e+00]\n","doorway: 0.940357506275177\n","drawbridge: 0.925424337387085\n","doors: 0.8861480355262756\n","closet: 0.8858508467674255\n","closed: 0.8701543211936951\n","locked: 0.870032012462616\n","wall: 0.8460908532142639\n","secret: 0.8290387392044067\n","square: 0.8033416271209717\n","stairs: 0.7987379431724548\n","downwards: 0.7928858995437622\n","hole: 0.7917185425758362\n","corridor: 0.7909206748008728\n","ladder: 0.7736232876777649\n","floor: 0.7626956105232239\n","portcullis: 0.7608734965324402\n","vibrating: 0.7536466717720032\n","downstairs: 0.7511041760444641\n","chest: 0.7509174346923828\n","doorways: 0.750511646270752\n","magically: 0.7503724694252014\n","couldnt: 0.7420575022697449\n","grave: 0.741557776927948\n","untrapped: 0.7406994104385376\n","unseen: 0.7403778433799744\n","drawbridges: 0.7375343441963196\n","closing: 0.7368388175964355\n","lock: 0.7343628406524658\n","shop: 0.7342779636383057\n","underneath: 0.7328135967254639\n"]}],"source":["from gensim.models import Word2Vec,KeyedVectors\n","w2v = Word2Vec.load(\"wiki.model\")\n","v1 = w2v.wv['dog']\n","print(v1) # Return a 300 Dim vector\n","\n","similar_words = w2v.wv.most_similar('door', topn=30)  # topn determines how many similar words you want\n","\n","# Print the similar words\n","for similar_word, similarity_score in similar_words:\n","    print(f\"{similar_word}: {similarity_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1725349442947,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"},"user_tz":-480},"id":"3lnZInqnVxJ6","outputId":"0a4994d6-1801-41f1-efb3-63921239b162"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'CartPole-v0': EnvSpec(id='CartPole-v0', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv'), 'CartPole-v1': EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv'), 'MountainCar-v0': EnvSpec(id='MountainCar-v0', entry_point='gymnasium.envs.classic_control.mountain_car:MountainCarEnv', reward_threshold=-110.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCar', version=0, additional_wrappers=(), vector_entry_point=None), 'MountainCarContinuous-v0': EnvSpec(id='MountainCarContinuous-v0', entry_point='gymnasium.envs.classic_control.continuous_mountain_car:Continuous_MountainCarEnv', reward_threshold=90.0, nondeterministic=False, max_episode_steps=999, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='MountainCarContinuous', version=0, additional_wrappers=(), vector_entry_point=None), 'Pendulum-v1': EnvSpec(id='Pendulum-v1', entry_point='gymnasium.envs.classic_control.pendulum:PendulumEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pendulum', version=1, additional_wrappers=(), vector_entry_point=None), 'Acrobot-v1': EnvSpec(id='Acrobot-v1', entry_point='gymnasium.envs.classic_control.acrobot:AcrobotEnv', reward_threshold=-100.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Acrobot', version=1, additional_wrappers=(), vector_entry_point=None), 'phys2d/CartPole-v0': EnvSpec(id='phys2d/CartPole-v0', entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxEnv', reward_threshold=195.0, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='CartPole', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxVectorEnv'), 'phys2d/CartPole-v1': EnvSpec(id='phys2d/CartPole-v1', entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.cartpole:CartPoleJaxVectorEnv'), 'phys2d/Pendulum-v0': EnvSpec(id='phys2d/Pendulum-v0', entry_point='gymnasium.envs.phys2d.pendulum:PendulumJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='phys2d', name='Pendulum', version=0, additional_wrappers=(), vector_entry_point='gymnasium.envs.phys2d.pendulum:PendulumJaxVectorEnv'), 'LunarLander-v2': EnvSpec(id='LunarLander-v2', entry_point='gymnasium.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='LunarLander', version=2, additional_wrappers=(), vector_entry_point=None), 'LunarLanderContinuous-v2': EnvSpec(id='LunarLanderContinuous-v2', entry_point='gymnasium.envs.box2d.lunar_lander:LunarLander', reward_threshold=200, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'continuous': True}, namespace=None, name='LunarLanderContinuous', version=2, additional_wrappers=(), vector_entry_point=None), 'BipedalWalker-v3': EnvSpec(id='BipedalWalker-v3', entry_point='gymnasium.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=1600, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='BipedalWalker', version=3, additional_wrappers=(), vector_entry_point=None), 'BipedalWalkerHardcore-v3': EnvSpec(id='BipedalWalkerHardcore-v3', entry_point='gymnasium.envs.box2d.bipedal_walker:BipedalWalker', reward_threshold=300, nondeterministic=False, max_episode_steps=2000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'hardcore': True}, namespace=None, name='BipedalWalkerHardcore', version=3, additional_wrappers=(), vector_entry_point=None), 'CarRacing-v2': EnvSpec(id='CarRacing-v2', entry_point='gymnasium.envs.box2d.car_racing:CarRacing', reward_threshold=900, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CarRacing', version=2, additional_wrappers=(), vector_entry_point=None), 'Blackjack-v1': EnvSpec(id='Blackjack-v1', entry_point='gymnasium.envs.toy_text.blackjack:BlackjackEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sab': True, 'natural': False}, namespace=None, name='Blackjack', version=1, additional_wrappers=(), vector_entry_point=None), 'FrozenLake-v1': EnvSpec(id='FrozenLake-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.7, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '4x4'}, namespace=None, name='FrozenLake', version=1, additional_wrappers=(), vector_entry_point=None), 'FrozenLake8x8-v1': EnvSpec(id='FrozenLake8x8-v1', entry_point='gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv', reward_threshold=0.85, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'map_name': '8x8'}, namespace=None, name='FrozenLake8x8', version=1, additional_wrappers=(), vector_entry_point=None), 'CliffWalking-v0': EnvSpec(id='CliffWalking-v0', entry_point='gymnasium.envs.toy_text.cliffwalking:CliffWalkingEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='CliffWalking', version=0, additional_wrappers=(), vector_entry_point=None), 'Taxi-v3': EnvSpec(id='Taxi-v3', entry_point='gymnasium.envs.toy_text.taxi:TaxiEnv', reward_threshold=8, nondeterministic=False, max_episode_steps=200, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Taxi', version=3, additional_wrappers=(), vector_entry_point=None), 'tabular/Blackjack-v0': EnvSpec(id='tabular/Blackjack-v0', entry_point='gymnasium.envs.tabular.blackjack:BlackJackJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={'sutton_and_barto': True, 'natural': False}, namespace='tabular', name='Blackjack', version=0, additional_wrappers=(), vector_entry_point=None), 'tabular/CliffWalking-v0': EnvSpec(id='tabular/CliffWalking-v0', entry_point='gymnasium.envs.tabular.cliffwalking:CliffWalkingJaxEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace='tabular', name='CliffWalking', version=0, additional_wrappers=(), vector_entry_point=None), 'Reacher-v2': EnvSpec(id='Reacher-v2', entry_point='gymnasium.envs.mujoco:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=2, additional_wrappers=(), vector_entry_point=None), 'Reacher-v4': EnvSpec(id='Reacher-v4', entry_point='gymnasium.envs.mujoco.reacher_v4:ReacherEnv', reward_threshold=-3.75, nondeterministic=False, max_episode_steps=50, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Reacher', version=4, additional_wrappers=(), vector_entry_point=None), 'Pusher-v2': EnvSpec(id='Pusher-v2', entry_point='gymnasium.envs.mujoco:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=2, additional_wrappers=(), vector_entry_point=None), 'Pusher-v4': EnvSpec(id='Pusher-v4', entry_point='gymnasium.envs.mujoco.pusher_v4:PusherEnv', reward_threshold=0.0, nondeterministic=False, max_episode_steps=100, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Pusher', version=4, additional_wrappers=(), vector_entry_point=None), 'InvertedPendulum-v2': EnvSpec(id='InvertedPendulum-v2', entry_point='gymnasium.envs.mujoco:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=2, additional_wrappers=(), vector_entry_point=None), 'InvertedPendulum-v4': EnvSpec(id='InvertedPendulum-v4', entry_point='gymnasium.envs.mujoco.inverted_pendulum_v4:InvertedPendulumEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedPendulum', version=4, additional_wrappers=(), vector_entry_point=None), 'InvertedDoublePendulum-v2': EnvSpec(id='InvertedDoublePendulum-v2', entry_point='gymnasium.envs.mujoco:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=2, additional_wrappers=(), vector_entry_point=None), 'InvertedDoublePendulum-v4': EnvSpec(id='InvertedDoublePendulum-v4', entry_point='gymnasium.envs.mujoco.inverted_double_pendulum_v4:InvertedDoublePendulumEnv', reward_threshold=9100.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='InvertedDoublePendulum', version=4, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v2': EnvSpec(id='HalfCheetah-v2', entry_point='gymnasium.envs.mujoco:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=2, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v3': EnvSpec(id='HalfCheetah-v3', entry_point='gymnasium.envs.mujoco.half_cheetah_v3:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=3, additional_wrappers=(), vector_entry_point=None), 'HalfCheetah-v4': EnvSpec(id='HalfCheetah-v4', entry_point='gymnasium.envs.mujoco.half_cheetah_v4:HalfCheetahEnv', reward_threshold=4800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HalfCheetah', version=4, additional_wrappers=(), vector_entry_point=None), 'Hopper-v2': EnvSpec(id='Hopper-v2', entry_point='gymnasium.envs.mujoco:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=2, additional_wrappers=(), vector_entry_point=None), 'Hopper-v3': EnvSpec(id='Hopper-v3', entry_point='gymnasium.envs.mujoco.hopper_v3:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=3, additional_wrappers=(), vector_entry_point=None), 'Hopper-v4': EnvSpec(id='Hopper-v4', entry_point='gymnasium.envs.mujoco.hopper_v4:HopperEnv', reward_threshold=3800.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Hopper', version=4, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v2': EnvSpec(id='Swimmer-v2', entry_point='gymnasium.envs.mujoco:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=2, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v3': EnvSpec(id='Swimmer-v3', entry_point='gymnasium.envs.mujoco.swimmer_v3:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=3, additional_wrappers=(), vector_entry_point=None), 'Swimmer-v4': EnvSpec(id='Swimmer-v4', entry_point='gymnasium.envs.mujoco.swimmer_v4:SwimmerEnv', reward_threshold=360.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Swimmer', version=4, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v2': EnvSpec(id='Walker2d-v2', entry_point='gymnasium.envs.mujoco:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=2, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v3': EnvSpec(id='Walker2d-v3', entry_point='gymnasium.envs.mujoco.walker2d_v3:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=3, additional_wrappers=(), vector_entry_point=None), 'Walker2d-v4': EnvSpec(id='Walker2d-v4', entry_point='gymnasium.envs.mujoco.walker2d_v4:Walker2dEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Walker2d', version=4, additional_wrappers=(), vector_entry_point=None), 'Ant-v2': EnvSpec(id='Ant-v2', entry_point='gymnasium.envs.mujoco:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=2, additional_wrappers=(), vector_entry_point=None), 'Ant-v3': EnvSpec(id='Ant-v3', entry_point='gymnasium.envs.mujoco.ant_v3:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=3, additional_wrappers=(), vector_entry_point=None), 'Ant-v4': EnvSpec(id='Ant-v4', entry_point='gymnasium.envs.mujoco.ant_v4:AntEnv', reward_threshold=6000.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Ant', version=4, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v2': EnvSpec(id='Humanoid-v2', entry_point='gymnasium.envs.mujoco:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=2, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v3': EnvSpec(id='Humanoid-v3', entry_point='gymnasium.envs.mujoco.humanoid_v3:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=3, additional_wrappers=(), vector_entry_point=None), 'Humanoid-v4': EnvSpec(id='Humanoid-v4', entry_point='gymnasium.envs.mujoco.humanoid_v4:HumanoidEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='Humanoid', version=4, additional_wrappers=(), vector_entry_point=None), 'HumanoidStandup-v2': EnvSpec(id='HumanoidStandup-v2', entry_point='gymnasium.envs.mujoco:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=2, additional_wrappers=(), vector_entry_point=None), 'HumanoidStandup-v4': EnvSpec(id='HumanoidStandup-v4', entry_point='gymnasium.envs.mujoco.humanoidstandup_v4:HumanoidStandupEnv', reward_threshold=None, nondeterministic=False, max_episode_steps=1000, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='HumanoidStandup', version=4, additional_wrappers=(), vector_entry_point=None), 'GymV21Environment-v0': EnvSpec(id='GymV21Environment-v0', entry_point=<function _raise_shimmy_error at 0x78a372580040>, reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='GymV21Environment', version=0, additional_wrappers=(), vector_entry_point=None), 'GymV26Environment-v0': EnvSpec(id='GymV26Environment-v0', entry_point=<function _raise_shimmy_error at 0x78a372580040>, reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=True, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='GymV26Environment', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHack-v0': EnvSpec(id='NetHack-v0', entry_point='nle.env.base:NLE', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHack', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackScore-v0': EnvSpec(id='NetHackScore-v0', entry_point='nle.env.tasks:NetHackScore', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackScore', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackStaircase-v0': EnvSpec(id='NetHackStaircase-v0', entry_point='nle.env.tasks:NetHackStaircase', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackStaircase', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackStaircasePet-v0': EnvSpec(id='NetHackStaircasePet-v0', entry_point='nle.env.tasks:NetHackStaircasePet', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackStaircasePet', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackOracle-v0': EnvSpec(id='NetHackOracle-v0', entry_point='nle.env.tasks:NetHackOracle', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackOracle', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackGold-v0': EnvSpec(id='NetHackGold-v0', entry_point='nle.env.tasks:NetHackGold', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackGold', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackEat-v0': EnvSpec(id='NetHackEat-v0', entry_point='nle.env.tasks:NetHackEat', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackEat', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackScout-v0': EnvSpec(id='NetHackScout-v0', entry_point='nle.env.tasks:NetHackScout', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackScout', version=0, additional_wrappers=(), vector_entry_point=None), 'NetHackChallenge-v0': EnvSpec(id='NetHackChallenge-v0', entry_point='nle.env.tasks:NetHackChallenge', reward_threshold=None, nondeterministic=False, max_episode_steps=None, order_enforce=False, autoreset=False, disable_env_checker=False, apply_api_compatibility=False, kwargs={}, namespace=None, name='NetHackChallenge', version=0, additional_wrappers=(), vector_entry_point=None)}\n","Discrete(23)\n","0 MiscAction.MORE\n","1 CompassDirection.N\n","2 CompassDirection.E\n","3 CompassDirection.S\n","4 CompassDirection.W\n","5 CompassDirection.NE\n","6 CompassDirection.SE\n","7 CompassDirection.SW\n","8 CompassDirection.NW\n","9 CompassDirectionLonger.N\n","10 CompassDirectionLonger.E\n","11 CompassDirectionLonger.S\n","12 CompassDirectionLonger.W\n","13 CompassDirectionLonger.NE\n","14 CompassDirectionLonger.SE\n","15 CompassDirectionLonger.SW\n","16 CompassDirectionLonger.NW\n","17 MiscDirection.UP\n","18 MiscDirection.DOWN\n","19 MiscDirection.WAIT\n","20 Command.KICK\n","21 Command.EAT\n","22 Command.SEARCH\n","Dict('blstats': Box(-2147483648, 2147483647, (27,), int64), 'chars': Box(0, 255, (21, 79), uint8), 'colors': Box(0, 15, (21, 79), uint8), 'glyphs': Box(0, 5976, (21, 79), int16), 'inv_glyphs': Box(0, 5976, (55,), int16), 'inv_letters': Box(0, 127, (55,), uint8), 'inv_oclasses': Box(0, 18, (55,), uint8), 'inv_strs': Box(0, 255, (55, 80), uint8), 'message': Box(0, 255, (256,), uint8), 'screen_descriptions': Box(0, 127, (21, 79, 80), uint8), 'specials': Box(0, 255, (21, 79), uint8), 'tty_chars': Box(0, 255, (24, 80), uint8), 'tty_colors': Box(0, 31, (24, 80), int8), 'tty_cursor': Box(0, 255, (2,), uint8))\n"]}],"source":["import gymnasium as gym\n","import nle\n","import random\n","import time\n","from IPython.display import clear_output\n","import torch\n","from nle.env.tasks import NetHackChallenge\n","from nle.env.tasks import NetHackScore\n","import numpy as np\n","random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","print(gym.envs.registry )\n","env = gym.make(\"NetHackScore-v0\",character=\"mon-hum-neu-mal\") # NetHack-v0 NetHackScore-v0 NetHackChallenge-v0\n","print(env.action_space)\n","env.unwrapped.print_action_meanings()\n","print(env.observation_space)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fydstESP6fBq"},"outputs":[],"source":["env.reset()  # each reset generates a new dungeon\n","#ob = env.step(1)  # move agent '@' north\n","#env.render()\n","\n","def to_char(l):\n","    try:\n","        char_message = [chr(n) for n in l if 0 <= n <= 127]\n","        char_message = ''.join(char_message)\n","        return char_message\n","    except TypeError:\n","        print(f\"Error processing key: {i}, value type: {type(obs[i])}, value: {obs[i]}\") # Print more info for debugging\n","\n","a_list = [9,10,9,10,9,10]#[0,9,3,8,8,21]\n","\n","for i in range(len(a_list)):\n","    #a = env.action_space.sample()\n","    a = a_list[i]\n","    print('act:',a)\n","    #clear_output(wait=True) ###\n","    obs, reward, done, truncation, info = env.step(a)\n","    cord = [obs['blstats'][0],obs['blstats'][1]]\n","    print(cord)\n","    for i in ['glyphs','chars','colors','specials']:\n","        print(i,crop(torch.tensor([obs[i]]),torch.tensor([cord])))\n","    print(get_edge_index(torch.tensor([obs['glyphs']])))\n","    # char_message = to_char(obs['message'])\n","    # print(char_message)\n","    # char_inv = to_char(obs['inv_letters'])\n","    # print(char_inv)\n","    # print(obs['inv_oclasses'])\n","    # print(obs['inv_strs'])\n","\n","    #print(obs)\n","    env.render()\n","    time.sleep(1.0) ###\n","\n","#env.close()"]},{"cell_type":"markdown","metadata":{"id":"muAOLekC7W18"},"source":["# CoPE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5463,"status":"ok","timestamp":1727536684540,"user":{"displayName":"Zijian Xue","userId":"11201134375165645464"},"user_tz":-480},"id":"LUPsuEr67bFh","outputId":"9a2e25b2-fab5-4742-81fc-83c756c06433"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 128, 64])\n"]}],"source":["import torch\n","from torch import nn, einsum\n","import math\n","\n","from einops import rearrange,repeat\n","\n","def exists(val):\n","    return val is not None\n","\n","def default(val, d):\n","    return val if exists(val) else d\n","\n","class CoPE(nn.Module):\n","    def __init__ (self, C, T=None) :\n","        '''\n","            CoPE实现\n","        :param C: 每个头的通道维度\n","        :param T: 最大的上下文长度\n","        '''\n","        super().__init__()\n","        self.T = T if(T is not None) else C\n","        self.pos_emb = nn.parameter.Parameter(\n","            torch.zeros(1, C, self.T)\n","        )\n","\n","    def forward (self, q, qk) :\n","        '''\n","            CoPE的思想是，融合上下文寻址和位置寻址。\n","            1. 通过qiT @ kj来产生上下文位置\n","            2. 利用传统的RPE方式，将通过上下文位置当做特征位置进行编码\n","        :param q: (B*H,N,C) or (B,H,N,C)\n","        :param qk: (B*H,N,N) or (B,H,N,N), q与k相乘的结果，不包含softmax部分\n","        :return: E: (B*H,N,N) or (B,H,N,N)\n","        '''\n","        '''\n","            q(B,H,N,C) @ kT(B,H,C,N) = qk(B,H,N,N)\n","            q中每一个行向量与kT中的列向量相乘，因此公式表达是qiT @ kj (qi,kj都表示的列向量)\n","            对上下文寻址，进行门限取值,取值范围为（0,1），取值越大，意味着权重越大\n","            这句代码，对应了公式(3), gij = sigmid(qiT @ ki)\n","            G(B,H,N,N)\n","        '''\n","        G = torch.sigmoid(qk)\n","        '''\n","            [B,N,N]沿着最后一个维度进行翻转特征，然后对最后一维度进行累计求和，最后再沿着最后一个维度将特征翻转刚回来\n","            [a,b,c] -> [c,b,a] -> [a+b+c,b+c,a] -> [a,b+c,a+b+c]\n","            这句代码，对应了公式(4), pij = sum{k=j ~ i}(gik)\n","            P(B,H,N,N)\n","        '''\n","        P = G.flip(-1).cumsum(dim=-1).flip(-1)\n","        P = P.clamp(max=self.T - 1)\n","        '''\n","            整型编码插值\n","            由于sigmod的原因，CoPE不能像传统的RPE一样，利用可学习的编码层学习位置信息\n","            因此，使用一种简单的整型向量插值方法，来融合可学习的编码特征\n","            以下代码对应公式(5)\n","            同时，公式(9)采用了一种更高效的实现\n","            E(B,H,N,N)\n","        '''\n","        P_ceil = P.ceil().long()\n","        P_floor = P.floor().long()\n","        # (B,H,N,C) @ (1,C,T) = (B,H,N,T)\n","        E = torch.matmul(q, self.pos_emb) # eij\n","        E_ceil = E.gather(-1, P_ceil)\n","        E_floor = E.gather(-1, P_floor)\n","        P_P_floor = P - P_floor\n","        #E = (P - P_floor) * E_cell + (1 - P + P_floor) * E_floor\n","        E = P_P_floor * E_ceil + (1 - P_P_floor) * E_floor\n","        return E\n","\n","\n","class Attention(nn.Module):\n","\n","    def __init__(\n","            self,\n","            Q_dim,\n","            KV_dim = None,\n","            heads = 8,\n","            dim_head = 64\n","    ):\n","        super().__init__()\n","        inner_dim = dim_head * heads\n","        KV_dim = default(KV_dim, Q_dim)\n","        self.scale = dim_head ** -0.5\n","        self.heads = heads\n","        self.to_q = nn.Linear(Q_dim, inner_dim, bias = False)\n","        self.to_kv = nn.Linear(KV_dim, inner_dim * 2, bias = False)\n","        self.to_out = nn.Linear(inner_dim, Q_dim)\n","\n","        self.cope = CoPE(dim_head)\n","\n","    def forward(self, x, kv = None, mask=None):\n","\n","        h = self.heads\n","        q = self.to_q(x)\n","        kv = default(kv, x)\n","        k,v = self.to_kv(kv).chunk(2, dim = -1)\n","        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n","        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n","\n","        if exists(mask):\n","            mask = rearrange(mask, 'b ... -> b (...)')\n","            max_neg_value = -torch.finfo(sim.dtype).max\n","            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n","            sim.masked_fill_(~mask, max_neg_value)\n","\n","        pe = self.cope(q, sim)\n","        sim = sim + pe\n","\n","        attn = sim.softmax(dim = -1)\n","        out = einsum('b i j, b j d -> b i d', attn, v)\n","        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n","        out = self.to_out(out)\n","        return out\n","\n","if __name__ == '__main__':\n","\n","    q = torch.ones(size=(2,128,64))\n","    a = Attention(Q_dim=64)\n","    x = a(q)\n","    print(x.shape)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["VPUFfjku6PYh","DRO_HN7y7ibG","BswoL49WBxye","OGzFIRamEp2S","ywqo6l91oOSV","Y0LyFoHBFCRi","JamJ_ow3E0W7","o87lJVnyNAFW","N31kKPYpMco0"],"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1RzYW2pd4FnpXn1bKmIMWwwvfbJ-dAjts","timestamp":1733920867079},{"file_id":"1HzANB-SddV7gaVjzlR4-Q-28iVDQ-FtB","timestamp":1730664310189}],"mount_file_id":"1RzYW2pd4FnpXn1bKmIMWwwvfbJ-dAjts","authorship_tag":"ABX9TyMO2jRuF38NYc5I9AsKFNxT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}